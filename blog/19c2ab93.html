<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="bigdata, ai, 中间件、算法、大数据、人工智能"><title>Spark性能调优实战 | Tony's Notes</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Spark性能调优实战</h1><a id="logo" href="/.">Tony's Notes</a><p class="description">Stay Hungry, Stay Foolish</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Spark性能调优实战</h1><div class="post-meta">Jul 29, 2021<span> | </span><span class="category"><a href="/categories/计算引擎/">计算引擎</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 3.2k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 12</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Spark（Spark SQL）在离线计算场景应用广泛，为了保证Spark应用更好地满足业务场景需求，同时能够在线上稳定地运行，我们需要关注Spark的调优工作。首先，需要了解Spark对外的接口并如何高效地使用；其次要搞清楚内部的运行机制以及参数配置体系；最后是要能够深入分析spark的日志信息。进一步来讲，对于Spark的深度使用者，需要关注社区各个版本的迭代、bug修复以及性能优化的情况，才能更好地打开思路，提高解决问题的效率。<font color="red">主要途径有：spark的release-note、databricks官方博客、源码。</font></p>
<p>为了方便Spark相关性能问题的排查，本文记录了日常Spark使用过程中遇到的问题和解决思路，用于积累过程中进行复盘总结，强化Spark的深入理解和实战经验。</p>
<a id="more"></a>
<h2 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h2><ol>
<li><p>如何优化Spark集群CPU利用率？</p>
<p>产生CPU利用率低一般有两种原因导致：Executor上线程挂起和Task任务数太多调度开销大。需要以下三方面来考虑：</p>
<ul>
<li>并行度：拆分任务的粒度，涉及参数有：spark.default.parallelism、spark.sql.shuffle.partitions、spark.sql.files.maxPartitionBytes、spark.sql.files.openCostInBytes</li>
<li>并发度N：集群总体cores，spark.executor.instances、spark.executor.cores、spark.task.cpus</li>
<li>执行内存M：executor在可获得执行内存，下限是spark.executor.memory <em> spark.memory.fraction </em> (1- spark.memory.storageFraction)，上限是spark.executor.memory * spark.memory.fraction</li>
</ul>
<p>在给定执行内存 M、线程池大小 N 和数据总量 D 的时候，想要有效地提升 CPU 利用率，我们就要计算出最佳并行度 P，计算方法是让数据分片的平均大小 D/P 坐落在（M/N/2, M/N）区间。这样，在运行时 CPU 利用率往往不会太差。</p>
<p>总而言之，需要平衡“并行度、并发度、执行内存”，去提升CPU利用率，所以更需要使用系统工具、监控工具，比如ganglia、Prometheus、Grafana、nmon、htop等等这些工具，去观察你的CPU利用率，然后回过头来平衡三者，然后再去观察CPU利用率是不是提升了，通过这种方式来去调优。</p>
</li>
<li><p>针对CET达到几百规模的大SQL在上千规模Hadoop集群的执行性能调优？</p>
<p>SparkSQL thriftserver 侧的优化：</p>
<ul>
<li>元数据读写一些锁的优化，从比较大的锁粒度降到比较小的锁粒度；</li>
<li>引入多线程，提高解析每个互不依赖的子查询的并行度。</li>
</ul>
<p>DAGScheduler侧的优化：</p>
<ul>
<li>引入线程池提高Task被调度到Executor的效率，降低调度延迟；</li>
<li>适当调小spark.locality.wait.node，降低延迟调度的时间，提高调度的效率；</li>
<li>适当调大spark.resultGetter.threads的数值，提高处理返回结果的效率。</li>
</ul>
</li>
<li><p>如何利用到ReuseExchange的特性？</p>
<p>比如有这样一个场景，读取一批parque文件，都是按照A字段做group by后，进行两个场景的计算。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//方式1</span></span><br><span class="line"><span class="keyword">val</span> filePath: <span class="type">String</span> = _</span><br><span class="line"><span class="keyword">val</span> df = spark.read.parquet(filePath) </span><br><span class="line"><span class="keyword">val</span> dfPV = df.groupBy(<span class="string">"userId"</span>).agg(count(<span class="string">"page"</span>).alias(<span class="string">"value"</span>)).withColumn(<span class="string">"metrics"</span>, lit(<span class="string">"PV"</span>))</span><br><span class="line"><span class="keyword">val</span> dfUV = df.groupBy(<span class="string">"userId"</span>).agg(countDistinct(<span class="string">"page"</span>).alias(<span class="string">"value"</span>)).withColumn(<span class="string">"metrics "</span>, lit(<span class="string">"UV"</span>)) </span><br><span class="line">dfPV.union(dfUV).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">//方式2</span></span><br><span class="line"><span class="keyword">val</span> filePath: <span class="type">String</span> = _</span><br><span class="line"><span class="comment">//预先分好组，当然如果dfPV和dfUV有过滤的操作的话（结果集较小），强行reuse就不太适合了</span></span><br><span class="line"><span class="keyword">val</span> df = spark.read.parquet(filePath).repartition($<span class="string">"userId"</span>) </span><br><span class="line"><span class="comment">//下面两个dfPV和dfUV，就会复用df</span></span><br><span class="line"><span class="keyword">val</span> dfPV = df.groupBy(<span class="string">"userId"</span>).agg(count(<span class="string">"page"</span>).alias(<span class="string">"value"</span>)).withColumn(<span class="string">"metrics"</span>, lit(<span class="string">"PV"</span>))</span><br><span class="line"><span class="keyword">val</span> dfUV = df.groupBy(<span class="string">"userId"</span>).agg(countDistinct(<span class="string">"page"</span>).alias(<span class="string">"value"</span>)).withColumn(<span class="string">"metrics "</span>, lit(<span class="string">"UV"</span>)) </span><br><span class="line">dfPV.union(dfUV).show()</span><br></pre></td></tr></table></figure>
<p>前提条件：</p>
<ul>
<li>多个查询所依赖的<strong>分区规则要与 Shuffle 中间数据的分区规则</strong>保持一致</li>
<li>多个查询所涉及的字段（Attributes）要保持一致</li>
</ul>
</li>
<li><p>Spark集群机器选型一般依据是什么？</p>
<ul>
<li>如果你的计算场景涉及到大量的聚合、排序、哈希计算、数值计算等等，那么你的机器配置就要加强CPU；</li>
<li>如果你的计算场景需要反复消耗同一份或是同一批数据集，比如机器学习、数据分析、图计算，那么为了把需要频繁访问的数据缓存进内存，你自然需要加大内存配置；</li>
<li>如果你的计算场景会引入大量shuffle，又不能通过广播来消除Shuffle，那么你就需要配置足够的SSD以及高吞吐网络。</li>
</ul>
</li>
<li><p>建模分析时，判断一批IP地址是否在原始海量的IP段范围中，性能不理想。抽象为非等值JOIN的优化？</p>
<p>分析物理算子org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec，扩展出一个BroadcastRangeJoinExec算子，通过给那个小表做索引并排序，Join时候就不需要每一条都扫描，只扫描一部分就可以了。<a href="https://github.com/apache/spark/pull/21109" target="_blank" rel="noopener">SPARK-24020</a></p>
</li>
<li><p>建模分析时，少量重点人员账号与原始数据进行碰撞，性能不理想。在大小表基于非分区字段join时，大表读取的数据过多，执行性能较差？</p>
<p>可以采用Runtime Filter的方式，它的原理和DPP（动态分区裁剪）类似，因为DPP要求你的Join条件中包含了分区字段才会开启DPP，Runtime Filter可以把一些非分区字段条件形成一个filter放到大表上，类似传统数据库Query Rewrite，如果表大表能够过滤较多数据，从而可以提高JOIN的性能。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">IndexJoinRule</span>(<span class="params">sparkSession: <span class="type">SparkSession</span></span>) </span></span><br><span class="line"><span class="class">				<span class="keyword">extends</span> <span class="title">Rule</span>[<span class="type">LogicalPlan</span>] </span></span><br><span class="line"><span class="class">				<span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(plan: <span class="type">LogicalPlan</span>):<span class="type">LogicalPlan</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> convertedPlan = applyIndexJoinRule(<span class="type">Plan</span>)</span><br><span class="line">        convertedPlan    </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">applyIndexJoinRule</span></span>(plan: <span class="type">LogicalPlan</span>) = plan transform &#123;</span><br><span class="line">        <span class="comment">// select a.col1, b.col2</span></span><br><span class="line">        <span class="keyword">case</span> j<span class="meta">@Join</span>(</span><br><span class="line">            left<span class="meta">@Project</span>(_,lFilter<span class="meta">@Filter</span>(lConditon,_))),</span><br><span class="line">        	right<span class="meta">@Proejct</span>(_, rFilter<span class="meta">@Filter</span>(rCondition, _)),_,</span><br><span class="line">        	<span class="type">Some</span>(joinCond),_) =&gt;</span><br><span class="line">        	convertInnerJoin(j, left, lFilter,lConditon, </span><br><span class="line">                             right, rFilter,rConditon, joinCond)</span><br><span class="line">        <span class="comment">// select a.*, b.col2</span></span><br><span class="line">        <span class="keyword">case</span> j<span class="meta">@Join</span>(</span><br><span class="line">            left<span class="meta">@Filter</span>(lCondition, _), </span><br><span class="line">            right<span class="meta">@Project</span>(_, rFilter<span class="meta">@Filter</span>(rCondtion, _)),_,</span><br><span class="line">        	<span class="type">Some</span>(joinCond), _) =&gt;</span><br><span class="line">        	convertInnerJoin(j, left, lFilter,lConditon, </span><br><span class="line">                             right, rFilter,rConditon, joinCond)</span><br><span class="line"> 		<span class="comment">//其他计划规则</span></span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">convertInnerJoin</span></span>(j: <span class="type">Join</span>,</span><br><span class="line">                                left: <span class="type">LogicalPlan</span>,</span><br><span class="line">                                lFilter: <span class="type">Filter</span>,</span><br><span class="line">                                lConditon： <span class="type">EXpression</span>，</span><br><span class="line">                                right: <span class="type">LogicalPlan</span>,</span><br><span class="line">                                rFilter: <span class="type">Filter</span>,</span><br><span class="line">                                rCondition: <span class="type">Expression</span>,</span><br><span class="line">                                joinCond: <span class="type">Expression</span>) : <span class="type">LogicalPlan</span> = &#123;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//基于统计信息和配置来确定是否转换logicalplan</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">//将小表的索引列值查询出来追加到大表的过滤条件</span></span><br><span class="line">        <span class="comment">//小表结果</span></span><br><span class="line">        <span class="keyword">val</span> result = &#123;</span><br><span class="line">            <span class="keyword">val</span> qe = sparkSession.sessionState.executePlan(planToQuery)</span><br><span class="line">            qe.assertAnalyzed()</span><br><span class="line">            <span class="keyword">new</span> <span class="type">DataSet</span>[<span class="type">Row</span>](qe, <span class="type">RowEncoder</span>(qe.analyzed.schema))</span><br><span class="line">        &#125;.collect().map&#123;row=&gt;<span class="type">Literal</span>.create(row.get(<span class="number">0</span>), joinCol.dataType)&#125;.distinct</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">val</span> newJoin = &#123;</span><br><span class="line">            <span class="comment">//转为in条件</span></span><br><span class="line">            <span class="keyword">val</span> inCond = <span class="type">In</span>(getJoinAttr(joinCond， right).get, result)</span><br><span class="line">            <span class="keyword">val</span> newCond = <span class="type">And</span>(rCondition, inCond)</span><br><span class="line">            <span class="keyword">val</span> newRight = right.transform&#123;</span><br><span class="line">                <span class="keyword">case</span> f<span class="meta">@Filter</span>(_,_) =&gt; f.copy(condition = newCond)</span><br><span class="line">            &#125;</span><br><span class="line">            j.copy(right = newRight)</span><br><span class="line">        &#125;</span><br><span class="line">        newJoin.copy(left, right)</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol start="7">
<li><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">org.apache.spark.shuffle.FetchFailedException: </span><br><span class="line">Failed to send RPC XXX to /xxx:<span class="number">7337</span>:java.nio.channels.ColsedChannelException</span><br></pre></td></tr></table></figure>
<ul>
<li>原因：external shuffle服务将数据发送给container时，发现container已经关闭连接，出现该异常应该和org.apache.spark.shuffle.FetchFailedException: Connection from /xxx:7337 closed同时出现；</li>
<li>解决方案：参考org.apache.spark.shuffle.FetchFailedException: Connection from /xxx:7337 closed的解决方案。</li>
<li>进一步补充：<ul>
<li>在验证中发现关闭参数spark.shuffle.readHostLocalDisk，可以规避该异常的出现；</li>
<li>顺着上述参数发现在spark3.0.0中org.apache.spark.network.shuffle.ExternalBlockStoreClient#getHostLocalDirs指定rpc操作后默认关闭了RPC client，导致后续其他任务使用该client时出现已经关闭的情况。排查发现社区在<a href="https://issues.apache.org/jira/browse/SPARK-32663" target="_blank" rel="noopener">SPARK-32663</a>已经修复了该问题。</li>
</ul>
</li>
</ul>
</li>
<li><p>如何加快netty堆外内存的回收？snappy+parquet格式数据会导致，netty堆外内存增长太快，导致netty使用过多direct memory报错？</p>
<p>首先，io.netty.util.internal.OutOfDirectMemoryError: failed to allocate 16777216 byte(s) of direct memory (used: 7633633280, max: 7635730432)，这个问题，往往出现在Shuffle read阶段，spark用netty的client/server来拉取远端节点数据，并且透过java.nio.DirectByteBuffers来缓存接收到的数据块。当数据分布存在比较严重的倾斜问题的时候，就会导致某些Block过大，从而导致单个线程占用的Direct Buffer超过16MB，从而报出上面的错误。</p>
<p>因此，要从根本上解决问题，可以先搞定数据倾斜的问题，如果数据倾斜消除了，那么这个问题大概率自己就会消失掉。关于消除数据倾斜的方法，可以参考后面AQE那几讲，以及两阶段Shuffle那一讲。</p>
<p>接下来，假设你消除了Data Skew之后，这个报错还在，那么就继续用下面的办法。DirectByteBuffers默认的大小就是spark.executor.memory的大小，也就是说，它在逻辑上，会“计入”Executor memory内存的消耗。spark.executor.memory这玩意其实指定的JVM heap堆内的内存大小，而DirectByteBuffers是堆外内存，按理说两者应该区别对待，然而默认情况下，并没有。因此，如果DirectByteBuffers消耗非常的过分，那么我们可以在spark.executor.extraJavaOptions当中，特意地去指定-XX:MaxDirectMemorySize这个参数，这个参数，就是用来指定DirectByteBuffers的内存大小，可以把它设置的大一些。</p>
<p>再者，假设上面的设置，还不能解决问题，那么接下来，我们就得做进一步的精细化调优。首先，把spark.reducer.maxSizeInFlight，设置成-XX:MaxDirectMemorySize / spark.executor.cores ，这个设置的意图，是降低每个线程需要缓存的数据量。然后，把spark.maxRemoteBlockSizeFetchToMem，设置成spark.reducer.maxSizeInFlight / 5，这个设置的意图，是为了把大的Block直接落盘，从而迅速释放线程占用的Direct buffer，降低Direct buffer（也就是堆外内存）的消耗，从而降低OOM的风险。</p>
</li>
<li><p>java.lang.OutOfMemoryError: GC overhead limit exceeded</p>
<ul>
<li>原因：数据量太大，内存不够。</li>
<li>解决方案：<ul>
<li>增大spark.executor.memory的值，减小spark.executor.cores</li>
<li>减少输入数据量，将原来的数据量分几次任务完成，每次读取其中一部分</li>
</ul>
</li>
</ul>
</li>
<li><p>ERROR An error occurred while trying to connect to the Java server (127.0.0.1:57439) Connection refused</p>
<ul>
<li>原因：<ul>
<li>节点上运行的container多，每个任务shuffle write到磁盘的量大，导致磁盘满，节点重启 </li>
<li>节点其他服务多，抢占内存资源，NodeManager处于假死状态</li>
</ul>
</li>
<li>解决方案：<ul>
<li>确保节点没有过多其他服务进程 </li>
<li>扩大磁盘容量 </li>
<li>降低内存可分配量，比如为总内存的90%，可分配内存少了，并发任务数就少了，出现问题概率降低 </li>
<li>增大NodeManager的堆内存</li>
</ul>
</li>
</ul>
</li>
<li><p>org.apache.spark.shuffle.FetchFailedException: Failed to connect to /9.4.36.40:7337</p>
<ul>
<li>背景：shuffle过程包括shuffle read和shuffle write两个过程。对于spark on yarn，shuffle write是container写数据到本地磁盘(路径由core-site.xml中hadoop.tmp.dir指定)过程； shuffle read是container请求external shuffle服务获取数据过程，external shuffle是NodeManager进程中的一个服务，默认端口是7337，或者通过spark.shuffle.service.port指定。</li>
<li>定位过程：拉取任务运行日志，查看container日志；查看对应ip上NodeManager进程运行日志，路径由yarn-env.sh中YARN_LOG_DIR指定。</li>
<li>原因：container请求NodeManager上external shufflle服务，不能正常connect，说明NodeManager可能挂掉了，原因可能是：<ul>
<li>节点上运行的container多，每个任务shuffle write到磁盘的量大，导致磁盘满，节点重启</li>
<li>节点其他服务多，抢占内存资源，NodeManager处于假死状态</li>
</ul>
</li>
<li>解决方案：<ul>
<li>确保节点没有过多其他服务进程 </li>
<li>扩大磁盘容量 </li>
<li>降低内存可分配量，比如为总内存的90%，可分配内存少了，并发任务数就少了，出现问题概率降低</li>
<li>增大NodeManager的堆内存</li>
</ul>
</li>
</ul>
</li>
<li><p>spark任务中stage有retry</p>
<ul>
<li>原因：<ul>
<li>下一个stage获取上一个stage没有获取到全部输出结果，只获取到部分结果，对于没有获取的输出结果retry stage以产出缺失的结果；</li>
<li>部分输出结果确实已经丢失 ，部分输出结果没有丢失，只是下一个stage获取结果超时，误认为输出结果丢失。</li>
</ul>
</li>
<li>解决方案：<ul>
<li>针对原因(1)，查看进程是否正常，查看机器资源是否正常，比如磁盘是否满或者其他；</li>
<li>针对原因(2)，调大超时时间，如调大spark.network.timeout值。</li>
</ul>
</li>
</ul>
</li>
<li><p>Final app status: FAILED, exitCode: 11, (reason: Max number of executor failures (200) reached)</p>
<ul>
<li>原因：executor失败重试次数达到阈值</li>
<li>解决方案：<ul>
<li>调整运行参数，减少executor失败次数；</li>
<li>调整spark.yarn.max.executor.failures的值，可在spark-defaults.conf中调整。确定方式，在日志中搜索”Final app status:”，确定原因，在日志统计”Container marked as failed:”出现次数。</li>
</ul>
</li>
</ul>
</li>
<li><p>task反复调度到有问题的executor？</p>
<p>通过这些黑名单的设置可以避免由于 task 反复调度在有问题的 executor/node （坏盘，磁盘满了，shuffle fetch 失败，环境错误等）上，进而导致整个 Application 运行失败的情况。</p>
</li>
</ol>
<h3 id="扩展阅读"><a href="#扩展阅读" class="headerlink" title="扩展阅读"></a>扩展阅读</h3><ul>
<li><a href="https://mp.weixin.qq.com/s/_KWwb80dlkEtu1SeRK4jqw" target="_blank" rel="noopener">Apache Spark 完全替代传统数仓的技术挑战及实践</a>，马刚@eBay，大数据团队成员</li>
<li><a href="https://time.geekbang.org/column/intro/400" target="_blank" rel="noopener">Spark性能调优实战</a>，吴磊，FreeWheel机器学习团队负责人</li>
<li><a href="https://databricks.com/blog" target="_blank" rel="noopener">https://databricks.com/blog</a></li>
<li><a href="https://www.yuque.com/tonyshu/gnfipq/tbeihk" target="_blank" rel="noopener">Spark性能调优指南笔记</a>，笔者</li>
</ul>
</div><iframe src="/donate/?AliPayQR=/uploads/alipay.png&amp;WeChatQR=null&amp;GitHub=null&amp;BTCQR=null&amp;BTCKEY=null&amp;PayPal=null" style="overflow-x:hidden; overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;" frameborder="0" scrolling="no"></iframe><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="http://changbo.tech/blog/19c2ab93.html" data-id="cktlq5e6v00206nvxmwh1odc5" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAK4AAACuCAAAAACKZ2kyAAAByElEQVR42u3aQY6DMBAEwPz/0+w10gbT48FWiMqnCAKUObTsGV6veBxv4/3I+Gx+/OaBi4vb5h7D8R83viqfQHL2w5RwcXE3cs8SY/z7DDeeWHLP0+O4uLgP4VZxuLi4v8RNJpBAcXFxn8Ktbmbmprd1r4aLi9vg5lXKdb+X1HdxcXGnuEdxjAOreofytbi4uFu4c62Us8VHskCpHv8Qebi4uFu4SWFiXO5MFkZJUTUpreLi4q7mdtqlc1umpKFysSLDxcVdzE2ipx9w1SjExcX9Bu44SsYPrv4nv/ai84OLi7uMm0CT/KteWy6C4OLibuTmpYpqD/Sul1L4ZgQXF/cmbt4U6W+HqpO56Png4uIu4yafXvVLotWlVSF3cXFxl3GrG558MZQUQaoLKVxc3D3c6gPy0JlrlhS2Ybi4uMu4c+2TPOaqE4saKri4uBu5ndZp9Q5JmBZWZLi4uDdxj+KY63XmgXVxZ1xc3C3cdQ3UJEGr2yRcXNyd3P52JS+jdOobuLi4+7mdBmoeWP3oxMXF/U5uFT1XHsXFxX0it18Snfv8CxcXdye3+klEP7aq6YSLi7uTW42SvMkxRicvolXfxcXFneH+ASHXAS8FWtx9AAAAAElFTkSuQmCC">分享</a><div class="tags"><a href="/tags/Spark/">Spark</a></div><div class="post-nav"><a class="pre" href="/blog/ce4ffa9b.html">Hive性能调优实践</a><a class="next" href="/blog/c50a937d.html">PMP实践之路</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/个人日志/">个人日志</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/云原生/">云原生</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/分布式/">分布式</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/存储引擎/">存储引擎</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据库与大数据/">数据库与大数据</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据结构与算法/">数据结构与算法</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/服务器/">服务器</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/编程语言/">编程语言</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/计算引擎/">计算引擎</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/论文阅读/">论文阅读</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/读书笔记/">读书笔记</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/运维工具/">运维工具</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/项目管理/">项目管理</a><span class="category-list-count">1</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/SQL引擎/" style="font-size: 15px;">SQL引擎</a> <a href="/tags/B-树/" style="font-size: 15px;">B+树</a> <a href="/tags/职场感悟/" style="font-size: 15px;">职场感悟</a> <a href="/tags/技术管理/" style="font-size: 15px;">技术管理</a> <a href="/tags/开源协议/" style="font-size: 15px;">开源协议</a> <a href="/tags/Drill/" style="font-size: 15px;">Drill</a> <a href="/tags/大数据/" style="font-size: 15px;">大数据</a> <a href="/tags/MPP/" style="font-size: 15px;">MPP</a> <a href="/tags/查询计划/" style="font-size: 15px;">查询计划</a> <a href="/tags/HBase/" style="font-size: 15px;">HBase</a> <a href="/tags/NoSQL/" style="font-size: 15px;">NoSQL</a> <a href="/tags/Hadoop/" style="font-size: 15px;">Hadoop</a> <a href="/tags/InnoDB/" style="font-size: 15px;">InnoDB</a> <a href="/tags/RocksDB/" style="font-size: 15px;">RocksDB</a> <a href="/tags/LSM树/" style="font-size: 15px;">LSM树</a> <a href="/tags/存储引擎/" style="font-size: 15px;">存储引擎</a> <a href="/tags/Hive/" style="font-size: 15px;">Hive</a> <a href="/tags/Nginx/" style="font-size: 15px;">Nginx</a> <a href="/tags/正向代理/" style="font-size: 15px;">正向代理</a> <a href="/tags/反向代理/" style="font-size: 15px;">反向代理</a> <a href="/tags/OLAP/" style="font-size: 15px;">OLAP</a> <a href="/tags/Java/" style="font-size: 15px;">Java</a> <a href="/tags/类加载器/" style="font-size: 15px;">类加载器</a> <a href="/tags/Kylin/" style="font-size: 15px;">Kylin</a> <a href="/tags/redis/" style="font-size: 15px;">redis</a> <a href="/tags/SQL/" style="font-size: 15px;">SQL</a> <a href="/tags/业务/" style="font-size: 15px;">业务</a> <a href="/tags/SparkSQL/" style="font-size: 15px;">SparkSQL</a> <a href="/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/tags/helm/" style="font-size: 15px;">helm</a> <a href="/tags/kubernetes/" style="font-size: 15px;">kubernetes</a> <a href="/tags/linux/" style="font-size: 15px;">linux</a> <a href="/tags/Calcite/" style="font-size: 15px;">Calcite</a> <a href="/tags/优化器/" style="font-size: 15px;">优化器</a> <a href="/tags/分布式事务/" style="font-size: 15px;">分布式事务</a> <a href="/tags/ACID/" style="font-size: 15px;">ACID</a> <a href="/tags/CAP/" style="font-size: 15px;">CAP</a> <a href="/tags/Raft/" style="font-size: 15px;">Raft</a> <a href="/tags/分布式一致性/" style="font-size: 15px;">分布式一致性</a> <a href="/tags/算法复杂度/" style="font-size: 15px;">算法复杂度</a> <a href="/tags/CBO/" style="font-size: 15px;">CBO</a> <a href="/tags/数据库/" style="font-size: 15px;">数据库</a> <a href="/tags/Intel/" style="font-size: 15px;">Intel</a> <a href="/tags/ARM/" style="font-size: 15px;">ARM</a> <a href="/tags/X86/" style="font-size: 15px;">X86</a> <a href="/tags/硬件/" style="font-size: 15px;">硬件</a> <a href="/tags/RAID/" style="font-size: 15px;">RAID</a> <a href="/tags/服务器/" style="font-size: 15px;">服务器</a> <a href="/tags/AnalyticDB/" style="font-size: 15px;">AnalyticDB</a> <a href="/tags/窗口函数/" style="font-size: 15px;">窗口函数</a> <a href="/tags/Catalyst/" style="font-size: 15px;">Catalyst</a> <a href="/tags/行列存储/" style="font-size: 15px;">行列存储</a> <a href="/tags/java/" style="font-size: 15px;">java</a> <a href="/tags/并发编程/" style="font-size: 15px;">并发编程</a> <a href="/tags/故障诊断/" style="font-size: 15px;">故障诊断</a> <a href="/tags/大数据meetup/" style="font-size: 15px;">大数据meetup</a> <a href="/tags/博客/" style="font-size: 15px;">博客</a> <a href="/tags/IT资讯/" style="font-size: 15px;">IT资讯</a> <a href="/tags/架构/" style="font-size: 15px;">架构</a> <a href="/tags/论文/" style="font-size: 15px;">论文</a> <a href="/tags/书籍/" style="font-size: 15px;">书籍</a> <a href="/tags/TopK/" style="font-size: 15px;">TopK</a> <a href="/tags/数据结构/" style="font-size: 15px;">数据结构</a> <a href="/tags/算法/" style="font-size: 15px;">算法</a> <a href="/tags/正则表达式/" style="font-size: 15px;">正则表达式</a> <a href="/tags/JVM/" style="font-size: 15px;">JVM</a> <a href="/tags/GC/" style="font-size: 15px;">GC</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/blog/2e4fb37a.html">极客邦连麦百位牛人观后实录</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/6d27f500.html">Java类加载机制</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/b9b847a8.html">SparkSQL业务分析集锦</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/bfe7107d.html">OLAP引擎-Kylin基本介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/ce4ffa9b.html">Hive性能调优实践</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/19c2ab93.html">Spark性能调优实战</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/c50a937d.html">PMP实践之路</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/7dec2e4.html">Calcite处理和扩展流程解析</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/4c70dee6.html">分布式事务与一致性</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/75c48487.html">2020-DTCC-参会分享</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://changbo.tech/blog/ec4cdf4c.html" title="行业博客" target="_blank">行业博客</a><ul></ul><a href="https://leetcode-cn.com/" title="leetcode" target="_blank">leetcode</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/." rel="nofollow">Tony's Notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>