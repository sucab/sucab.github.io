<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>极客邦连麦百位牛人观后实录</title>
    <url>/blog/2e4fb37a.html</url>
    <content><![CDATA[<p><strong>从书上学·在事中练·与高人聊！心力!</strong></p>
<p>极客邦访谈100位牛人，通过直播方式为程序员提供知识服务，从技术、架构、业务、管理、思想等多方面来给正在路上的技术人提供一些参考，解答一些疑惑。现将听取的部分大牛的讲话内容记录下来，以此来总结反思自己。</p>
<h3 id="张雪峰"><a href="#张雪峰" class="headerlink" title="张雪峰"></a>张雪峰</h3><p>2005年至2011年，张雪峰曾在微软任职超过5年，之后为携程国际事业部CTO。2015年3月，受时任饿了么联合创始人兼产品负责人汪渊的邀请，张雪峰正式加入饿了么担任首席技术官。2020年8月，张雪峰刚刚调任阿里巴巴本地生活服务公司CEO助理一职。</p>
<p>今天的访谈由于个人是带着耳机在路上听的，聊的内容也有些宽泛，所以也没有get到一些自己关注的点，于是就找了一下极客时间关于张老师的一些介绍和专栏信息，后面会附上链接。</p>
<p>整体的访谈对话上来看，张老师是一个比较谦虚、直白、“负面性”的人，看问题的level也比较高。毕竟在饿了么搭建并领导上千号的技术团队，承载饿了么日均千万级订单量的基础架构。也回答了以下几个问题？</p>
<ul>
<li><p>如何看待架构？</p>
<p>他认为后端架构层面的东西，没有标准化好坏之分，关键是要在实践中去检验它是否合理。</p>
</li>
<li><p>技术驱动业务？</p>
<p>国内大部分公司不要想着技术来驱动业务，还达不到那样的level，除了google等这样的公司。优先做小而美的业务，做的深入了再说，要有核心竞争力和差异化，然后才是广度。不要一上来就要蕴量一个颠覆性的产品，超越微信、抖音之类的。</p>
</li>
<li><p>国内和国外程序员差异？</p>
<p>单从工程师文化来看，国内的人其实更加适合，只是过去大环境所导致的。现在已经越来越向好的方向发展，不一定非要去硅谷才能发展，谈到了像左耳朵耗子、黄东旭等大佬的成功经历。</p>
</li>
<li><p>研发效能问题？</p>
<p>张老师提到，还是要看最终的结果以及业务价值等。而不是想着如何提高效率，来提升业绩，比如一些规模在50人以内的公司。当时在饿了么，技术的团队为了检验产品提出的需求有效性，跟踪上线后的效果，最终也没有一个相对合理的衡量标准，特别是过了业务增长期了，如何迭代产品特性，后来也是不了了之了。</p>
</li>
</ul>
<p>最后，张老师给出建议，追寻自己的内心，抓住自己的兴趣点深入下去就可以了。</p>
<p>参考链接：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/426886113" target="_blank" rel="noopener">前饿了么 CTO 张雪峰：如何从程序员走到CTO？</a></li>
<li>极客专栏，<a href="https://time.geekbang.org/column/intro/100095401" target="_blank" rel="noopener">《超级访谈：对话张雪峰》</a></li>
</ul>
<h3 id="周志明"><a href="#周志明" class="headerlink" title="周志明"></a>周志明</h3><p>周志明老师，就职于华为从事架构师、研发管理等工作，个人详情查看<a href="https://lk.linkedin.com/in/icyfenix" target="_blank" rel="noopener">领英</a>链接或<a href="http://icyfenix.cn/" target="_blank" rel="noopener">个人网站</a>，很多人认识他的应该是通过《深入理解Java虚拟机》这本书。本次连麦讨论的内容主要围绕参与者在线上提出的内容展开，以下记录整理部分个人比较关注的内容。</p>
<ul>
<li><p>Java在云原生下的趋势？</p>
<p>过去Java提到一次编译多次运行，也就是跨平台特性是相对于C/C++这样的语言来说的，依赖底层操作系统、芯片架构等。然而，现如今K8s作为一个云操作系统，通过调度Pod方式运行多个实例，天生已经做到了屏蔽底层硬件或操作系统。反而，Java的运行环境所要求的包变得更加重，某种程度来说这也使得像Go\Rust等后端语言变得更加兴起。</p>
<p>因此，未来10年，随着微服务、云原生、云计算的发展，新的云原生语言可能会有更好的市场。也就出现了两个方向代表：一个是JVM系：Java/Scala，另一个是云原生系：Go/Rust。</p>
</li>
<li><p>如何看待ToC和ToB发展趋势？</p>
<p>国外互联网时代的发展，出现了Google这样的ToC公司（当然也有ToB），同时也出现了Oracle这样的ToB的公司。反观国内互联网的发展ToC领域，也出现了很多头部公司。但是ToB领域，还没有看到。此外，像编程语言、基础软件几乎都是国外的。当然，周老师也比较看好国内未来10年的发展，随着国家在人才、经济、调控等政策向ToB企业倾斜，包括信创战略，ToB领域的公司会迎来类似过去10年ToC互联网式的发展。</p>
<p>此外，做ToB如何和做ToC的互联网一线技术保持同步，周老师认为需要自身主动去寻求沟通，做好技术的交流，形成互补。</p>
</li>
<li><p>怎么理解架构？如何看待业务架构和技术架构？</p>
<p>过去谈架构，更多是在业务架构、应用架构、数据架构以及技术架构的纬度去讨论，而在当下会基于微服务、云原生的思路来讨论架构。微服务强调全生命周期的管理，而不是过去测试、开发、SRE等细化的分工。</p>
<p>优秀的业务架构师，大多也是合格的技术架构师。但优秀的技术架构师，就不一定是合格的业务架构师了，贴近业务或许要多走几步，比如像ToB方向。</p>
</li>
<li><p>如何构建知识体系？</p>
<p>这个问题也可以换个说法，知识不成体系怎么办？广度和深度的平衡？知识体系需要通过知识的积累来构建，而技能更多是用于工作。</p>
<p>首先，是要构建金字塔基座，也就是广度，比如对计算机领域的关键核心知识的掌握，比如计算机体系架构、操作系统等。其次，是要构建金字塔塔尖，也就是深度，比如分布式领域。</p>
<p>如何验证知识是否已经成为体系，那就需要讲自己认为掌握的去进行输出，比如博客、演讲，让别人去理解你的阐述，并提出质疑和讨论，在这个过程中去完善自己其实还不太明白的地方，夯实知识体系。</p>
<p>作为有多年工作经验的人来说，关键是要提高自己接受新知识的加速度。</p>
</li>
<li><p>技术与管理？</p>
<p>周老师提到AWS的贝索斯的一些理念，认为8-12人的团队是最为健康的团队。对于一个技术团队来说，技术管理者出身技术，同时能带领团队发展是比较和谐的，相比于一个纯管理者来说，有先天的优势，比如技术上的认可、关键问题的指导。</p>
</li>
<li><p>如何看待算法的学习？</p>
<p>周老师认为算法是一种锻炼逻辑思维的方式，要成为一种习惯或常态化的内容，甚至是茶余饭后的一种消遣，而不是一种负担。</p>
</li>
<li><p>推荐书籍？很经典值得一看！！！</p>
<p>《<a href="https://book.douban.com/subject/26912767/" target="_blank" rel="noopener">深入理解计算机系统</a>》、《<a href="https://book.douban.com/subject/27096665/" target="_blank" rel="noopener">现代操作系统</a>》、《<a href="https://book.douban.com/subject/30329536/" target="_blank" rel="noopener">数据密集型应用系统设计</a>》</p>
<p>《<a href="https://book.douban.com/subject/35492898/" target="_blank" rel="noopener">凤凰架构</a>》、《<a href="https://book.douban.com/subject/30379536/" target="_blank" rel="noopener">智慧的疆界</a>》</p>
</li>
</ul>
<p>阅读经典的书籍，广度和深度的平衡，知识体系的构建，技术前沿的把握与思考，内在的兴趣要大于自律的约束，向前辈们学习。</p>
<h3 id="沈剑"><a href="#沈剑" class="headerlink" title="沈剑"></a>沈剑</h3><p>沈架老师，08-09年百度工程师，专注于后端即时通信领域。11年到58同城商家平台，做即时通信的平台，从0起步开始后端架构搭建，角色从架构师到负责人，带领团队5-10人。后期，轮岗为各个业务做架构设计。15年转到到家集团的58到家，从刚开始的5个人，到了16-17年团队发展达到了100人，承担整个技术体系建设。公众号：架构师之路。</p>
<p>本次访谈主要以技术管理为主题，我摘录了部分关注的话题内容？</p>
<ul>
<li><p>对各个层级管理者的理解？</p>
<p>一线管理者：腿部力量，要落地。10人以内，也会参与核心架构涉及、代码编写。</p>
<p>二线管理者(总监)：腰部力量，上传下达。4-5个人，下面核心骨干和架构师，关注公司重点方向。</p>
<p>技术VP：脑部力量。需要下面总监给反馈一线那一侧的情况。来构思一些能力解决一线的问题。比如技术选型，包括运维、开发、测试的熟悉程度，人力成本之类的。</p>
</li>
<li><p>技术和管理的取舍？</p>
<p>技术管理者，一线和二线是需要懂技术的，需要帮助员工解决问题，但不需要是专家。自己懂得就解决，不懂得就需要协调资源协助解决。新的前沿技术是需要去关注的。绝大部分的精力在团队目标，沟通，规划方面。深入一个技术傍身和带好一个团队还是有很大区别的。</p>
</li>
<li><p>员工做事拖沓如何处理？</p>
<p>设定清晰的个人和团队的目标，需要结果导向。特殊的情况是需要鼓励员工，价值认同感，但不能长期处于这样。设置合理的OKR，帮助团队成长和提升。如果团队超过7个人，必须要设置一个1。</p>
<p>10%-20%技术驱动业务，主动去想的。其他大部分时间在交付。技术团队为交付团队负责，如何技术团队评价优？比如固定的时间段内能迭代并交付多少需求，也依赖一些研发效率工具的，收集各个指标，各个团队之间的横向对比。</p>
</li>
<li><p>如何培养人才梯队？</p>
<p>提拔271中的头部同学，以及他们的意愿。让对方来做自己正在做的事情，比如80%的事情必须要做，可以让对方锻炼尝试去做。</p>
<p>权威：按照我说的做，最高效，但要能很确信自己的方案；</p>
<p>支持：帮助协调资源；</p>
<p>教练：辅导指名一些方向；</p>
<p>甩手张掌柜：不管不顾，能力达到一定程度；</p>
</li>
<li><p>架构师如何引导新人？</p>
<p>新人提出方案，可以给出优劣势，引导新人提出自己的想法，提升新人的架构思维；</p>
<p>如果架构师给出根据我的经验，就这么办来回应的话。员工积极性打压，而架构师本身长期也会思维定势和固化；</p>
<p>关键问题一定要专家决策而不是管理者决策。</p>
</li>
<li><p>技术成长？</p>
<p>技术，架构，团队，演讲，写文章。自己认准的事儿会全力以赴。比如，写文章的动因？输出倒逼输入，前期调研、梳理、画图、系统性总结，从而提升了笔者本身。时刻问内心是否喜欢，或帮助他人，做原创，持续做下去。</p>
</li>
</ul>
<p>技术人做到一定阶段，不要纠结于所有技术的全面性，需要把早期所在领域积累好后，要提高技术视野以及架构思维、组织思维的能力，协调组织其他领域的专家来解决不是自己擅长的问题，从而完成整体的目标。同时，技术人也要保持技术的前瞻性和战略，带领好团队做好规划技术和产品，支撑业务，驱动业务向前发展，设置合理的OKR，激发员工，快乐高效工作。</p>
<h3 id="王保平"><a href="#王保平" class="headerlink" title="王保平"></a>王保平</h3><p>阿里前端大牛P10，花名玉伯。之前在软件所写.net, c#写cs端软件。08年加入淘宝, 13年时间。08-12: 写代码, 交易项目(至暗时刻, 创新项目失败)。13-18: 做技术产品, 到蚂蚁做可视化, 技术平台, 语雀。19-至今: 管理, 帮助别人成事, 做产品的心态带团队.  早期以事情驱动团队, 越来越大,重新思考. 产品架构\组织结构\团队规划与职责。关键词: 成事能力(包括帮助团队), 要保持谦卑心态。守正出奇:  守住当下,服务好业务，同时投资未来。(比如华为海思)。</p>
<p>本次访谈，主持人基于自己的感悟、听友的提问和玉伯讨论了以下几个问题？</p>
<ul>
<li><p>数据可视化? 相关趋势?</p>
<p>可视化: 数据分析\BI 智能可视化 AIoT   canavs(性能)  svg(兼容)  webAssembly(硬件)  webGL</p>
<p>智能化: 在端侧跑tensorflow, 智能UI(千人千面)</p>
<p>低代码: 后续成千上万的saas系统的开发问题, 人员开发和平台自动化, 降低开发门槛.</p>
<p>云原生</p>
</li>
<li><p>低代码和无代码?</p>
<p>低代码降低saas开发门槛,  一种针对垂直领域(逐渐增多, 小型创业公司在做), 一种是通用的,比如蚂蚁云蝶凤舞等, 对接后端接口生成应用(较少). 更多是给后端开发的人或生态用工使用的,  而针对专业的前端人员, 依旧要采用代码解决专业的问题. </p>
</li>
<li><p>作为前端团队leader需要考虑什么? </p>
<p>支撑好公司业务, 技术的先进性(5-10年), 团队效能(快乐工作)</p>
<p>定义OKR, 关注目标. 中期问卷,关注团队人员的满意度.  以事聚人, 因人成事. </p>
</li>
<li><p>在公司太安逸,如何改变?</p>
<p>需要寻求突破, 优先内部寻找机会.</p>
</li>
<li><p>如何学习?评判人员能力?</p>
<p>找经典书籍去看, 比如AI, 用python写demo. 或者线上一些课程.</p>
<p>技术能力, 潜力(学习能力, 成事欲望), 开放度(保持好奇心, 与人沟通) </p>
</li>
<li><p>如何看待新技术?</p>
<p>主要跟当前工程化工作有关, 能否解决问题.</p>
</li>
<li><p>P系列各阶段要求?</p>
<p>p5-6: 关注技术的深度和广度;</p>
<p>p7-8: 技术+业务.  将业务问题转化为专业问题去解决, 以及基于技术做好业务架构;</p>
<p>p9-10: 全局视野, 让别的人1 + 自己的 1 &gt; 2, 形成更大的价值, 关注那个+号整合的能力.</p>
</li>
<li><p>推荐书籍?</p>
<p>被讨厌的勇气、象与骑象人、思考快与慢</p>
</li>
<li><p>个人签名：因上努力, 果上随缘，自己的做事观?</p>
<p>努力做好当下的事情。但行好事, 莫问前程。</p>
</li>
</ul>
<p>从玉伯的谈话中，感悟到个人要从自身的因去出发，向内求索，每个阶段做好每个阶段的事情，锻炼自己的心力，寻求自己的突破。</p>
<h3 id="吴其敏"><a href="#吴其敏" class="headerlink" title="吴其敏"></a>吴其敏</h3><p>现任平安银行首席架构师·20年互联网老兵，具有eBay、大众点评、携程、平安银行四家公司的从业经历。他认为每换一个家公司，都要通过做事情来证明自己，构架自己的技术影响力，而不是作为领导来命令大家。作为开源的分布式监控系统<a href="http://github.com/dianping/cat" target="_blank" rel="noopener">CAT</a>主要推动者，一直致力于监控领域的能力建设、架构设计和团队管理工作。</p>
<p>本次访谈，主持人基于听友的提问和吴架讨论了以下几个问题？</p>
<ul>
<li><p>平安银行的基础架构技术栈？</p>
<p>中间件部分涉及RPC能力Dubbo、消息队列RocketMQ和Kafka、高速缓存Redis、数据库访问ShardingSphere、数据库MySQL、配置中心Apollo、监控中心CAT、调度和网关等。</p>
<p>同时，还谈到了对分布式事务的理解。事务强调一种统一性、一致性，分布式事务有一定的应用场景，但并不会在大范围内使用，性能会受到局限。</p>
</li>
<li><p>大数据的前景？</p>
<p>认为批处理在当下仍然具有一定的应用场景，但更加看好“实时处理”的未来。他认为，对于业务来说有些信息的时效性分钟级相对于秒级，它的价值就会大打折扣。同时，他也比较关注CAT在采集端输出海量数据的能力，从而给各方去做分析处理、可视化展示。</p>
</li>
<li><p>对于当下各种语言如何看待的？</p>
<p>分为两大类，强类型和弱类型。强类型，比如Java，适合大型系统，但维护成本高。弱类型，比如Python、JavaScript，适合快速迭代小系统。针对Java语言，动态编译、反射、字节码技术，都是非常好的特性，都在CAT中得到了体现。同时，后期也会比较关注Rust语言。</p>
</li>
<li><p>架构师应该具备什么样的能力？对架构的理解？</p>
<p>解决问题的能力，在多种解决方案中寻找合适的可以解决问题的方案。这里面涉及到的不仅仅是技术层面的，还有管理层面的。要具备在关键节点的攻坚能力，同时要业务协调、团队沟通（一组人的思想碰撞而非单个人的）、分工协作等等。总结两方面：技术+管理。</p>
<p>没有最好的架构，只有合适的架构。在做架构时，要根据业务、团队、技术、成本等多方面来考量，对于当下来说比较适合的带有时间属性的架构，来解决当下的最本质的痛点问题。当然，有了坚实的架构，后续的实施、维护各个环节都要得到保证。最后，他的架构名言就是“高内聚，低耦合”。</p>
</li>
<li><p>如何学习？推荐哪些书籍？</p>
<p>谈到学习，首先要打好基础（计算网络、数据结构、分布式系统、数据库等等），看书多看经典的书籍，提升思维，练内功，不要太拘泥于招式本身；其次要学会分享让别人能听懂，这样才能转化为自己的知识；最后是要多动手实践，整理总结。他选人的标准是基础好、有悟性、肯努力。总的来说，要保持对技术好奇心，根据自己的兴趣，选择某些领域，扎根到社区。</p>
<p>吴架认为要远离哪些凑页数的书，看再多都没有提升。对于他而言，要抓住基础。比如，<a href="https://docs.oracle.com/javase/specs/" target="_blank" rel="noopener">Java Language and Virtual Machine Specifications</a>、<a href="https://book.douban.com/subject/10945606/" target="_blank" rel="noopener">精益创业</a>。</p>
</li>
</ul>
<p>从吴架的访谈中，我个人体会到技术和管理的认知提升的重要性，是需要一点一滴逐步沉淀的。不管是做基础架构还是业务，在大的方向和整体目标没有偏差的前提下，还是要关注怎么做，而不是纠结于做什么。</p>
]]></content>
      <categories>
        <category>个人日志</category>
      </categories>
  </entry>
  <entry>
    <title>Java类加载机制</title>
    <url>/blog/6d27f500.html</url>
    <content><![CDATA[<h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>从事Java开发的小伙伴，在工作中一定遇到过如<code>ClassNotFoundException</code>、<code>NoClassDefFoundError</code>、<code>NoSuchMethodError</code>等等之类的异常或错误，大家肯定会想到类缺失了、Jar包有冲突了等原因。知其然更要知其所以然，本文将从Java类加载机制的原理层面来分析出现这些问题的原因，同时以一些基础框架为例来说明它们是如何规避的。以此来加深我们对Java类加载机制的了解，达到快速解决问题和开发基础工具时如何善用Java类加载器。</p>
<h3 id="类加载过程"><a href="#类加载过程" class="headerlink" title="类加载过程"></a>类加载过程</h3><p>一个Class的生命周期包括<strong>加载</strong>、<strong>连接（验证、准备、解析）</strong>、<strong>初始化</strong>、<strong>使用</strong>和<strong>卸载</strong>。其中，系统寻找Class文件的过程涉及加载、连接和初始化。</p>
<h4 id="加载：对应loadClass"><a href="#加载：对应loadClass" class="headerlink" title="加载：对应loadClass"></a>加载：对应loadClass</h4><p>主要是根据类路径读取二进制流，将静态信息加载到方法区，堆内存中创建Class对象来访问方法区的信息。这个里面在java实现里，涉及的调用链：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">this</span>.parent.loadClass-&gt;</span><br><span class="line"><span class="keyword">this</span>.findBootstrapClassOrNull-&gt;</span><br><span class="line"><span class="keyword">this</span>.findClass-&gt;</span><br><span class="line"><span class="keyword">this</span>.resolveClass</span><br></pre></td></tr></table></figure>
<h4 id="连接：对应resolveClass"><a href="#连接：对应resolveClass" class="headerlink" title="连接：对应resolveClass"></a>连接：对应resolveClass</h4><ul>
<li><strong>验证阶段</strong>是验证文件格式、元数据、字节码、符号引用，比如魔数校验、主次版本号、常量池常量类型、是否符合Java语言规范；</li>
<li><strong>准备阶段</strong>是正式为类变量分配内存并设置类变量初始值的阶段，这些内存都将在方法区中分配，比如静态基本数据类型的默认值；</li>
<li><strong>解析阶段</strong>是虚拟机将常量池内的符号引用替换为直接引用的过程。解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用限定符 7 类符号引用进行。<strong>也就是得到类或者字段、方法在内存中的指针或者偏移量</strong>。</li>
</ul>
<h4 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h4><p>执行初始化方法 <code>&lt;clinit&gt;()</code>方法的过程，是类加载的最后一步，这一步 JVM 才开始真正执行类中定义的 Java 程序代码(字节码)。<code>&lt;clinit&gt;()</code>方法是编译之后自动生成的，它是<strong>带锁线程安全</strong>（单例模式）。比如<code>new</code>一个类，读取一个静态字段(未被<code>final</code>修饰)、或调用一个类的静态方法时。</p>
<h4 id="卸载"><a href="#卸载" class="headerlink" title="卸载"></a>卸载</h4><p>卸载类即该类的 Class 对象被 GC。由 jvm 自带的类加载器加载的类是不会被卸载的。但是由我们自定义的类加载器加载的类是可能被卸载的。</p>
<h3 id="双亲委派模型"><a href="#双亲委派模型" class="headerlink" title="双亲委派模型"></a>双亲委派模型</h3><p><img src="/blog/6d27f500/parent_classloader.png" alt></p>
<p><code>AppClassLoader</code>的父类加载器为<code>ExtClassLoader</code>，<code>ExtClassLoader</code>的父类加载器为null，null并不代表<code>ExtClassLoader</code>没有父类加载器，而是<code>BootstrapClassLoader</code>。</p>
<p>加载的流程源码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">protected</span> Class&lt;?&gt; loadClass(String name, <span class="keyword">boolean</span> resolve)</span><br><span class="line">        <span class="keyword">throws</span> ClassNotFoundException</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">synchronized</span> (getClassLoadingLock(name)) &#123;</span><br><span class="line">            <span class="comment">// First, check if the class has already been loaded</span></span><br><span class="line">            Class&lt;?&gt; c = findLoadedClass(name);</span><br><span class="line">            <span class="keyword">if</span> (c == <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">long</span> t0 = System.nanoTime();</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    <span class="keyword">if</span> (parent != <span class="keyword">null</span>) &#123;</span><br><span class="line">                        c = parent.loadClass(name, <span class="keyword">false</span>);</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        c = findBootstrapClassOrNull(name);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; <span class="keyword">catch</span> (ClassNotFoundException e) &#123;</span><br><span class="line">                    <span class="comment">// ClassNotFoundException thrown if class not found</span></span><br><span class="line">                    <span class="comment">// from the non-null parent class loader</span></span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (c == <span class="keyword">null</span>) &#123;</span><br><span class="line">                    <span class="comment">// If still not found, then invoke findClass in order</span></span><br><span class="line">                    <span class="comment">// to find the class.</span></span><br><span class="line">                    <span class="keyword">long</span> t1 = System.nanoTime();</span><br><span class="line">                    c = findClass(name);</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// this is the defining class loader; record the stats</span></span><br><span class="line">                    sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0);</span><br><span class="line">                    sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1);</span><br><span class="line">                    sun.misc.PerfCounter.getFindClasses().increment();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (resolve) &#123;</span><br><span class="line">                resolveClass(c);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> c;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>加载的过程如下：</p>
<ul>
<li>首先，在当前类加载中调用<code>loadClass</code>方法，通过<code>findLoadedClass</code>来判断类是否已经被加载过。</li>
<li>如果还没有被记载：<ul>
<li>若父类加载器存在，则通过调用父加载器<code>loadClass()</code>方法处理；</li>
<li>否则，通过<code>findBootstrapClassOrNull</code>使用启动类加载器 <code>BootstrapClassLoader</code> 加载。</li>
</ul>
</li>
<li>如果无法成功记载对应class文件，就通过当前类加载器的<code>findClass</code>方法，否则就会抛出<code>ClassNotFoundException</code>；</li>
<li>如果已经加载过，则调用<code>resolveClass</code>执行连接操作。</li>
</ul>
<p>自定义加载器的话，需要继承 <code>ClassLoader</code> 。如果我们不想打破双亲委派模型，就重写 <code>ClassLoader</code> 类中的 <code>findClass()</code> 方法即可，无法被父类加载器加载的类最终会通过这个方法被加载。但是，如果想打破双亲委派模型则需要重写 <code>loadClass()</code> 方法。在<code>findClass</code>一般自定义找到class文件二进制流的方式，同时还要调用<code>defineClass</code>。部分源码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Finds the class with the specified &lt;a href="#name"&gt;binary name&lt;/a&gt;.</span></span><br><span class="line"><span class="comment">    * This method should be overridden by class loader implementations that</span></span><br><span class="line"><span class="comment">    * follow the delegation model for loading classes, and will be invoked by</span></span><br><span class="line"><span class="comment">    * the &#123;<span class="doctag">@link</span> #loadClass &lt;tt&gt;loadClass&lt;/tt&gt;&#125; method after checking the</span></span><br><span class="line"><span class="comment">    * parent class loader for the requested class.  The default implementation</span></span><br><span class="line"><span class="comment">    * throws a &lt;tt&gt;ClassNotFoundException&lt;/tt&gt;.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * <span class="doctag">@param</span>  name</span></span><br><span class="line"><span class="comment">    *         The &lt;a href="#name"&gt;binary name&lt;/a&gt; of the class</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * <span class="doctag">@return</span>  The resulting &lt;tt&gt;Class&lt;/tt&gt; object</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * <span class="doctag">@throws</span>  ClassNotFoundException</span></span><br><span class="line"><span class="comment">    *          If the class could not be found</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * <span class="doctag">@since</span>  1.2</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">   <span class="keyword">protected</span> Class&lt;?&gt; findClass(String name) <span class="keyword">throws</span> ClassNotFoundException &#123;</span><br><span class="line">       <span class="keyword">throw</span> <span class="keyword">new</span> ClassNotFoundException(name);</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> Class&lt;?&gt; defineClass(String name, <span class="keyword">byte</span>[] b, <span class="keyword">int</span> off, <span class="keyword">int</span> len,</span><br><span class="line">                                        ProtectionDomain protectionDomain)</span><br><span class="line">       <span class="keyword">throws</span> ClassFormatError</span><br><span class="line">   &#123;</span><br><span class="line">       protectionDomain = preDefineClass(name, protectionDomain);</span><br><span class="line">       String source = defineClassSourceLocation(protectionDomain);</span><br><span class="line">       Class&lt;?&gt; c = defineClass1(name, b, off, len, protectionDomain, source);</span><br><span class="line">       postDefineClass(c, protectionDomain);</span><br><span class="line">       <span class="keyword">return</span> c;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<h3 id="类加载异常分析"><a href="#类加载异常分析" class="headerlink" title="类加载异常分析"></a>类加载异常分析</h3><p>针对<code>ClassNotFoundException</code>，从双亲委派模型中，我们可以看到在调用<code>loadClass</code>方法过程中，如果一直无法成功加载class信息，就会抛出<code>ClassNotFoundException</code>。这个时候我们就要检查class的类路径是否正确以及对应的类加载器是否设置正确，一般是出现在反射或类加载器<code>loadClass</code>方法上，classpath中找不到类。</p>
<p>针对<code>NoClassDefFoundError</code>，一般出现在new一个对象实例时，class文件中出现该代码说明已经通过了编译，但是在实际运行时发现该类在初始化时发生了错误，比如自身静态代码块初始化异常、依赖的其他类不存在，就会抛出<code>NoClassDefFoundError</code>。</p>
<p>针对<code>NoSuchMethodError</code>，一般是对象实例调用了某个方法，在编译时classpath中的类存在该方法，但在实际运行中，加载进来该类没有该方法说明了存在编译时和运行时该类的版本不同或者是存在多个版本的JAR冲突问题。</p>
<p>为了避免上述的一些异常，我们在编写业务代码的时候，要注意设置好classpath或者做好不同版本的class隔离。为此，我们一般会通过自定义类加载器，比如扩展<code>URLClassLoader</code>重写<code>findClass</code>来加载指定目录下的class文件，与<code>AppClassLoader</code>读取的路径隔离开来。</p>
<h3 id="线程上下文类加载器"><a href="#线程上下文类加载器" class="headerlink" title="线程上下文类加载器"></a>线程上下文类加载器</h3><p>作为基础框架来说，比如Java中提供了一个SPI的机制，SPI接口是由<code>BootstrapClassLoader</code>来加载，SPI实现类需要<code>AppClassLoader</code>来加载，然而按照双亲委派机制这是无法实现的。为此，JAVA当中引入了<code>ThreadContextClassLoader</code>，破坏了“双亲委派模型”，可以在执行线程中抛弃双亲委派加载链模式，使程序可以<strong>逆向使用类加载器</strong>。</p>
<p>这里拿JDBC中的SPI使用来举例说明，代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DriverManager</span> </span>&#123;</span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Load the initial JDBC drivers by checking the System property</span></span><br><span class="line"><span class="comment">     * jdbc.properties and then use the &#123;<span class="doctag">@code</span> ServiceLoader&#125; mechanism</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        loadInitialDrivers();</span><br><span class="line">        println(<span class="string">"JDBC DriverManager initialized"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">loadInitialDrivers</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 省略部分代码</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">// If the driver is packaged as a Service Provider, load it.</span></span><br><span class="line">        <span class="comment">// Get all the drivers through the classloader</span></span><br><span class="line">        <span class="comment">// exposed as a java.sql.Driver.class service.</span></span><br><span class="line">        <span class="comment">// ServiceLoader.load() replaces the sun.misc.Providers()</span></span><br><span class="line"></span><br><span class="line">        AccessController.doPrivileged(<span class="keyword">new</span> PrivilegedAction&lt;Void&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Void <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">                ServiceLoader&lt;Driver&gt; loadedDrivers = ServiceLoader.load(Driver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">                Iterator&lt;Driver&gt; driversIterator = loadedDrivers.iterator();</span><br><span class="line"></span><br><span class="line">                <span class="keyword">try</span>&#123;</span><br><span class="line">                    <span class="keyword">while</span>(driversIterator.hasNext()) &#123;</span><br><span class="line">                        <span class="comment">//对应c = Class.forName(cn, false, loader); </span></span><br><span class="line">                        <span class="comment">// 此处的loader是由ServiceLoader.load(service, cl);</span></span><br><span class="line">                        driversIterator.next();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; <span class="keyword">catch</span>(Throwable t) &#123;</span><br><span class="line">                <span class="comment">// Do nothing</span></span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//省略部分代码</span></span><br><span class="line">&#125;</span><br><span class="line">    </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">ServiceLoader</span>&lt;<span class="title">S</span>&gt; <span class="keyword">implements</span> <span class="title">Iterable</span>&lt;<span class="title">S</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String PREFIX = <span class="string">"META-INF/services/"</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> &lt;S&gt; <span class="function">ServiceLoader&lt;S&gt; <span class="title">load</span><span class="params">(Class&lt;S&gt; service)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 如果没有设置contextClassLoader，默认是AppClassLoader</span></span><br><span class="line">        ClassLoader cl = Thread.currentThread().getContextClassLoader();</span><br><span class="line">        <span class="keyword">return</span> ServiceLoader.load(service, cl);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>由上述代码可以看出<code>ServiceLoader</code>由<code>BootstrapClassLoader</code>加载，内部找到所有Driver的实现类要加载需要通过当前线程的上下文加载器才可以识别到。同时，如果我们下游有不同版本的实现的话，可以自定义各自的类加载器，通过<code>Thread.currentThread().setContextClassLoader(ClassLoader cl)</code>设置进去，这样就可以做到隔离。</p>
<h3 id="扩展阅读"><a href="#扩展阅读" class="headerlink" title="扩展阅读"></a>扩展阅读</h3><ul>
<li><a href="https://snailclimb.gitee.io/javaguide/#/docs/java/jvm/%E7%B1%BB%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B" target="_blank" rel="noopener">类加载过程</a>，JavaGuide</li>
<li><a href="https://snailclimb.gitee.io/javaguide/#/docs/java/jvm/%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8?id=%e5%8f%8c%e4%ba%b2%e5%a7%94%e6%b4%be%e6%a8%a1%e5%9e%8b" target="_blank" rel="noopener">双亲委派模型</a>，JavaGuide</li>
<li><a href="https://cloud.tencent.com/developer/article/1356060" target="_blank" rel="noopener">理解ClassNotFoundException与NoClassDefFoundError的区别</a></li>
<li><a href="https://www.sofastack.tech/projects/sofa-boot/sofa-ark-readme/" target="_blank" rel="noopener">SOAFArk</a>，Java轻量级隔离容器，蚂蚁金服开源</li>
<li><a href="https://blog.csdn.net/yangcheng33/article/details/52631940" target="_blank" rel="noopener">真正理解线程上下文类加载器（多案例分析）</a></li>
<li><a href="https://mp.weixin.qq.com/s/JXHsdLwujT0pRKIImA1-3g" target="_blank" rel="noopener">Java 多线程上下文传递在复杂场景下的实践</a>，vivo</li>
<li><a href="https://mp.weixin.qq.com/s/mNfrgq9IC_4iPGBdt-1Qdg" target="_blank" rel="noopener">详解Apache Dubbo的SPI实现机制</a>， vivo</li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>类加载器</tag>
      </tags>
  </entry>
  <entry>
    <title>SparkSQL业务分析集锦</title>
    <url>/blog/b9b847a8.html</url>
    <content><![CDATA[<p>作为ROLAP引擎的SparkSQL具备丰富的SQL算子，比如关联、聚合、分组、窗口函数以及内置的各种函数、hint等等，在功能层面较好地满足了业务分析场景的需要。作为平台工具的开发者，在提供好用、稳定的工具的同时，更要掌握一定的SQL使用方式，来加深各个算子在实际场景的应用效果。故开此文，长期记录使用SQL进行经典OLAP分析的场景或者是一些奇淫技巧。</p>
<h3 id="场景1"><a href="#场景1" class="headerlink" title="场景1"></a>场景1</h3><p>有用户每日访问数据表，<code>user_visit_log(userid, visit_date, visit_count)</code>，使用SQL统计出每个用户在当月累积访问次数和总累计访问次数？</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> user_visit_log(user_id <span class="keyword">string</span>, visit_date <span class="keyword">string</span>, visti_count <span class="built_in">int</span>) <span class="keyword">using</span> orc;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> usr_visit_log</span><br><span class="line"><span class="keyword">values</span></span><br><span class="line">(<span class="string">'u01'</span>, <span class="string">'2017-01-01'</span>,<span class="number">5</span>),(<span class="string">'u02'</span>,<span class="string">'2017-01-23'</span>,<span class="number">6</span>),</span><br><span class="line">(<span class="string">'u03'</span>, <span class="string">'2017-01-22'</span>,<span class="number">8</span>),(<span class="string">'u04'</span>,<span class="string">'2017-01-20'</span>,<span class="number">3</span>),</span><br><span class="line">(<span class="string">'u01'</span>, <span class="string">'2017-01-23'</span>,<span class="number">6</span>),(<span class="string">'u01'</span>,<span class="string">'2017-02-21'</span>,<span class="number">8</span>),</span><br><span class="line">(<span class="string">'u02'</span>, <span class="string">'2017-01-23'</span>,<span class="number">6</span>),(<span class="string">'u01'</span>,<span class="string">'2017-02-22'</span>,<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> a <span class="keyword">as</span> (</span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    *,</span><br><span class="line">    <span class="keyword">date_format</span>(visit_date, <span class="string">'yyyyMM'</span>) <span class="keyword">as</span> visit_month</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    user_visit_log</span><br><span class="line">),</span><br><span class="line">b <span class="keyword">as</span> (</span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    user_id,</span><br><span class="line">    visit_month,</span><br><span class="line">    <span class="keyword">sum</span>(visit_count) <span class="keyword">as</span> month_total_visit_count</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    a</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    user_id, visit_month</span><br><span class="line">)</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">	*,</span><br><span class="line">	<span class="keyword">sum</span>(month_total_visit_count) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id) <span class="keyword">as</span> total_visit_count</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	b</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">	user_id, visit_month;</span><br></pre></td></tr></table></figure>
<h3 id="场景2"><a href="#场景2" class="headerlink" title="场景2"></a>场景2</h3><p>有用户表<code>user(user_id, name, age)</code>和观影记录表<code>view_record(user_id, moive_name)</code>，记录各个年龄段的观影次数？（10年作为一个区间，70岁以上作为一类）</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> a <span class="keyword">as</span> (</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    user_id,</span><br><span class="line">    <span class="keyword">count</span>(<span class="number">1</span>) <span class="keyword">as</span> cnt</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    view_record</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    user_id</span><br><span class="line">),</span><br><span class="line">b <span class="keyword">as</span> (</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="keyword">case</span></span><br><span class="line">    	<span class="keyword">when</span> age&lt;<span class="number">10</span> <span class="keyword">then</span> <span class="string">'10岁以下'</span></span><br><span class="line">    	<span class="keyword">when</span> age&gt;=<span class="number">10</span> <span class="keyword">and</span> age&lt;<span class="number">20</span> <span class="keyword">then</span> <span class="string">'10-20'</span></span><br><span class="line">    	<span class="keyword">when</span> age&gt;=<span class="number">20</span> <span class="keyword">and</span> age&lt;<span class="number">30</span> <span class="keyword">then</span> <span class="string">'20-30'</span></span><br><span class="line">    	<span class="keyword">when</span> age&gt;=<span class="number">30</span> <span class="keyword">and</span> age&lt;<span class="number">40</span> <span class="keyword">then</span> <span class="string">'30-40'</span></span><br><span class="line">    	<span class="keyword">when</span> age&gt;=<span class="number">40</span> <span class="keyword">and</span> age&lt;<span class="number">50</span> <span class="keyword">then</span> <span class="string">'40-50'</span></span><br><span class="line">    	<span class="keyword">when</span> age&gt;=<span class="number">50</span> <span class="keyword">and</span> age&lt;<span class="number">60</span> <span class="keyword">then</span> <span class="string">'50-60'</span></span><br><span class="line">    	<span class="keyword">when</span> age&gt;=<span class="number">60</span> <span class="keyword">and</span> age&lt;<span class="number">70</span> <span class="keyword">then</span> <span class="string">'60-70'</span></span><br><span class="line">    	<span class="keyword">else</span> <span class="string">'70岁以上'</span></span><br><span class="line">    <span class="keyword">end</span> <span class="keyword">as</span> age_level,</span><br><span class="line">    a.cnt</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    a</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> <span class="keyword">user</span> b <span class="keyword">on</span> a.user_id = b.user_id</span><br><span class="line">)</span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">	age_level, </span><br><span class="line">	<span class="keyword">sum</span>(b.cnt) <span class="keyword">as</span> cnt</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	b</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> age_level <span class="keyword">order</span> <span class="keyword">by</span> cnt;</span><br></pre></td></tr></table></figure>
<h3 id="场景3"><a href="#场景3" class="headerlink" title="场景3"></a>场景3</h3><p>有用户访问日志表user_access_log(date_time, user_id, age)，请用SQL计算出活跃用户数量和平均年龄？（活跃用户表示连续两天访问的用户）</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> user_access_log(date_time <span class="keyword">string</span>, user_id <span class="keyword">string</span>, age <span class="built_in">int</span>) <span class="keyword">using</span> orc;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> user_access_log</span><br><span class="line"><span class="keyword">values</span></span><br><span class="line">(<span class="string">'2019-02-12'</span>, <span class="number">2</span>, <span class="number">19</span>), (<span class="string">'2019-02-11'</span>, <span class="number">1</span>, <span class="number">23</span>),</span><br><span class="line">(<span class="string">'2019-02-11'</span>, <span class="number">3</span>, <span class="number">39</span>), (<span class="string">'2019-02-11'</span>, <span class="number">1</span>, <span class="number">23</span>),</span><br><span class="line">(<span class="string">'2019-02-11'</span>, <span class="number">3</span>, <span class="number">39</span>), (<span class="string">'2019-02-13'</span>, <span class="number">1</span>, <span class="number">23</span>),</span><br><span class="line">(<span class="string">'2019-02-15'</span>, <span class="number">2</span>, <span class="number">19</span>), (<span class="string">'2019-02-11'</span>, <span class="number">2</span>, <span class="number">19</span>),</span><br><span class="line">(<span class="string">'2019-02-11'</span>, <span class="number">1</span>, <span class="number">23</span>), (<span class="string">'2019-02-16'</span>, <span class="number">2</span>, <span class="number">19</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> a <span class="keyword">as</span> (</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    *,</span><br><span class="line">    <span class="comment">-- datediff(enddate, startdate) 相差的天数</span></span><br><span class="line">    <span class="keyword">datediff</span>(</span><br><span class="line">    	date_time,</span><br><span class="line">        <span class="comment">-- 在当前行向前取一行</span></span><br><span class="line">        <span class="keyword">lead</span>(date_time, <span class="number">-1</span>, date_time) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> date_time)</span><br><span class="line">    ) <span class="keyword">as</span> flag</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    user_access_log</span><br><span class="line">)</span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">	<span class="keyword">count</span>(<span class="keyword">distinct</span>(user_id)) <span class="keyword">as</span> active_user_cnt,</span><br><span class="line">	<span class="keyword">avg</span>(age) <span class="keyword">as</span> avg_age</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	a</span><br><span class="line"><span class="keyword">where</span> a.flag = <span class="number">1</span>;</span><br></pre></td></tr></table></figure>
<h3 id="场景4"><a href="#场景4" class="headerlink" title="场景4"></a>场景4</h3><p>有用户购买记录表，<code>user_pay_log(user_id, money, pay_time, order_id)</code>，请用sql写出所有用户中在2020年10月份第一次购买商品的金额？</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> user_pay_log(user_id <span class="keyword">string</span>, money <span class="keyword">double</span>, pay_time <span class="keyword">string</span>, order_id <span class="keyword">string</span>) <span class="keyword">using</span> orc;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> user_pay_log</span><br><span class="line"><span class="keyword">values</span></span><br><span class="line">(<span class="string">'01'</span>,<span class="number">18.5</span>,<span class="string">'2017-10-01'</span>,<span class="string">'o00001'</span>),(<span class="string">'01'</span>,<span class="number">205</span>,<span class="string">'2020-10-01'</span>,<span class="string">'o00002'</span>),</span><br><span class="line">(<span class="string">'02'</span>,<span class="number">18.5</span>,<span class="string">'2017-09-01'</span>,<span class="string">'o00003'</span>),(<span class="string">'02'</span>,<span class="number">20.5</span>,<span class="string">'2020-10-11'</span>,<span class="string">'o00004'</span>),</span><br><span class="line">(<span class="string">'03'</span>,<span class="number">100</span>,<span class="string">'2017-09-01'</span>,<span class="string">'o00005'</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> user_pay_log</span><br><span class="line"><span class="keyword">values</span></span><br><span class="line">(<span class="string">'01'</span>,<span class="number">18.5</span>,<span class="string">'2017-10-05'</span>,<span class="string">'o00006'</span>),(<span class="string">'01'</span>,<span class="number">205</span>,<span class="string">'2020-10-11'</span>,<span class="string">'o00007'</span>),</span><br><span class="line">(<span class="string">'02'</span>,<span class="number">18.5</span>,<span class="string">'2017-09-15'</span>,<span class="string">'o00008'</span>),(<span class="string">'02'</span>,<span class="number">20.5</span>,<span class="string">'2020-10-30'</span>,<span class="string">'o00009'</span>),</span><br><span class="line">(<span class="string">'03'</span>,<span class="number">100</span>,<span class="string">'2017-10-01'</span>,<span class="string">'o00010'</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> a <span class="keyword">as</span> (</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">	user_id,</span><br><span class="line">	money,</span><br><span class="line">	pay_time,</span><br><span class="line">	ROW_NUMBER() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> pay_time <span class="keyword">asc</span>) <span class="keyword">as</span> <span class="keyword">rank</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	user_pay_log</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">	<span class="keyword">DATE_FORMAT</span>(pay_time, <span class="string">'yyyy-MM'</span>)=<span class="string">'2020-10'</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">	user_id,</span><br><span class="line">	pay_time,</span><br><span class="line">	money</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	a</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">	<span class="keyword">rank</span> = <span class="number">1</span>;</span><br></pre></td></tr></table></figure>
<h3 id="场景5"><a href="#场景5" class="headerlink" title="场景5"></a>场景5</h3><p>有表<code>account(dist_id, account, gold_coin)</code> ，请写出SQL语句，查询各自区组的money排名前3的账号？</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span>  <span class="keyword">table</span> <span class="keyword">account</span>(dist_id <span class="keyword">string</span>, <span class="keyword">account</span> <span class="keyword">string</span>, gold_coin <span class="built_in">int</span>) <span class="keyword">using</span> orc;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">account</span></span><br><span class="line"><span class="keyword">values</span></span><br><span class="line">(<span class="string">'A'</span>, <span class="string">'user1'</span>, <span class="number">10</span>),(<span class="string">'A'</span>, <span class="string">'user2'</span>, <span class="number">2</span>),(<span class="string">'A'</span>, <span class="string">'user3'</span>, <span class="number">11</span>),</span><br><span class="line">(<span class="string">'A'</span>, <span class="string">'user4'</span>, <span class="number">8</span>),(<span class="string">'B'</span>, <span class="string">'user1'</span>, <span class="number">1</span>),(<span class="string">'B'</span>, <span class="string">'user2'</span>, <span class="number">103</span>),</span><br><span class="line">(<span class="string">'C'</span>, <span class="string">'user1'</span>, <span class="number">10</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> a (</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">	dist_id,</span><br><span class="line">	<span class="keyword">account</span>,</span><br><span class="line">	gold_coin,</span><br><span class="line">	row_number() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> dist_id <span class="keyword">order</span> <span class="keyword">by</span> gold_coin <span class="keyword">desc</span>) <span class="keyword">as</span> <span class="keyword">rank</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	<span class="keyword">account</span> </span><br><span class="line">)</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">	dist_id,</span><br><span class="line">	<span class="keyword">account</span>,</span><br><span class="line">	gold_coin</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	a</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">	<span class="keyword">rank</span> &lt;= <span class="number">3</span>;</span><br></pre></td></tr></table></figure>
<h3 id="场景6"><a href="#场景6" class="headerlink" title="场景6"></a>场景6</h3><p>有充值记录表<code>credit_log(dist_id, account, money, create_time)</code>，请写出SQL语句，查询充值日志表2020年08月08号每个区组下充值额最大的账号，要求结果： 区组id，账号，金额，充值时间 ？</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> credit_log(dist_id <span class="built_in">int</span>, <span class="keyword">account</span> <span class="keyword">string</span>, money <span class="built_in">int</span>, create_time <span class="keyword">string</span>) <span class="keyword">using</span> orc;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> credit_log</span><br><span class="line"><span class="keyword">values</span></span><br><span class="line">(<span class="number">1</span>, <span class="string">'user1'</span>, <span class="number">100</span>, <span class="string">'2021-01-01'</span>),</span><br><span class="line">(<span class="number">1</span>, <span class="string">'user1'</span>, <span class="number">200</span>, <span class="string">'2021-08-08'</span>),</span><br><span class="line">(<span class="number">1</span>, <span class="string">'user2'</span>, <span class="number">10</span>, <span class="string">'2021-01-01'</span>),</span><br><span class="line">(<span class="number">1</span>, <span class="string">'user3'</span>, <span class="number">1000</span>, <span class="string">'2020-08-08'</span>),</span><br><span class="line">(<span class="number">2</span>, <span class="string">'user4'</span>, <span class="number">100</span>, <span class="string">'2021-01-01'</span>),</span><br><span class="line">(<span class="number">2</span>, <span class="string">'user5'</span>, <span class="number">20</span>, <span class="string">'2021-08-08'</span>),</span><br><span class="line">(<span class="number">3</span>, <span class="string">'user6'</span>, <span class="number">10</span>, <span class="string">'2021-01-01'</span>),</span><br><span class="line">(<span class="number">4</span>, <span class="string">'user7'</span>, <span class="number">1500</span>, <span class="string">'2020-08-08'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> credit_log</span><br><span class="line"><span class="keyword">values</span></span><br><span class="line">(<span class="number">1</span>, <span class="string">'user8'</span>, <span class="number">1000</span>, <span class="string">'2020-08-08'</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> a (</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">	*,</span><br><span class="line">    <span class="comment">-- 这里用rank原因是防止有同等金额的都属于最大的。row_number是打编号，求topn</span></span><br><span class="line">	<span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> dist_id <span class="keyword">order</span> <span class="keyword">by</span> money <span class="keyword">desc</span>) <span class="keyword">as</span> <span class="keyword">rank</span> </span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	credit_log</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">	create_time = <span class="string">'2020-08-08'</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">	dist_id,</span><br><span class="line">	<span class="keyword">account</span>,</span><br><span class="line">	money,</span><br><span class="line">	create_time</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	a</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">	<span class="keyword">rank</span> = <span class="number">1</span>;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据库与大数据</category>
      </categories>
      <tags>
        <tag>OLAP</tag>
        <tag>业务</tag>
        <tag>SparkSQL</tag>
      </tags>
  </entry>
  <entry>
    <title>OLAP引擎-Kylin基本介绍</title>
    <url>/blog/bfe7107d.html</url>
    <content><![CDATA[<h3 id="基本介绍"><a href="#基本介绍" class="headerlink" title="基本介绍"></a>基本介绍</h3><p><a href="https://kylin.apache.org/cn/" target="_blank" rel="noopener">Apache Kylin</a>是Hadoop大数据平台上的一个开源MOLAP引擎。它采用多维立方体预计算技术，可以将大数据的SQL查询速度提升到亚秒级别。在2015年成为Apache的顶级项目，在2016年核心团队创立了Kyligence公司。</p>
<p>自Hadoop诞生以来，大数据的存储和批处理问题均得到了妥善解决，而如何高速地分析数据也就成为了下一个挑战。于是各式各样的“SQL on Hadoop”技术应运而生，其中以Hive为代表，Impala、Presto、Phoenix、Drill、Spark SQL等紧随其后。它们的主要技术是<strong>大规模并行处理</strong>（Massive Parallel Processing，MPP）和<strong>列式存储</strong>（Columnar Storage）。而<strong>预计算</strong>就是Kylin在<strong>大规模并行处理</strong>和<strong>列式存储</strong>之外，提供给大数据分析的第三个关键技术。</p>
<p>工作原理就是对数据模型做Cube预计算，并利用计算的结果加速查询，具体工作过程如下。 </p>
<ul>
<li>指定数据模型，定义维度和度量。 </li>
<li>预计算Cube，计算所有Cuboid并保存为物化视图。 </li>
<li>执行查询时，读取Cuboid，运算，产生查询结果。 </li>
</ul>
<h3 id="架构演进"><a href="#架构演进" class="headerlink" title="架构演进"></a>架构演进</h3><p><img src="/blog/bfe7107d/kylin_archi.png" alt></p>
<p>Kylin的三大依赖模块分别是数据源、构建引擎和存储引擎。比如，早期版本采用Hive接入数据源、MR离线构建cube，HBase存储cube，对外提供rest、jdbc接口来接收sql查询。Kylin为了更好地与时俱进，将这三部分进行了抽象，默认实现是Hive/MR/HBase。</p>
<p>当前，kylin经历了2.x、3.x到现在的4.0版本。支持了Kafka、第三方数据源接入，采用Spark进行Cube的构建，采用Parquet列式存储Cube，采用Spark SQL进行分布式查询。</p>
<p>kylin4的架构更适合上云，同时数据量越大性越好。但是针对那种简单的查询，是有一定的衰减，需要进行优化或者直接路由至查询服务，走本地spark的方式，避开spark on yarn的调度耗时。</p>
<h3 id="主要概念"><a href="#主要概念" class="headerlink" title="主要概念"></a>主要概念</h3><table>
<thead>
<tr>
<th>名词</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr>
<td>维度</td>
<td>观察数据的角度，比如时间、地区等</td>
</tr>
<tr>
<td>度量</td>
<td>统计值</td>
</tr>
<tr>
<td>事实表</td>
<td>事实表（Fact Table）指存储有事实记录（明细数据）的表，如系统日志、销售记录等；事实表的记录在不断地动态增长，数据量大。维度表（维表）：保存了维度值，可以跟事实表做关联。常见的维度表如：日期表，地点表，分类表</td>
</tr>
<tr>
<td>维表</td>
<td>维度表（Dimension Table）或维表，有时也称查找表（Lookup Table）， 是与事实表相对应的一种表；它保存了维度的属性值，可以跟事实表做关联；相当于将事实表上经常重复出现的属性抽取、规范出来用一张表进行管理。<br>常见的维度表有：日期表（存储与日期对应的周、月、季度等的属 性）、地点表（包含国家、省／州、城市等属性）等<br>优点：缩小了事实表的大小。便于维度的管理和维护，增加、删除和修改维度的属性，不必对事实表的大量记录进行改动。维度表可以为多个事实表重用，以减少重复工作。</td>
</tr>
<tr>
<td>OLAP</td>
<td>OLAP（Online Analytical Process），联机分析处理，以多维度的方式分析数据，而且能够弹性地提供上卷（Roll-up）、下钻（Drill-down）、切片（Slicing、Dicing）和旋转（Pivot/Rotate）。</td>
</tr>
<tr>
<td>BI</td>
<td>（Business Intelligence）即商务智能，指用现代数据仓库技术、在线分析技术、数据挖掘和数据展现技术进行数据分析以实现商业价值。</td>
</tr>
<tr>
<td>cubeid</td>
<td>对于N个维度来说，组合的所有可能性共有2^N 种。对于每一种维度的组合，将度量做聚合运算，然后将运算的结果保存为一个物化视图，称为Cuboid.</td>
</tr>
<tr>
<td>cube</td>
<td>即数据立方体，是一种常用于数据分析与索引的技术；它可以对原始数据建立多维度索引。通过Cube对数据进行分析， 可以大大加快数据的查询效率。<br>所有维度组合的Cuboid作为一个整体，被称为Cube。所以简单来说，一个Cube就是许多按维度聚合的物化视图的集合。</td>
</tr>
<tr>
<td>Cube Segment</td>
<td>是指针对源数据中的某一个片段，计算出来的Cube数据。通常数据仓库中的数据数量会随着时间的增长而增长，而Cube Segment也是按时间顺序来构建的。</td>
</tr>
<tr>
<td>星形模型</td>
<td>星形模型（Star Schema）中有一张事实表，以及零个或多个维度表；事实表与维度表通过主键外键相关联，维度表之间没有关联，就像很多星星围绕在一个 恒星周围，故取名为星形模型。</td>
</tr>
<tr>
<td>雪花模型</td>
<td>如果将星形模型中某些维度的表再做规范，抽取成更细的维度表， 然后让维度表之间也进行关联，那么这种模型称为雪花模型（Snowflake Schema）。</td>
</tr>
<tr>
<td>事实星座模型</td>
<td>星座模型是更复杂的模型，其中包含了多个事实表，而维度表是公用的，可以共享。</td>
</tr>
<tr>
<td>cubeid剪枝优化</td>
<td>Cube的剪枝优化则是一种试图减少 额外空间占用的方法，这种方法的前提是不会明显影响查询时间的缩减。在做剪枝优化的时候，需要选择跳过那些“多余”的Cuboid：有的 Cuboid因为查询样式的原因永远不会被查询到，因此显得多余；有的Cuboid的能力和其他Cuboid接近，因此显得多余。但是Cube管理员无法提前甄别每一个Cuboid是否多余，因此Kylin提供了一系列简单的工具来帮助他们完成Cube的剪枝优化。<br>以减少Cuboid数量为目的的Cuboid优化统称为Cuboid剪枝。主要方式衍生维度和聚合组。</td>
</tr>
</tbody>
</table>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ul>
<li>Apache Kylin权威指南</li>
<li><a href="https://mp.weixin.qq.com/s/0OgbvVggi8qgVtU53hg_Aw" target="_blank" rel="noopener">什么是OLAP？主流八大开源OLAP技术架构对比</a></li>
<li><a href="https://kylin.apache.org/docs40/" target="_blank" rel="noopener">https://kylin.apache.org/docs40/</a></li>
<li><a href="https://www.zhihu.com/org/kyligence" target="_blank" rel="noopener">Kylin知乎专栏</a></li>
<li><a href="https://kylin.apache.org/blog/2021/06/17/Why-did-Youzan-choose-Kylin4/" target="_blank" rel="noopener">Kylin4在有赞的应用</a></li>
<li><a href="https://cn.kyligence.io/blog/count-distinct-hyperloglog/" target="_blank" rel="noopener">大数据分析常用去重算法分析『HyperLogLog 篇』</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/352369781" target="_blank" rel="noopener">Kylin在各厂商的应用介绍</a></li>
<li><a href="https://kylin.apache.org/cn/blog/" target="_blank" rel="noopener">Apache Kylin™ 技术博客</a></li>
<li><a href="https://cn.kyligence.io/blog-zh/" target="_blank" rel="noopener">Kylin的创业公司技术博客</a></li>
<li><a href="http://www.jackywoo.cn/kylin-druid-clickhouse-comparing/" target="_blank" rel="noopener">KYLIN、DRUID、CLICKHOUSE核心技术对比</a></li>
</ul>
]]></content>
      <categories>
        <category>数据库与大数据</category>
      </categories>
      <tags>
        <tag>OLAP</tag>
        <tag>Kylin</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive性能调优实践</title>
    <url>/blog/ce4ffa9b.html</url>
    <content><![CDATA[<h2 id="Hive基本介绍"><a href="#Hive基本介绍" class="headerlink" title="Hive基本介绍"></a>Hive基本介绍</h2><h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p><img src="/blog/ce4ffa9b/hive_archi.png" alt></p>
<p>Hive通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop中执行，最后，将执行返回的结果输出到用户交互接口。</p>
<ul>
<li><p>用户接口：Client</p>
<p>CLI（hive shell）、JDBC/ODBC(java访问hive)、WEBUI（浏览器访问hive）</p>
</li>
<li><p>元数据：Metastore</p>
<p>元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等；默认存储在自带的derby数据库中，也可以使用MySQL/PG存储Metastore。</p>
</li>
<li><p>Hadoop</p>
<p>使用HDFS进行存储，使用MapReduce进行计算。</p>
</li>
<li><p>驱动器：Driver</p>
<ul>
<li>解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。</li>
<li>编译器（Physical Plan）：将AST编译生成逻辑执行计划。</li>
<li>优化器（Query Optimizer）：对逻辑执行计划进行优化。</li>
<li>执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/Spark。</li>
</ul>
</li>
</ul>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul>
<li><p>操作接口采用类SQL语法，提供快速开发的能力（简单、容易上手）。</p>
</li>
<li><p>避免了去写MapReduce，减少开发人员的学习成本。</p>
</li>
<li><p>Hive的执行延迟比较高，因此Hive常用于数据分析，对实时性要求不高的场合。</p>
</li>
<li><p>Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高。</p>
</li>
<li><p>Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。</p>
</li>
</ul>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li><p>Hive的HQL表达能力有限</p>
<ul>
<li><p>迭代式算法无法表达</p>
</li>
<li><p>数据挖掘方面不擅长</p>
</li>
</ul>
</li>
<li><p>Hive的效率比较低</p>
<ul>
<li>Hive自动生成的MapReduce作业，通常情况下不够智能化</li>
<li>Hive调优比较困难，粒度较粗</li>
</ul>
</li>
</ul>
<h2 id="Hive核心概念"><a href="#Hive核心概念" class="headerlink" title="Hive核心概念"></a>Hive核心概念</h2><h3 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h3><ul>
<li>Order By：全局排序，一个Reducer，ASC（ascend）升序（默认），DESC（descend） 降序；</li>
<li>Sort By：每个Reducer内部进行排序，对全局结果集来说不是排序。<code>set mapreduce.job.reduces=3;</code>；</li>
<li>Distribute By：类似MR中partition进行分区，结合sort by使用。注意，Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前；</li>
<li>Cluster By：当distribute by和sorts by字段相同时，可以使用cluster by方式。cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。</li>
</ul>
<h3 id="基本类型"><a href="#基本类型" class="headerlink" title="基本类型"></a>基本类型</h3><table>
<thead>
<tr>
<th>Hive数据类型</th>
<th>Java数据类型</th>
<th>长度</th>
<th>例子</th>
</tr>
</thead>
<tbody>
<tr>
<td>TINYINT</td>
<td>byte</td>
<td>1byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>SMALINT</td>
<td>short</td>
<td>2byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>INT</td>
<td>int</td>
<td>4byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>BIGINT</td>
<td>long</td>
<td>8byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>BOOLEAN</td>
<td>boolean</td>
<td>布尔类型，true或者false</td>
<td>TRUE  FALSE</td>
</tr>
<tr>
<td>FLOAT</td>
<td>float</td>
<td>单精度浮点数</td>
<td>3.14159</td>
</tr>
<tr>
<td>DOUBLE</td>
<td>double</td>
<td>双精度浮点数</td>
<td>3.14159</td>
</tr>
<tr>
<td>STRING</td>
<td>string</td>
<td>字符系列。可以指定字符集。可以使用单引号或者双引号。</td>
<td>‘now is the time’ “for all good men”</td>
</tr>
<tr>
<td>TIMESTAMP</td>
<td></td>
<td>时间类型</td>
<td></td>
</tr>
<tr>
<td>BINARY</td>
<td></td>
<td>字节数组</td>
</tr>
</tbody>
</table>
<h3 id="复合类型"><a href="#复合类型" class="headerlink" title="复合类型"></a>复合类型</h3><table>
<thead>
<tr>
<th>数据类型</th>
<th>描述</th>
<th>语法示例</th>
</tr>
</thead>
<tbody>
<tr>
<td>STRUCT</td>
<td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用。</td>
<td>struct()</td>
</tr>
<tr>
<td>MAP</td>
<td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素</td>
<td>map()</td>
</tr>
<tr>
<td>ARRAY</td>
<td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。</td>
<td>Array()</td>
</tr>
</tbody>
</table>
<h2 id="调优实践"><a href="#调优实践" class="headerlink" title="调优实践"></a>调优实践</h2><h3 id="Fetch抓取"><a href="#Fetch抓取" class="headerlink" title="Fetch抓取"></a>Fetch抓取</h3><p>Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算。在hive-default.xml.template文件中hive.fetch.task.conversion默认是more，老版本hive默认是minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce。</p>
<h3 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h3><p>用户可以通过设置hive.exec.mode.local.auto的值为true，来让Hive在适当的时候自动启动这个优化。Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。</p>
<h3 id="MapJoin"><a href="#MapJoin" class="headerlink" title="MapJoin"></a>MapJoin</h3><p>如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即：在Reduce阶段完成join。容易发生数据倾斜。可以用MapJoin把小表全部加载到内存在map端进行join，避免reducer处理。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 设置自动选择Mapjoin</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join = <span class="literal">true</span>; 默认为true</span><br><span class="line"><span class="comment">-- 大表小表的阈值设置（默认25M一下认为是小表）：</span></span><br><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize=<span class="number">25000000</span>;</span><br></pre></td></tr></table></figure>
<h3 id="Group-By"><a href="#Group-By" class="headerlink" title="Group By"></a>Group By</h3><p>默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 是否在Map端进行聚合，默认为True</span></span><br><span class="line"><span class="keyword">set</span> hive.map.aggr = <span class="literal">true</span></span><br><span class="line"><span class="comment">-- 在Map端进行聚合操作的条目数目</span></span><br><span class="line"><span class="keyword">set</span> hive.groupby.mapaggr.checkinterval = <span class="number">100000</span></span><br><span class="line"><span class="comment">-- 有数据倾斜的时候进行负载均衡（默认是false）</span></span><br><span class="line"><span class="keyword">set</span> hive.groupby.skewindata = <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h3 id="Count-Distinct-去重统计"><a href="#Count-Distinct-去重统计" class="headerlink" title="Count(Distinct) 去重统计"></a>Count(Distinct) 去重统计</h3><p>数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT操作需要用一个Reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换。</p>
<h3 id="数据倾斜"><a href="#数据倾斜" class="headerlink" title="数据倾斜"></a>数据倾斜</h3><ul>
<li><p>合理设置Map数</p>
<ul>
<li>决定map数量的因素有哪些？</li>
</ul>
<p>主要的决定因素有：<strong>input的文件总个数，input的文件大小，集群设置的文件块大小</strong>。</p>
<ul>
<li>是不是map数越多越好？<br><strong>如果一个任务有很多小文件（远远小于块大小128m）</strong>，则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。<br><strong>比如有一个127m的文件</strong>，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。</li>
</ul>
</li>
<li><p>小文件合并</p>
<p>在map执行前合并小文件，减少map数：<code>CombineHiveInputFormat</code>具有对小文件进行合并的功能（系统默认的格式），<code>HiveInputFormat</code>没有对小文件合并功能。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br></pre></td></tr></table></figure>
</li>
<li><p>调整Map数</p>
<p>当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 调整maxSize最大值</span></span><br><span class="line"><span class="comment">// 让maxSize最大值低于blocksize就可以增加map的个数</span></span><br><span class="line">computeSliteSize(Math.max(minSize, Math.min(maxSize, blocksize))) = blocksize = <span class="number">128</span>M</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 设置最大切片值为100个字节</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.input.fileinputformat.split.maxsize=<span class="number">100</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>调整Reduce数</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 方法一</span></span><br><span class="line"><span class="comment">-- 每个Reduce处理的数据量默认是256MB</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer=<span class="number">256000000</span></span><br><span class="line"><span class="comment">-- 每个任务最大的reduce数，默认为1009</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.reducers.max=<span class="number">1009</span></span><br><span class="line"><span class="comment">-- 计算reducer数的公式</span></span><br><span class="line">N=<span class="keyword">min</span>(参数<span class="number">2</span>，总输入数据量/参数<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 方法二</span></span><br><span class="line"><span class="comment">-- 在hadoop的mapred-default.xml文件中修改</span></span><br><span class="line"><span class="comment">-- 设置每个job的Reduce个数</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces = <span class="number">15</span>;</span><br></pre></td></tr></table></figure>
<p>过多的启动和初始化reduce也会消耗时间和资源。另外，<strong>有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题</strong>。</p>
</li>
</ul>
<h3 id="并行执行"><a href="#并行执行" class="headerlink" title="并行执行"></a>并行执行</h3><p>Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 打开任务并行执行</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel=<span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 同一个sql允许最大并行度，默认为8</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel.thread.number=<span class="number">16</span>;</span><br></pre></td></tr></table></figure>
<h3 id="JVM重用"><a href="#JVM重用" class="headerlink" title="JVM重用"></a>JVM重用</h3><p>JVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是对于很难避免小文件的场景或task特别多的场景，这类场景大多数执行时间都很短。</p>
<p>Hadoop的默认配置通常是使用派生JVM来执行map和Reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。JVM重用可以使得JVM实例在同一个job中重新使用N次。N的值可以在Hadoop的mapred-site.xml文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.jvm.numtasks<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>10<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>How many tasks to run per jvm. If set to -1, there is</span><br><span class="line">  no limit. </span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>这个功能的缺点是，开启JVM重用将一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。</p>
<h3 id="推测执行"><a href="#推测执行" class="headerlink" title="推测执行"></a>推测执行</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- mapred-site.xml --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some map tasks </span><br><span class="line">               may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some reduce tasks </span><br><span class="line">               may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- hive配置--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.mapred.reduce.tasks.speculative.execution<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether speculative execution for reducers should be turned on. <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="扩展内容"><a href="#扩展内容" class="headerlink" title="扩展内容"></a>扩展内容</h2><h3 id="样例"><a href="#样例" class="headerlink" title="样例"></a>样例</h3><ul>
<li>union调优</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> xx</span><br><span class="line"><span class="keyword">select</span> ... <span class="keyword">from</span> a</span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span> ... <span class="keyword">from</span> a</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> a</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> xx <span class="keyword">select</span> ...</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> xx <span class="keyword">select</span> ...</span><br></pre></td></tr></table></figure>
<ul>
<li>数据块大小</li>
</ul>
<p>一个文件由2个数据块构成，在hdfs namenode不紧张的情况，多个不同节点上的map读取这个数据块，本地性较差，需要走网络拉取数据。相比于，一个文件由10个数据块构成，多个不同节点上map本地性较好，减少了网络传输，性能会有所提升。</p>
<ul>
<li>数据格式</li>
</ul>
<p>ORC对hive数据存储主流之一</p>
<ul>
<li>表设计</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- hive分区表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> xx (...)</span><br><span class="line">partitioned <span class="keyword">by</span> (f1 <span class="built_in">bigint</span>)</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc;</span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line"></span><br><span class="line"><span class="comment">--hive分桶表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> yy(..., f2 <span class="built_in">int</span>)</span><br><span class="line">clustered <span class="keyword">by</span> (f2) <span class="keyword">into</span> n buckets</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc;</span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line"><span class="keyword">set</span> hive.enforce.bucketing=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--hive分区分桶表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> yy(..., f2 <span class="built_in">int</span>)</span><br><span class="line">partitioned <span class="keyword">by</span>(f1 <span class="built_in">bigint</span>)</span><br><span class="line">clustered <span class="keyword">by</span> (f2) <span class="keyword">into</span> n buckets</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc;</span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line"><span class="keyword">set</span> hive.enforce.bucketing=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<p>表的设计对于HiveSQL的性能有一定的影响，并不能说明分区分桶表性能一定比只分桶的表性能差，因为基于不同业务和上层的计算逻辑，表现出来的性能差异也会不同。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Hive构建在大数据集群之上，包括了<strong>分布式计算</strong>、<strong>分布式存储</strong>和<strong>分布式调度</strong>等多个系统。因此，Hive的调优并不是单方面的调优，是一个涉及多个组件的系统化工程。</p>
<h2 id="调优思路"><a href="#调优思路" class="headerlink" title="调优思路"></a>调优思路</h2><h3 id="分布式计算"><a href="#分布式计算" class="headerlink" title="分布式计算"></a>分布式计算</h3><p>学习大数据分布式计算的基本原理，比如MapReduce、Spark等。</p>
<h3 id="分布式调度"><a href="#分布式调度" class="headerlink" title="分布式调度"></a>分布式调度</h3><p>学习使用YARN提供的日志，查看Job运行量化信息。</p>
<h3 id="调优三部曲"><a href="#调优三部曲" class="headerlink" title="调优三部曲"></a>调优三部曲</h3><h4 id="改写SQL"><a href="#改写SQL" class="headerlink" title="改写SQL"></a>改写SQL</h4><p>grouping sets代替union、分解count(distinct)为count(group by)</p>
<p>比如，在小数据量下count(distinct)会优于count(group by)，但大数据量数据发生倾斜时，count(group by)优于count(distinct)，因为count(group by)会分为两个作业，第一阶段先处理一部分数据，缩小数据量。第二阶段在缩小的数据量上继续处理。在hive3.0中，开启hive.optimize.countdistinct，会自动调优count(disctinct)。</p>
<p>调优讲究适时调优，过早进行调优有可能做无用功，因此调优需要遵循一定的原则。如下：</p>
<ul>
<li><p>理透需求原则，这是优化根本；</p>
</li>
<li><p>把握数据全链路原则，这是优化脉络；</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查看元数据</span></span><br><span class="line">desc formatted table;</span><br><span class="line"><span class="comment">-- 查看表字段、所属数据、创建时间、hdfs路径、文件个数、数据量、serdes、input/outputformat、是否压缩、分桶等</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 运行环境</span></span><br><span class="line"><span class="comment">-- 资源管理：yarn、任务调度：oozie、airflow坚持代码的简洁原则，这让优化更加简单；</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>没有瓶颈时谈论优化，这是自寻烦恼。</p>
<ul>
<li>第一类问题：影响项目整体落地的问题、重大性能问题，在项目设计阶段就要规避。</li>
<li>第二类问题：不影响项目整体落地，但是影响部分功能，根据具体的业务要求进行调优，将优化放到有瓶颈点的地方去考虑和讨论。</li>
</ul>
</li>
</ul>
<h4 id="SQL-hint使用"><a href="#SQL-hint使用" class="headerlink" title="SQL-hint使用"></a>SQL-hint使用</h4><p>mapjoin、streamtable等hint使用</p>
<h4 id="数据库配置调整"><a href="#数据库配置调整" class="headerlink" title="数据库配置调整"></a>数据库配置调整</h4><p>hive.vectorized.execution.enabled开启向量化、hive.exec.parallel、hive.exec.paralledl.thread.number</p>
<h3 id="Hive规范"><a href="#Hive规范" class="headerlink" title="Hive规范"></a>Hive规范</h3><h4 id="开发规范"><a href="#开发规范" class="headerlink" title="开发规范"></a>开发规范</h4><ul>
<li>单条SQL长度不宜超过一屏。</li>
<li>SQL子查询嵌套不宜超过3层。</li>
<li>少用或者不用Hint，特别是在Hive2.0之后，增强HiveSQL对于成本优化(CBO)的支持，在业务环境变化时可能会导致Hive无法选用最优的执行计划。</li>
<li>避免SQL代码的复制、粘贴。如果有多处逻辑一致的代码，可以将执行结果存储在临时表中。</li>
<li>尽可能使用SQL自带党的高级命令做操作。比如多维统计中使用cube、grouping set和roll up等命令去替代多个SQL子句的union all。</li>
<li>使用set命令，进行配置属性的更改，要有注释。</li>
<li>代码里面不允许包含对表、分区、列的DDL语句，除了新增或删除分区。</li>
<li>Hive SQL更加适合处理多条数据组合的数据集，不适合处理单条数据，且单条数据之间存在顺序依赖等逻辑关系。</li>
<li>保持一个查询语句所处理的表类型单一。比如，一个SQL语句中的表都是ORC或Parquet。</li>
<li>关注NULL值得数据处理。</li>
<li>SQL表连接的条件列和查询的过滤列最好要有分区列和分桶列。</li>
<li>存在多层嵌套，内层嵌套表的过滤条件不要写到外层。</li>
</ul>
<h4 id="设计规范"><a href="#设计规范" class="headerlink" title="设计规范"></a>设计规范</h4><ul>
<li>表结构要有注释。</li>
<li>列等属性字段要有注释。</li>
<li>尽量不要使用索引。hive中处理批量处理大量数据，hive索引在表和分区有数据更新时不会自动维护，需要手动触发。索引在hive3.0之后被废弃，使用物化视图或数据存储采用ORC格式可以替代索引的功能。</li>
<li>创建内部表不允许指定数据存储路劲，由管理人员统一规划目录并固化在配置中。</li>
<li>创建非接口表，只允许使用ORC或Parquet。接口表指与其他系统进行的数据表，比如导入hive的临时表或者提供给其他系统使用的输出表。</li>
<li>HIVE适合处理宽边（列数多的表），适当的冗余有助于Hive的处理性能。</li>
<li>表的文件块大小要与HDFS的数据块大小大致相等。</li>
<li>分区表和分桶表的适用。</li>
</ul>
<h4 id="命名规范"><a href="#命名规范" class="headerlink" title="命名规范"></a>命名规范</h4><ul>
<li>表以tb_开头；</li>
<li>临时表以tmp_开头；</li>
<li>视图以v_开头；</li>
<li>自定义函数以udf_开头；</li>
<li>原始数据所在的库以db_org_ 开头，明细数据所在库以db_detail_ 开头，数据仓库以db_dw_ 开头。</li>
</ul>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>优化基本流程：</p>
<ul>
<li>选择性能评估以及各自目标，时延和吞吐量</li>
<li>系统由多个组件和服务构成，分组件和服务定义性能目标</li>
<li>明确当前环境下各个组件的性能</li>
<li>分析定位性能瓶颈：hive常见是磁盘和网络IO的瓶颈，其次是内存。涉及hive的执行计划、计算引擎基本原理。</li>
<li>优化产生性能瓶颈的程序或者系统：优化存储、执行过程和作业调度。</li>
<li>性能监控和告警：<ul>
<li>在操作系统和硬件层面借助linux系统提供的工具，或者zabbix和ganglia开源工具。</li>
<li>软件层面，借助prometheus和grafana定制监控大数据组件。</li>
<li>作业层面，借助yarn timeline提供查看作业信息的监控。</li>
</ul>
</li>
</ul>
<h3 id="扩展阅读"><a href="#扩展阅读" class="headerlink" title="扩展阅读"></a>扩展阅读</h3><ul>
<li><a href="https://mp.weixin.qq.com/s/ORYKS1gS0S8PHgKROhmXmw" target="_blank" rel="noopener">Hive知识体系保姆级教程-超全概括</a></li>
</ul>
]]></content>
      <categories>
        <category>计算引擎</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark性能调优实战</title>
    <url>/blog/19c2ab93.html</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Spark（Spark SQL）在离线计算场景应用广泛，为了保证Spark应用更好地满足业务场景需求，同时能够在线上稳定地运行，我们需要关注Spark的调优工作。首先，需要了解Spark对外的接口并如何高效地使用；其次要搞清楚内部的运行机制以及参数配置体系；最后是要能够深入分析spark的日志信息。进一步来讲，对于Spark的深度使用者，需要关注社区各个版本的迭代、bug修复以及性能优化的情况，才能更好地打开思路，提高解决问题的效率。<font color="red">主要途径有：spark的release-note、databricks官方博客、源码。</font></p>
<p>为了方便Spark相关性能问题的排查，本文记录了日常Spark使用过程中遇到的问题和解决思路，用于积累过程中进行复盘总结，强化Spark的深入理解和实战经验。</p>
<a id="more"></a>
<h2 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h2><ol>
<li><p>如何优化Spark集群CPU利用率？</p>
<p>产生CPU利用率低一般有两种原因导致：Executor上线程挂起和Task任务数太多调度开销大。需要以下三方面来考虑：</p>
<ul>
<li>并行度：拆分任务的粒度，涉及参数有：spark.default.parallelism、spark.sql.shuffle.partitions、spark.sql.files.maxPartitionBytes、spark.sql.files.openCostInBytes</li>
<li>并发度N：集群总体cores，spark.executor.instances、spark.executor.cores、spark.task.cpus</li>
<li>执行内存M：executor在可获得执行内存，下限是spark.executor.memory <em> spark.memory.fraction </em> (1- spark.memory.storageFraction)，上限是spark.executor.memory * spark.memory.fraction</li>
</ul>
<p>在给定执行内存 M、线程池大小 N 和数据总量 D 的时候，想要有效地提升 CPU 利用率，我们就要计算出最佳并行度 P，计算方法是让数据分片的平均大小 D/P 坐落在（M/N/2, M/N）区间。这样，在运行时 CPU 利用率往往不会太差。</p>
<p>总而言之，需要平衡“并行度、并发度、执行内存”，去提升CPU利用率，所以更需要使用系统工具、监控工具，比如ganglia、Prometheus、Grafana、nmon、htop等等这些工具，去观察你的CPU利用率，然后回过头来平衡三者，然后再去观察CPU利用率是不是提升了，通过这种方式来去调优。</p>
</li>
<li><p>针对CET达到几百规模的大SQL在上千规模Hadoop集群的执行性能调优？</p>
<p>SparkSQL thriftserver 侧的优化：</p>
<ul>
<li>元数据读写一些锁的优化，从比较大的锁粒度降到比较小的锁粒度；</li>
<li>引入多线程，提高解析每个互不依赖的子查询的并行度。</li>
</ul>
<p>DAGScheduler侧的优化：</p>
<ul>
<li>引入线程池提高Task被调度到Executor的效率，降低调度延迟；</li>
<li>适当调小spark.locality.wait.node，降低延迟调度的时间，提高调度的效率；</li>
<li>适当调大spark.resultGetter.threads的数值，提高处理返回结果的效率。</li>
</ul>
</li>
<li><p>如何利用到ReuseExchange的特性？</p>
<p>比如有这样一个场景，读取一批parque文件，都是按照A字段做group by后，进行两个场景的计算。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//方式1</span></span><br><span class="line"><span class="keyword">val</span> filePath: <span class="type">String</span> = _</span><br><span class="line"><span class="keyword">val</span> df = spark.read.parquet(filePath) </span><br><span class="line"><span class="keyword">val</span> dfPV = df.groupBy(<span class="string">"userId"</span>).agg(count(<span class="string">"page"</span>).alias(<span class="string">"value"</span>)).withColumn(<span class="string">"metrics"</span>, lit(<span class="string">"PV"</span>))</span><br><span class="line"><span class="keyword">val</span> dfUV = df.groupBy(<span class="string">"userId"</span>).agg(countDistinct(<span class="string">"page"</span>).alias(<span class="string">"value"</span>)).withColumn(<span class="string">"metrics "</span>, lit(<span class="string">"UV"</span>)) </span><br><span class="line">dfPV.union(dfUV).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">//方式2</span></span><br><span class="line"><span class="keyword">val</span> filePath: <span class="type">String</span> = _</span><br><span class="line"><span class="comment">//预先分好组，当然如果dfPV和dfUV有过滤的操作的话（结果集较小），强行reuse就不太适合了</span></span><br><span class="line"><span class="keyword">val</span> df = spark.read.parquet(filePath).repartition($<span class="string">"userId"</span>) </span><br><span class="line"><span class="comment">//下面两个dfPV和dfUV，就会复用df</span></span><br><span class="line"><span class="keyword">val</span> dfPV = df.groupBy(<span class="string">"userId"</span>).agg(count(<span class="string">"page"</span>).alias(<span class="string">"value"</span>)).withColumn(<span class="string">"metrics"</span>, lit(<span class="string">"PV"</span>))</span><br><span class="line"><span class="keyword">val</span> dfUV = df.groupBy(<span class="string">"userId"</span>).agg(countDistinct(<span class="string">"page"</span>).alias(<span class="string">"value"</span>)).withColumn(<span class="string">"metrics "</span>, lit(<span class="string">"UV"</span>)) </span><br><span class="line">dfPV.union(dfUV).show()</span><br></pre></td></tr></table></figure>
<p>前提条件：</p>
<ul>
<li>多个查询所依赖的<strong>分区规则要与 Shuffle 中间数据的分区规则</strong>保持一致</li>
<li>多个查询所涉及的字段（Attributes）要保持一致</li>
</ul>
</li>
<li><p>Spark集群机器选型一般依据是什么？</p>
<ul>
<li>如果你的计算场景涉及到大量的聚合、排序、哈希计算、数值计算等等，那么你的机器配置就要加强CPU；</li>
<li>如果你的计算场景需要反复消耗同一份或是同一批数据集，比如机器学习、数据分析、图计算，那么为了把需要频繁访问的数据缓存进内存，你自然需要加大内存配置；</li>
<li>如果你的计算场景会引入大量shuffle，又不能通过广播来消除Shuffle，那么你就需要配置足够的SSD以及高吞吐网络。</li>
</ul>
</li>
<li><p>建模分析时，判断一批IP地址是否在原始海量的IP段范围中，性能不理想。抽象为非等值JOIN的优化？</p>
<p>分析物理算子org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec，扩展出一个BroadcastRangeJoinExec算子，通过给那个小表做索引并排序，Join时候就不需要每一条都扫描，只扫描一部分就可以了。<a href="https://github.com/apache/spark/pull/21109" target="_blank" rel="noopener">SPARK-24020</a></p>
</li>
<li><p>建模分析时，少量重点人员账号与原始数据进行碰撞，性能不理想。在大小表基于非分区字段join时，大表读取的数据过多，执行性能较差？</p>
<p>可以采用Runtime Filter的方式，它的原理和DPP（动态分区裁剪）类似，因为DPP要求你的Join条件中包含了分区字段才会开启DPP，Runtime Filter可以把一些非分区字段条件形成一个filter放到大表上，类似传统数据库Query Rewrite，如果表大表能够过滤较多数据，从而可以提高JOIN的性能。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">IndexJoinRule</span>(<span class="params">sparkSession: <span class="type">SparkSession</span></span>) </span></span><br><span class="line"><span class="class">				<span class="keyword">extends</span> <span class="title">Rule</span>[<span class="type">LogicalPlan</span>] </span></span><br><span class="line"><span class="class">				<span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(plan: <span class="type">LogicalPlan</span>):<span class="type">LogicalPlan</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> convertedPlan = applyIndexJoinRule(<span class="type">Plan</span>)</span><br><span class="line">        convertedPlan    </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">applyIndexJoinRule</span></span>(plan: <span class="type">LogicalPlan</span>) = plan transform &#123;</span><br><span class="line">        <span class="comment">// select a.col1, b.col2</span></span><br><span class="line">        <span class="keyword">case</span> j<span class="meta">@Join</span>(</span><br><span class="line">            left<span class="meta">@Project</span>(_,lFilter<span class="meta">@Filter</span>(lConditon,_))),</span><br><span class="line">        	right<span class="meta">@Proejct</span>(_, rFilter<span class="meta">@Filter</span>(rCondition, _)),_,</span><br><span class="line">        	<span class="type">Some</span>(joinCond),_) =&gt;</span><br><span class="line">        	convertInnerJoin(j, left, lFilter,lConditon, </span><br><span class="line">                             right, rFilter,rConditon, joinCond)</span><br><span class="line">        <span class="comment">// select a.*, b.col2</span></span><br><span class="line">        <span class="keyword">case</span> j<span class="meta">@Join</span>(</span><br><span class="line">            left<span class="meta">@Filter</span>(lCondition, _), </span><br><span class="line">            right<span class="meta">@Project</span>(_, rFilter<span class="meta">@Filter</span>(rCondtion, _)),_,</span><br><span class="line">        	<span class="type">Some</span>(joinCond), _) =&gt;</span><br><span class="line">        	convertInnerJoin(j, left, lFilter,lConditon, </span><br><span class="line">                             right, rFilter,rConditon, joinCond)</span><br><span class="line"> 		<span class="comment">//其他计划规则</span></span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">convertInnerJoin</span></span>(j: <span class="type">Join</span>,</span><br><span class="line">                                left: <span class="type">LogicalPlan</span>,</span><br><span class="line">                                lFilter: <span class="type">Filter</span>,</span><br><span class="line">                                lConditon： <span class="type">EXpression</span>，</span><br><span class="line">                                right: <span class="type">LogicalPlan</span>,</span><br><span class="line">                                rFilter: <span class="type">Filter</span>,</span><br><span class="line">                                rCondition: <span class="type">Expression</span>,</span><br><span class="line">                                joinCond: <span class="type">Expression</span>) : <span class="type">LogicalPlan</span> = &#123;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//基于统计信息和配置来确定是否转换logicalplan</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">//将小表的索引列值查询出来追加到大表的过滤条件</span></span><br><span class="line">        <span class="comment">//小表结果</span></span><br><span class="line">        <span class="keyword">val</span> result = &#123;</span><br><span class="line">            <span class="keyword">val</span> qe = sparkSession.sessionState.executePlan(planToQuery)</span><br><span class="line">            qe.assertAnalyzed()</span><br><span class="line">            <span class="keyword">new</span> <span class="type">DataSet</span>[<span class="type">Row</span>](qe, <span class="type">RowEncoder</span>(qe.analyzed.schema))</span><br><span class="line">        &#125;.collect().map&#123;row=&gt;<span class="type">Literal</span>.create(row.get(<span class="number">0</span>), joinCol.dataType)&#125;.distinct</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">val</span> newJoin = &#123;</span><br><span class="line">            <span class="comment">//转为in条件</span></span><br><span class="line">            <span class="keyword">val</span> inCond = <span class="type">In</span>(getJoinAttr(joinCond， right).get, result)</span><br><span class="line">            <span class="keyword">val</span> newCond = <span class="type">And</span>(rCondition, inCond)</span><br><span class="line">            <span class="keyword">val</span> newRight = right.transform&#123;</span><br><span class="line">                <span class="keyword">case</span> f<span class="meta">@Filter</span>(_,_) =&gt; f.copy(condition = newCond)</span><br><span class="line">            &#125;</span><br><span class="line">            j.copy(right = newRight)</span><br><span class="line">        &#125;</span><br><span class="line">        newJoin.copy(left, right)</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol start="7">
<li><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">org.apache.spark.shuffle.FetchFailedException: </span><br><span class="line">Failed to send RPC XXX to /xxx:<span class="number">7337</span>:java.nio.channels.ColsedChannelException</span><br></pre></td></tr></table></figure>
<ul>
<li>原因：external shuffle服务将数据发送给container时，发现container已经关闭连接，出现该异常应该和org.apache.spark.shuffle.FetchFailedException: Connection from /xxx:7337 closed同时出现；</li>
<li>解决方案：参考org.apache.spark.shuffle.FetchFailedException: Connection from /xxx:7337 closed的解决方案。</li>
<li>进一步补充：<ul>
<li>在验证中发现关闭参数spark.shuffle.readHostLocalDisk，可以规避该异常的出现；</li>
<li>顺着上述参数发现在spark3.0.0中org.apache.spark.network.shuffle.ExternalBlockStoreClient#getHostLocalDirs指定rpc操作后默认关闭了RPC client，导致后续其他任务使用该client时出现已经关闭的情况。排查发现社区在<a href="https://issues.apache.org/jira/browse/SPARK-32663" target="_blank" rel="noopener">SPARK-32663</a>已经修复了该问题。</li>
</ul>
</li>
</ul>
</li>
<li><p>如何加快netty堆外内存的回收？snappy+parquet格式数据会导致，netty堆外内存增长太快，导致netty使用过多direct memory报错？</p>
<p>首先，io.netty.util.internal.OutOfDirectMemoryError: failed to allocate 16777216 byte(s) of direct memory (used: 7633633280, max: 7635730432)，这个问题，往往出现在Shuffle read阶段，spark用netty的client/server来拉取远端节点数据，并且透过java.nio.DirectByteBuffers来缓存接收到的数据块。当数据分布存在比较严重的倾斜问题的时候，就会导致某些Block过大，从而导致单个线程占用的Direct Buffer超过16MB，从而报出上面的错误。</p>
<p>因此，要从根本上解决问题，可以先搞定数据倾斜的问题，如果数据倾斜消除了，那么这个问题大概率自己就会消失掉。关于消除数据倾斜的方法，可以参考后面AQE那几讲，以及两阶段Shuffle那一讲。</p>
<p>接下来，假设你消除了Data Skew之后，这个报错还在，那么就继续用下面的办法。DirectByteBuffers默认的大小就是spark.executor.memory的大小，也就是说，它在逻辑上，会“计入”Executor memory内存的消耗。spark.executor.memory这玩意其实指定的JVM heap堆内的内存大小，而DirectByteBuffers是堆外内存，按理说两者应该区别对待，然而默认情况下，并没有。因此，如果DirectByteBuffers消耗非常的过分，那么我们可以在spark.executor.extraJavaOptions当中，特意地去指定-XX:MaxDirectMemorySize这个参数，这个参数，就是用来指定DirectByteBuffers的内存大小，可以把它设置的大一些。</p>
<p>再者，假设上面的设置，还不能解决问题，那么接下来，我们就得做进一步的精细化调优。首先，把spark.reducer.maxSizeInFlight，设置成-XX:MaxDirectMemorySize / spark.executor.cores ，这个设置的意图，是降低每个线程需要缓存的数据量。然后，把spark.maxRemoteBlockSizeFetchToMem，设置成spark.reducer.maxSizeInFlight / 5，这个设置的意图，是为了把大的Block直接落盘，从而迅速释放线程占用的Direct buffer，降低Direct buffer（也就是堆外内存）的消耗，从而降低OOM的风险。</p>
</li>
<li><p>java.lang.OutOfMemoryError: GC overhead limit exceeded</p>
<ul>
<li>原因：数据量太大，内存不够。</li>
<li>解决方案：<ul>
<li>增大spark.executor.memory的值，减小spark.executor.cores</li>
<li>减少输入数据量，将原来的数据量分几次任务完成，每次读取其中一部分</li>
</ul>
</li>
</ul>
</li>
<li><p>ERROR An error occurred while trying to connect to the Java server (127.0.0.1:57439) Connection refused</p>
<ul>
<li>原因：<ul>
<li>节点上运行的container多，每个任务shuffle write到磁盘的量大，导致磁盘满，节点重启 </li>
<li>节点其他服务多，抢占内存资源，NodeManager处于假死状态</li>
</ul>
</li>
<li>解决方案：<ul>
<li>确保节点没有过多其他服务进程 </li>
<li>扩大磁盘容量 </li>
<li>降低内存可分配量，比如为总内存的90%，可分配内存少了，并发任务数就少了，出现问题概率降低 </li>
<li>增大NodeManager的堆内存</li>
</ul>
</li>
</ul>
</li>
<li><p>org.apache.spark.shuffle.FetchFailedException: Failed to connect to /9.4.36.40:7337</p>
<ul>
<li>背景：shuffle过程包括shuffle read和shuffle write两个过程。对于spark on yarn，shuffle write是container写数据到本地磁盘(路径由core-site.xml中hadoop.tmp.dir指定)过程； shuffle read是container请求external shuffle服务获取数据过程，external shuffle是NodeManager进程中的一个服务，默认端口是7337，或者通过spark.shuffle.service.port指定。</li>
<li>定位过程：拉取任务运行日志，查看container日志；查看对应ip上NodeManager进程运行日志，路径由yarn-env.sh中YARN_LOG_DIR指定。</li>
<li>原因：container请求NodeManager上external shufflle服务，不能正常connect，说明NodeManager可能挂掉了，原因可能是：<ul>
<li>节点上运行的container多，每个任务shuffle write到磁盘的量大，导致磁盘满，节点重启</li>
<li>节点其他服务多，抢占内存资源，NodeManager处于假死状态</li>
</ul>
</li>
<li>解决方案：<ul>
<li>确保节点没有过多其他服务进程 </li>
<li>扩大磁盘容量 </li>
<li>降低内存可分配量，比如为总内存的90%，可分配内存少了，并发任务数就少了，出现问题概率降低</li>
<li>增大NodeManager的堆内存</li>
</ul>
</li>
</ul>
</li>
<li><p>spark任务中stage有retry</p>
<ul>
<li>原因：<ul>
<li>下一个stage获取上一个stage没有获取到全部输出结果，只获取到部分结果，对于没有获取的输出结果retry stage以产出缺失的结果；</li>
<li>部分输出结果确实已经丢失 ，部分输出结果没有丢失，只是下一个stage获取结果超时，误认为输出结果丢失。</li>
</ul>
</li>
<li>解决方案：<ul>
<li>针对原因(1)，查看进程是否正常，查看机器资源是否正常，比如磁盘是否满或者其他；</li>
<li>针对原因(2)，调大超时时间，如调大spark.network.timeout值。</li>
</ul>
</li>
</ul>
</li>
<li><p>Final app status: FAILED, exitCode: 11, (reason: Max number of executor failures (200) reached)</p>
<ul>
<li>原因：executor失败重试次数达到阈值</li>
<li>解决方案：<ul>
<li>调整运行参数，减少executor失败次数；</li>
<li>调整spark.yarn.max.executor.failures的值，可在spark-defaults.conf中调整。确定方式，在日志中搜索”Final app status:”，确定原因，在日志统计”Container marked as failed:”出现次数。</li>
</ul>
</li>
</ul>
</li>
<li><p>task反复调度到有问题的executor？</p>
<p>通过这些黑名单的设置可以避免由于 task 反复调度在有问题的 executor/node （坏盘，磁盘满了，shuffle fetch 失败，环境错误等）上，进而导致整个 Application 运行失败的情况。</p>
</li>
</ol>
<h3 id="扩展阅读"><a href="#扩展阅读" class="headerlink" title="扩展阅读"></a>扩展阅读</h3><ul>
<li><a href="https://mp.weixin.qq.com/s/_KWwb80dlkEtu1SeRK4jqw" target="_blank" rel="noopener">Apache Spark 完全替代传统数仓的技术挑战及实践</a>，马刚@eBay，大数据团队成员</li>
<li><a href="https://time.geekbang.org/column/intro/400" target="_blank" rel="noopener">Spark性能调优实战</a>，吴磊，FreeWheel机器学习团队负责人</li>
<li><a href="https://databricks.com/blog" target="_blank" rel="noopener">https://databricks.com/blog</a></li>
<li><a href="https://www.yuque.com/tonyshu/gnfipq/tbeihk" target="_blank" rel="noopener">Spark性能调优指南笔记</a>，笔者</li>
</ul>
]]></content>
      <categories>
        <category>计算引擎</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>PMP实践之路</title>
    <url>/blog/c50a937d.html</url>
    <content><![CDATA[<p>人的精力总是有限的，只有通过团队的力量，才能获得更高的成就。作为一个项目的负责人，需要带领项目团队成员，一起完成目标，顺利交付。所以，其中涉及到的个人能力提升，调动组员积极性发挥其最大能力，与同部门、跨部门的不同人员进行沟通和协作，都是每一个项目经理需要不断学习的。</p>
<h3 id="项目经理的六种能力模型"><a href="#项目经理的六种能力模型" class="headerlink" title="项目经理的六种能力模型"></a>项目经理的六种能力模型</h3><p>一个优秀的项目经理，应该具备哪些基本能力呢？我们可以参考这个项目经理的<strong>六种能力模型</strong>——<strong>知识、技能、逻辑思维、执行力、心智成熟和领导力。</strong></p>
<ul>
<li><p>理事</p>
<ul>
<li>知识：项目管理理论知识、<strong>行业知识</strong>、专业知识（<strong><em>知道怎么做</em></strong>）</li>
<li>技能：<strong>项目管理技能、沟通表达、写作技能</strong>（<strong><em>会做</em></strong>）</li>
<li>逻辑思维：归纳能力、判断力（<strong><em>确认方向和路线</em></strong>）</li>
<li>执行力：（<strong><em>去做并达到目标</em></strong>）<ul>
<li>执行可行计划，不盲目行动</li>
<li>自我激励，激发自身潜能</li>
<li><strong>今日事，今日毕</strong></li>
<li><strong>不怕麻烦，把小事做细做透</strong></li>
</ul>
</li>
</ul>
</li>
<li><p>管人</p>
<ul>
<li>心智成熟：<strong>情绪稳定、换位思考、承担责任</strong>（<strong><em>与人相处发挥团队力量</em></strong>）</li>
<li>领导力：（<strong><em>众人划桨开大船</em></strong>）<ul>
<li>管理就是<strong>让团队完成事情</strong></li>
<li><strong>调动团队成员积极性</strong>，而不是自己累死而无法规划后续工作</li>
<li>提高团队成员目标感、责任感，而<strong>不是被管理</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>作为项目经理，在工作中要有意识的刻意培养自己这6个方面的能力。</p>
<h3 id="如何处理好与团队成员的关系"><a href="#如何处理好与团队成员的关系" class="headerlink" title="如何处理好与团队成员的关系"></a>如何处理好与团队成员的关系</h3><p>很多时候，我们完成一个项目都是临时抽调组成一个新的项目组，为了保证项目经理能够快速融入团队处理好与团队成员的关系，需要了解团队发展的各个阶段情况。</p>
<table>
<thead>
<tr>
<th>阶段</th>
<th>成员情绪</th>
<th>主要工作要点</th>
<th>领导者作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>形成</td>
<td>兴奋情绪、焦虑怀疑</td>
<td>创造清晰的结构、目标、方向和角色定位</td>
<td>指导式领导：引导团队成员分享团队发展阶段的概念，达成共识。</td>
</tr>
<tr>
<td>震荡</td>
<td>挫折愤怒、紧张对立</td>
<td>将大目标分解成更小的、可实现的步骤。发展与任务相关的技能和应对团队冲突的技能。</td>
<td>教练式领导：强调团队成员的差异，相互包容。</td>
</tr>
<tr>
<td>规范</td>
<td>明确信任、规范交流</td>
<td>实现组织和谐，增加动机水平。有意识地解决问题。</td>
<td>参与式领导：允许团队有更大的自治性。</td>
</tr>
<tr>
<td>成熟</td>
<td>开放沟通、积极激情</td>
<td>工作顺利、高效完成，没有冲突，不需要外部监督</td>
<td>委任式领导：让团队自己执行必要的决策。</td>
</tr>
<tr>
<td>解散</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>在整个项目结束前，作为项目团队中的灵魂人物，项目经理需要在不同的阶段有侧重的做一些事情，来保证团队及其内部成员能够稳定、一致的向目标前进。跟项目团队的成员处好关系，调动他们的主观能动性，参考以下几条：</p>
<ul>
<li>关怀：对他人感兴趣；</li>
<li>学习：了解他人；</li>
<li>欣赏：尊重他人；</li>
<li>付出：帮助他人；</li>
<li>言语表达：肯定他人；</li>
<li>领导：影响他人；</li>
<li>成功：与他们在一起。</li>
</ul>
<p>项目经理在一个团队中，<strong>要从使用各种方法“催”进度“赶”工期，到敏锐的感觉团队的氛围和组员的情绪，关心他们的工作和生活，与其站在一起，激励团队士气。</strong>这些都是项目经理需要做的事。</p>
<h3 id="编制责任分配矩阵"><a href="#编制责任分配矩阵" class="headerlink" title="编制责任分配矩阵"></a>编制责任分配矩阵</h3><p>一个项目的顺利交付，是需要团队成员共同努力才能完成的。这就需要项目经理能够合理合情的将相关职责分配给对应的个人或者小团队。这里我们介绍一个工具——<strong>RACI责任分配矩阵。</strong></p>
<ul>
<li>R(Responsible)：<strong>谁负责</strong>，即负责执行任务的角色，TA具体负责操控项目、解决问题；</li>
<li>A(Accountable)：<strong>谁批准</strong>，即对任务负全责的角色，只有经过TA的同意或签署之后，项目才得以进行；</li>
<li>C(Consulted)：<strong>咨询谁</strong>，即拥有完成项目所需的信息或能力的人，属于辅助人员；</li>
<li>I(Informed)：<strong>通知谁</strong>，即应及时得到通知的人员。</li>
</ul>
<p><strong>使用RACI责任分配矩阵，让团队成员在具体的项目活动中，明确各自的职责和任务，更有利于彼此间的协作，确保工作顺利执行下去</strong>。</p>
<p>那么，在制定RACI的注意点有：</p>
<ul>
<li>应该由项目团队集体完成，每个人明确认领各自的任务，主动承担责任；</li>
<li><strong>并不是每个人都愿意主动担责的，所以就需要项目经理发挥自己出色的沟通能力，适当的使用强硬的分配方式，并辅以必要的解释和鼓励</strong>；</li>
<li><strong>每项任务确定唯一责任人，避免职责不清导致相互之间的推诿</strong>。</li>
</ul>
<h3 id="项目管理箴言"><a href="#项目管理箴言" class="headerlink" title="项目管理箴言"></a>项目管理箴言</h3><table>
<thead>
<tr>
<th style="text-align:center">编号</th>
<th style="text-align:left">内容</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:left">开会+ 不落实 = 零；布置工作 + 不检查 = 零；抓住不落实的事 + 追究不落实的人 = 落实</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:left">以身作则：要求下属的管理者自己要先做到，还要做得更好。为结果负责；组织意识</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:left">滚动式规划是一种迭代式的规划技术，即详细规划近期要完成的工作，同时在较高层级上粗略规划远期工作</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:left">概率和影响矩阵是把每个风险发生的概率和一旦发生对项目目标的影响映射起来的表格。此矩阵对概率和影响进行组合，以便于把单个项目风险划分成不同的优先级组别。</td>
</tr>
<tr>
<td style="text-align:center">５</td>
<td style="text-align:left">效率来自分工，分工需要协作，配合和协作是一个团队的基石。–《技术领导力实战笔记》</td>
</tr>
<tr>
<td style="text-align:center">6</td>
<td style="text-align:left">人就是缺乏毅力的动物，好不容易下定决心做一件事，却往往因为一些鸡毛蒜皮的理由放弃，可以说，这是99.9%的人的通病。“必须坚持”，人人都这么想，却往往做不到。但是改成“不要放弃”，或许就没有那么大压力，大家反而能坚持下去。</td>
</tr>
<tr>
<td style="text-align:center">7</td>
<td style="text-align:left">项目经理应该主动识别出那些利益受损的相关方，及时给予必要的修复和补偿。这种补偿行为越早、越主动，相关方后期出现“搅局”的可能性就会越低。</td>
</tr>
<tr>
<td style="text-align:center">8</td>
<td style="text-align:left">作为领导者，在给下属下达目标时，采取模糊期望的方法，其实就是在暗示下属：这件工作的优先级和完成标准，都按你自己的意思去做吧！既然如此，领导者就不得不接受和容忍下属糟糕的表现，因为你并没有告诉他们：什么才是好的！<br>如果要求模糊，等于没有要求，请明确无误量化结果，并聆听反馈，取得共识。如果实在难以量化的结果，则一定要有清晰的描述。建议工具：工作目标设定的四要素，就是工作的（ 数量 ）、（ 时间）、（ 质量 ）、（成本）或者它们的组合。</td>
</tr>
<tr>
<td style="text-align:center">9</td>
<td style="text-align:left">在面对一个新领域的时候，首先会从行业角度去看目前业界有哪些最先进的相关技术。在从宏观层面对这个技术领域有了初步了解之后，我就会结合实际工作需求去重点了解对应的技术课题，设立好一个架构，然后从微观层面有针对性的去学习、使用新技术。在团队层面，也会去看需要引进哪些关键人才来补齐缺失的技术能力。通过这种层层拆分的方式，就让一个复杂问题逐渐简单化，完成那些刚开始看起来似乎是不可能完成的任务。</td>
</tr>
<tr>
<td style="text-align:center">10</td>
<td style="text-align:left">如何搞定人（搞人）？PMP，谐音拍马屁，这招真的很好用，没有人是不喜欢被肯定的，但往往过于浮夸的赞美让人感觉不好意思，或者不知道如何回应，这时善于观察的你需要抓住对方真实的优点，从而提炼出应景的，适度的，真心的夸赞才是上策，记住这点你的项目管理之路将所向披靡。<br>如何搞定事（搞事），一切的源头在于项目规划，一切的挑战在于变更管理，一切的精髓在于专业地把事情做好，且必须以双赢为目的导向。</td>
</tr>
<tr>
<td style="text-align:center">11</td>
<td style="text-align:left">低水平的人实际上只是依靠自己的能力在苦干；<br>中等水平的人可以运用别人的力量来做事；<br>高水平的人可以激发别人的智慧来实现目标。<br>管理人员应该具备后两种能力，不仅能够激励团队成员积极工作，还能够发挥团队的智慧去突破和创新。</td>
</tr>
<tr>
<td style="text-align:center">12</td>
<td style="text-align:left">员工坚持每天提高1%的工作能力，70天工作水平就可以提升近一倍。　<br>管理者不要期望员工一步登天，因为这样会出现“拔苗助长”的现象。应该正确引导员工们循序渐进地提高自己的水平，哪怕每天有细微的进步，时间长了员工们的’工作水平就会有质的飞跃。前提是员工愿意提高自己的工作能力。</td>
</tr>
<tr>
<td style="text-align:center">13</td>
<td style="text-align:left">每个职场人都希望自己能在一个更好的平台上工作，毋庸置疑，平台好了，对每个员工都有好处。所以我们每个人都应该帮助这个平台向好的方向发展，而从组织的角度来看，则是要淘汰给平台造成拖累和阻力的人。<br>员工的目标是使自己在组织中变得有价值，让自己最终成为不可替代的人。作为管理人员，要能够识别出“可有可无”的人。</td>
</tr>
<tr>
<td style="text-align:center">14</td>
<td style="text-align:left">西点军校最贵一课：卓越领导者，须具备8大品质<br>1.勇气（Courage）　<br>2.决策能力（Ability to Make Decisions）<br>3.诚信可靠（Integrity/Reliability）　<br>4.坚韧不拔的意志（Persistence/Tenacity）　<br>5.理解士兵，换位思考（Empathy for the Soldier）　<br>6.专家/知识（Expertise/Knowledge）　<br>7.适应性（Adaptability）　<br>8.恢复力（Resilience）</td>
</tr>
<tr>
<td style="text-align:center">15</td>
<td style="text-align:left">员工价值=能力×价值观×心智模式。<br>这个公式告诉我们，提升自己有三个大的努力方向：<br>第一个是提升个人能力<br>第二个是树立与公司一致的价值观<br>第三个是改善自己的心智模式。</td>
</tr>
<tr>
<td style="text-align:center">16</td>
<td style="text-align:left">当与另一个团队成员发生冲突时，有四种应对方法。<br>1.被动逃避。<br>这意味着什么都不做，假装问题不存在。<br>2.消极抵抗。<br>这可能是最具破坏性的策略。<br>这种策略会破坏整个团队的气氛，对团队的运作非常具有破坏性。<br>3.咄咄逼人。<br>但它往往会留下浓郁的苦涩、怨恨和对立，这会伤害团队成员和团队中更深层次的社交氛围。<br>4.从容不迫。<br>这意味着向其他人倾诉自己的感受，并要求对方改变其行为，以防止冲突的再次发生。</td>
</tr>
<tr>
<td style="text-align:center">17</td>
<td style="text-align:left">鸟儿们聚在一起推举它们的国王。孔雀说它最漂亮，应该由它当，立刻得到所有鸟儿的赞成。只有穴鸟不以为然地说：当你统治鸟国的时候，如果有老鹰来追赶我们，你如何救我们呢？<br>做任何事一定要深谋远虑，才不至于害了自己。<br>一个经理，当储备多方才能，不只在才识方面要有过人之处，更当有应万变的能力。如此，不但可服人，并且还能对付不可预知的意外事件。</td>
</tr>
<tr>
<td style="text-align:center">18</td>
<td style="text-align:left">一只山猪在大树旁勤奋地磨獠牙。狐狸看到了，好奇地问它，既没有猎人来追赶，也没有任何危险，为什么要这般用心地磨牙。山猪答道：你想想看，一旦危险来临，就没时间磨牙了。现在磨利，等到要用的时候就不会慌张了。<br>防患于未然的工作是绝对需要的。<br>说明：书到用时方恨少，平常若不充实学问，临时抱佛脚是来不及的。也有人抱怨没有机会，然而当升迁机会来临时，再叹自己平时没有积蓄足够的学识与能力，以致不能胜任，也只好后悔莫及。</td>
</tr>
<tr>
<td style="text-align:center">19</td>
<td style="text-align:left">领在前面才容易导，这就是领导。于说服别人不光用嘴巴，还要让你的行动走在前面来证明，这样才能发挥你的领导力。</td>
</tr>
<tr>
<td style="text-align:center">20</td>
<td style="text-align:left">员工价值=能力×价值观×心智模式。<br>这个公式告诉我们，提升自己有三个大的努力方向：<br>第一个是提升个人能力<br>第二个是树立与公司一致的价值观<br>第三个是改善自己的心智模式。</td>
</tr>
<tr>
<td style="text-align:center">21</td>
<td style="text-align:left">学习的核心过程与关键要素<br>从本质上讲，学习是个人主动地进行知识构建、提升有效行动能力以及绩效表现的过程，它离不开自身的能力、努力与坚持，也不可避免地会受到环境的影响。<br>因此，学习是一个高度个性化的过程。<br>学习能力强的人善于从外部各种途径获取对自己有价值的信息，并消化、吸收，内化为自身的能力。</td>
</tr>
<tr>
<td style="text-align:center">22</td>
<td style="text-align:left">学习既离不开主动地检视、反省自身，也要广泛地向他人学习，就像荀子所讲：“君子博学而日参省乎己，则知明而行无过矣”（《荀子·劝学》）。<br>也就是说，君子要广泛地学习，又能每天检查、反省自己，那就会智慧澄明，行为也就不会犯错了。综上所述，在我看来，学习一点儿也不简单，它是一个复杂而微妙的系统工程。<br>为了主动、有意识地提升学习效果，个人不仅要具备系统思考的智慧，对学习这一系统工程做到心中有数，并且觉察或识别出制约自己高效学习的关键要素，使得各项要素相互协调配合、产生良好的效果，而且要善于利用自身经历和外部资源。</td>
</tr>
<tr>
<td style="text-align:center">23</td>
<td style="text-align:left">公司请你来是发现问题和解决问题的，不是制造问题的；你能解决多大问题，你就做多高的位置。<br>你解决需求的问题，可以做程序员。你能搞定架构的问题，才能做架构师。能解决技术体系建设以及技术战略的问题，才能做技术VP。<br>问题就是我们的机会：客户的问题是我们提供更好服务的机会；竞争对手的问题是我们变强的机会；同事的问题就是支持和建立合作的机会；领导的问题就是我们积极解决获取信任的机会；自己的问题就是我们成长的机会。解决问题的人高升，制造问题的人让位，抱怨问题的人下课。</td>
</tr>
<tr>
<td style="text-align:center">24</td>
<td style="text-align:left">有提拔特质的人：（1）对公司的前景始终看好，而不是遇到一点波动就撤退的人；（2）在业务调整、团队调整的过程中不计较个人得失，顾全大局，能找到自己的位置，并始终愿意跟着团队一起成长的人；（3）为了团队新的目标，不断学习新东西，有冲劲，对新鲜的事物充满好奇的人。总之，并不是精致的、职场利己主义者。</td>
</tr>
<tr>
<td style="text-align:center">25</td>
<td style="text-align:left">技术专家：一线员工，做什么以及怎么做；<br>新晋管理者：为什么要做，目标是什么，交付时间是什么，好的交付标准是什么。给员工授权以及挑战，充分发挥员工的主观能动性，并在他们遇到风险和困境时，给予指导和支持他们。<br>思路的转变，而不是做一个事必躬亲的管理者。</td>
</tr>
<tr>
<td style="text-align:center">26</td>
<td style="text-align:left">如果有一个项目，首先要考虑有没有人来做。如果没有人做，就要放弃，这是一个必要条件。——柳传志。感悟：人的因素是一个项目能否成功的关键。企业选项目时，要依靠人的能力来进行选择，如果没有合适的人选，再诱人的项目也不能贸然进入。</td>
</tr>
<tr>
<td style="text-align:center">27</td>
<td style="text-align:left">当你有选择的时候，选择更重要；而当你没有选择的时候，努力才重要。努力是为了让你有更好的选择。</td>
</tr>
<tr>
<td style="text-align:center">28</td>
<td style="text-align:left">管理是重器，保长远；经营是利器，赢天下。<br>管理=管人+理事，经营人才，就是经营企业。<br>管理者能扛多大的责任，承受多大的压力，就能带多大的队伍，做多大的事情。建强队列，才能干好业务。</td>
</tr>
<tr>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul>
<li>公众号-<a href="https://mp.weixin.qq.com/s/2HdYrvbee4P7ZKm1O1UzOQ" target="_blank" rel="noopener">PM圈子</a></li>
<li>公众号－阿里技术｜<a href="https://mp.weixin.qq.com/s/oDcCerQ-0v918fqEn_JpkA" target="_blank" rel="noopener">如何把事做成</a></li>
</ul>
]]></content>
      <categories>
        <category>项目管理</category>
      </categories>
  </entry>
  <entry>
    <title>Calcite处理和扩展流程解析</title>
    <url>/blog/7dec2e4.html</url>
    <content><![CDATA[<p>相关概述与特性，可以查看之前的文章《<a href="https://changbo.tech/blog/b227762f.html">Calcite原理和经验总结</a>》。</p>
<h1 id="SQL处理流程"><a href="#SQL处理流程" class="headerlink" title="SQL处理流程"></a>SQL处理流程</h1><h2 id="处理过程"><a href="#处理过程" class="headerlink" title="处理过程"></a>处理过程</h2><p><img src="/blog/7dec2e4/sql.png" alt></p>
<ul>
<li>Parser（SQL-&gt;SqlNode）： 把 SQL 转换成为 AST （抽象语法树），Calcite 使用 JavaCC 做 SQL 解析.</li>
<li>Validate（SqlNode-&gt;RelNode）： <ul>
<li>语法检查，根据数据库的元数据信息进行语法验证，验证之后还是用 SqlNode 表示 AST 语法树；</li>
<li>语义分析，根据 SqlNode 及元信息构建 RelNode 树，也就是最初版本的逻辑计划（Logical Plan）。</li>
</ul>
</li>
<li>Optimize（RelNode-&gt;RelNode）： <strong>逻辑计划优化，优化器的核心</strong>，根据前面生成的逻辑计划（relational expression，即关系代数）按照相应的规则（Rule）进行优化；</li>
<li>Execute: 物理执行，生成物理计划，物理执行计划执行。</li>
</ul>
<a id="more"></a>
<h2 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h2><table>
<thead>
<tr>
<th><strong>类型</strong></th>
<th><strong>描述</strong></th>
<th><strong>特点</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>SqlNode</td>
<td>SQL解析树</td>
<td>Sql 经过解析就会转化SqlNode，然后生成一个未经验证的抽象语法树</td>
</tr>
<tr>
<td>RelNode</td>
<td>relational expression，SqlNode 经过语法分析就会生成RelNode</td>
<td>代表了<strong>对数据的一个处理操作</strong>，常见的操作有 Sort、Join、Project、Filter、Scan 等。它包含了对整个 Relation 的操作，而不是对具体数据的处理逻辑。</td>
</tr>
<tr>
<td>RelOptRule</td>
<td>transforms an expression into another。对 expression 做等价转换</td>
<td>根据它里面的一些规则来对目标 RelNode 树进行局部规则匹配，匹配成功后，则调用 onMatch() 方法进行转换。</td>
</tr>
<tr>
<td>ConverterRule</td>
<td>规则的抽象类，该规则从一种调用规则转化为另一种调用规则而无需更改语义</td>
<td>它是 RelOptRule 的子类，专门用来做数据源之间的转换（Calling convention），<strong>ConverterRule 一般会调用对应的 Converter 来完成工作</strong>，比如说：JdbcToSparkConverterRule 调用 JdbcToSparkConverter 来完成对 JDBC Table 到 Spark RDD 的转换。</td>
</tr>
<tr>
<td>Converter</td>
<td>A relational expression implements the interface <code>Converter</code> to indicate that it converts a physical attribute, or RelTrait of a relational expression from one value to another.</td>
<td><strong>用来把一种 RelTrait 转换为另一种 RelTrait 的 RelNode</strong>。如 JdbcToSparkConverter 可以把 JDBC 里的 table 转换为 Spark RDD。如果需要在一个 RelNode 中处理来源于异构系统的逻辑表，Calcite 要求先用 Converter 把异构系统的逻辑表转换为同一种 Convention。</td>
</tr>
<tr>
<td>RexNode</td>
<td>Row-level expression行表达式</td>
<td>行表达式（标量表达式），蕴含的是对一行数据的处理逻辑。每个行表达式都有数据的类型。这是因为在 Valdiation 的过程中，编译器会推导出表达式的结果类型。常见的行表达式包括字面量 RexLiteral， 变量 RexVariable， 函数或操作符调用 RexCall 等。 RexNode 通过 RexBuilder 进行构建。</td>
</tr>
<tr>
<td>RelTrait</td>
<td>RelTrait表示特征定义中关系表达式特质的表现形式</td>
<td>用来定义逻辑表的物理相关属性（physical property），三种主要的特征类型是：1、<strong>Convention</strong>（用于表示单个数据源的调用约定，一个 relational expression 必须在同一个 convention 中）、2、<strong>RelCollation</strong>（指的是该关系表达式所定义的数据的排序）、3、<strong>RelDistribution</strong>（标识数据的分布特点，比如single、hash、range、random等）；</td>
</tr>
<tr>
<td>RelTraitDef</td>
<td></td>
<td>主要有三种：ConventionTraitDef：用来代表数据源 ；RelCollationTraitDef：用来定义参与排序的字段；RelDistributionTraitDef：用来定义数据在物理存储上的分布方式（比如：single、hash、range、random 等）；</td>
</tr>
<tr>
<td>RelOptCluster</td>
<td>An environment for related relational expressions during the optimization of a query.</td>
<td>palnner运行时的环境，保存上下文信息；</td>
</tr>
<tr>
<td>RelOptPlanner</td>
<td>A RelOptPlanner is a query optimizer: it transforms a relational expression into a semantically equivalent relational expression, according to a given set of rules and a cost model.</td>
<td>也就是<strong>优化器</strong>，Calcite 支持RBO（Rule-Based Optimizer） 和 CBO（Cost-Based Optimizer）。Calcite 的 RBO （HepPlanner）称为启发式优化器（heuristic implementation ），它简单地按 AST 树结构匹配所有已知规则，直到没有规则能够匹配为止；Calcite 的 CBO 称为火山式优化器（VolcanoPlanner）成本优化器也会匹配并应用规则，当整棵树的成本降低趋于稳定后，优化完成，成本优化器依赖于比较准确的成本估算。<strong>RelOptCost 和 Statistic 与成本估算相关</strong>；</td>
</tr>
<tr>
<td>RelOptCost</td>
<td>defines an interface for optimizer cost in terms of number of rows processed, CPU cost, and I/O cost.</td>
<td>主要依赖于IO、CPU、RowCount、memory</td>
</tr>
</tbody>
</table>
<h2 id="关系代数"><a href="#关系代数" class="headerlink" title="关系代数"></a>关系代数</h2><h3 id="关系"><a href="#关系" class="headerlink" title="关系"></a>关系</h3><p>关系是关系模型中用于描述数据的主要结构。包括关系模式（relation schema）和关系实例（relation instance）。</p>
<ul>
<li><p>关系实例：一个二维表。</p>
</li>
<li><p>关系模式：对表的每个列进行描述。</p>
<p>例如：<code>Students(sid:string, name:string, age:integer)</code>。一个关系代数表达式可以用关系、一元或二元代数操作符来递归定义。代数操作符的输入和输出都是关系实例。</p>
</li>
</ul>
<h3 id="关系代数-1"><a href="#关系代数-1" class="headerlink" title="关系代数"></a>关系代数</h3><p>关系代数是关系型数据库操作的理论基础，关系代数支持并、差、笛卡尔积、投影和选择等基本运算。</p>
<p>关系代数也是 Calcite 的核心，任何一个查询都可以表示成由关系运算符组成的树。在 Calcite 中，它会先将 SQL 转换成关系表达式（relational expression），然后通过规则匹配（rules match）进行相应的优化，优化会有一个成本（cost）模型为参考。</p>
<h2 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h2><p>优化器的作用将解析器生成的关系代数表达式转换成执行计划，供执行引擎执行，在这个过程中，会应用一些规则优化，以帮助生成更高效的执行计划。SQL 查询优化器分为两种类型：</p>
<h3 id="RBO"><a href="#RBO" class="headerlink" title="RBO"></a>RBO</h3><ul>
<li>HepPlanner 是一个启发式优化器；</li>
<li>将会匹配定义的所有 rules 直到一个 rule 被满足；</li>
<li>相比CBO优化器更快；</li>
<li>如果没有每次都不匹配规则，可能会有无限递归风险；</li>
<li>规则是基于经验的，经验就可能是有偏的，总有些问题经验解决解不了。</li>
</ul>
<h3 id="CBO"><a href="#CBO" class="headerlink" title="CBO"></a>CBO</h3><ul>
<li>VolcanoPlanner是一个代价优化器；</li>
<li>迭代应用 rules，直到找到cost最小的plan；</li>
<li>成本由关系表达式提供；</li>
<li>不会计算所有可能的计划；</li>
<li>根据已知的情况，如果下面的迭代不能带来提升时，这些计划将会停止优化。</li>
<li><strong>CBO 中有两个依赖：统计信息和代价模型。统计信息的准确与否、代价模型的合理与否都会影响CBO选择最优计划。</strong></li>
</ul>
<p>无论是 RBO，还是 CBO 都包含了一系列优化规则，这些优化规则可以对关系表达式进行等价转换。常见的优化规则包含：谓词下推 Predicate Pushdown、常量折叠 Constant Folding、列裁剪 Column Pruning等等。见如下实例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="number">10</span>+<span class="number">30</span>, users.name, users.age </span><br><span class="line"><span class="keyword">from</span> <span class="keyword">users</span> <span class="keyword">join</span> jobs <span class="keyword">on</span> users.id = jobs.id </span><br><span class="line"><span class="keyword">where</span> users.age &gt; <span class="number">30</span> <span class="keyword">and</span> jobs.id &gt; <span class="number">10</span>;</span><br></pre></td></tr></table></figure>
<p>优化过程如下图所示：</p>
<p><img src="/blog/7dec2e4/sql_optimizer.png" alt></p>
<h3 id="代价模型"><a href="#代价模型" class="headerlink" title="代价模型"></a>代价模型</h3><p>代价模型指的用于计算Cost来选择最优的执行计划，一个好的代价模型可能会影响整个系统的性能。其中，</p>
<ul>
<li>Calcite代价模型</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isLe</span><span class="params">(RelOptCost other)</span> </span>&#123;</span><br><span class="line">    VolcanoCost that = (VolcanoCost)other;</span><br><span class="line">    <span class="keyword">if</span>(xx) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span> == that || <span class="keyword">this</span>.rowCount &lt;= that.rowCount;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span> == that </span><br><span class="line">        || <span class="keyword">this</span>.rowCount &lt;= that.rowCount</span><br><span class="line">        || <span class="keyword">this</span>.cpu &lt;= that.cpu</span><br><span class="line">        || <span class="keyword">this</span>.io &lt;= that.io</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><p>Ali的MaxComputer代价模型</p>
<p>涉及的指标有：CPU、IO、RowCount、Memory、NetWork</p>
</li>
</ul>
<h1 id="适配器"><a href="#适配器" class="headerlink" title="适配器"></a>适配器</h1><h2 id="Schema-amp-Catalog"><a href="#Schema-amp-Catalog" class="headerlink" title="Schema&amp;Catalog"></a>Schema&amp;Catalog</h2><p>用户通过使用SchemaFactory和Schema interfaces来自定义schema。以JSON模型文件声明schemas或者views。通过Table interface自定义table。定义table的record类型。三种表类型：</p>
<ul>
<li>使用ScannableTable interface作为Table的简单实现，来直接枚举所有的rows；</li>
<li>进阶实现FilterableTable，来根据简单的谓词predicates过滤rows；</li>
<li>以TranslatableTable进阶实现Table，将关系型算子转换为执行计划规则；</li>
<li>扩展了CsvStreamScannableTable继承于Scannable Table，流的扩展，即STREAM扩展，窗口扩展，通过联接中的窗口表达式对流的隐式引用等，为流查询提供了支持。</li>
</ul>
<p><img src="/blog/7dec2e4/sql_adapter.png" alt></p>
<p>Catalog主要定义SQL语义相关的元数据与命名空间。Calcite利用schema的层级关系，构造出来namespace的概念，如图所示，schema自身是一个树形结构，这样设计的优点很明显，可以兼容所有已知和未知的数据库，基于namespace结构，schema无论是横向还是纵向都可以无限扩展。</p>
<p><img src="/blog/7dec2e4/schema.png" alt></p>
<h2 id="Avatica"><a href="#Avatica" class="headerlink" title="Avatica"></a>Avatica</h2><p>JDBC驱动程序由Avatica提供支持。Avatica是用于为数据库构建JDBC和ODBC驱动程序以及RPC协议的框架。连接可以是本地连接或远程连接（基于HTTP的JSON或基于HTTP的Protobuf）。JDBC连接字符串的基本形式<code>jdbc:calcite:property=value;property2=value2</code>。</p>
<p><img src="/blog/7dec2e4/avatica.png" alt></p>
<h2 id="CSV适配器示例"><a href="#CSV适配器示例" class="headerlink" title="CSV适配器示例"></a>CSV适配器示例</h2><h3 id="扩展结构"><a href="#扩展结构" class="headerlink" title="扩展结构"></a>扩展结构</h3><p><img src="/blog/7dec2e4/csv.png" alt></p>
<h3 id="配置信息"><a href="#配置信息" class="headerlink" title="配置信息"></a>配置信息</h3><p><img src="/blog/7dec2e4/model.png" alt></p>
<p><img src="/blog/7dec2e4/model-view.png" alt></p>
<p><img src="/blog/7dec2e4/model-stream.png" alt></p>
<h1 id="扩展功能"><a href="#扩展功能" class="headerlink" title="扩展功能"></a>扩展功能</h1><h2 id="Lin4j"><a href="#Lin4j" class="headerlink" title="Lin4j"></a>Lin4j</h2><p>Linq4j（Language-Integrated Query for Java），Calcite可以用于查询多个数据源，而不仅仅是关系数据库。但是，它的目的还不仅仅是支持SQL语言。尽管SQL仍然是主要的数据库语言，但许多程序员还是喜欢LINQ等语言集成语言。与Java或C ++代码中嵌入的SQL不同，语言集成的查询语言允许程序员使用一种语言编写所有代码。 Calcite提供Java语言集成查询（简称LINQ4J），该查询紧密遵循Mirosoft的LINQ为.NET语言制定的约定。</p>
<h2 id="GeoSpatial"><a href="#GeoSpatial" class="headerlink" title="GeoSpatial"></a>GeoSpatial</h2><p>地理空间支持在Calcite中是初步的，使用Calicte的关系代数来实现。 此实现的核心在于添加新的GEOMETRY数据类型，该数据类型封装了不同的几何对象，例如点，曲线和多边形。 预计Calcite将完全符合OpenGIS Simple Feature Access 规范，该规范定义了SQL接口访问地理空间数据的标准。</p>
<h1 id="开源项目应用"><a href="#开源项目应用" class="headerlink" title="开源项目应用"></a>开源项目应用</h1><p><img src="/blog/7dec2e4/calcite_use.png" alt></p>
<h2 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h2><p>Calcite灵活可插拔的架构，使得Hive可以完全使用自己独立的SQL Parser和Validator，<strong>而只用 Calcite 的 Query Optimizer</strong>。而Hive在代码层面和 Calcite 的结合体现在 CalcitePlanner 这个类：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CalcitePlanner</span> <span class="keyword">extends</span> <span class="title">SemanticAnalyzer</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">CalcitePlanner</span><span class="params">(QueryState queryState)</span> <span class="keyword">throws</span> SemanticException </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>(queryState);</span><br><span class="line">    <span class="keyword">if</span> (!HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_CBO_ENABLED)) &#123;</span><br><span class="line">      runCBO = <span class="keyword">false</span>;</span><br><span class="line">      disableSemJoinReordering = <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;   </span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> RelOptPlanner <span class="title">createPlanner</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">      HiveConf conf, Set&lt;RelNode&gt; corrScalarRexSQWithAgg,</span></span></span><br><span class="line"><span class="function"><span class="params">      StatsSource statsSource)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> Double maxSplitSize = (<span class="keyword">double</span>) HiveConf.getLongVar(</span><br><span class="line">            conf, HiveConf.ConfVars.MAPREDMAXSPLITSIZE);</span><br><span class="line">    <span class="keyword">final</span> Double maxMemory = (<span class="keyword">double</span>) HiveConf.getLongVar(</span><br><span class="line">            conf, HiveConf.ConfVars.HIVECONVERTJOINNOCONDITIONALTASKTHRESHOLD);</span><br><span class="line">    <span class="comment">// 省略部分代码</span></span><br><span class="line">    <span class="keyword">boolean</span> isCorrelatedColumns = HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_CBO_STATS_CORRELATED_MULTI_KEY_JOINS);</span><br><span class="line">    <span class="keyword">boolean</span> heuristicMaterializationStrategy = HiveConf.getVar(conf,</span><br><span class="line">        HiveConf.ConfVars.HIVE_MATERIALIZED_VIEW_REWRITING_SELECTION_STRATEGY).equals(<span class="string">"heuristic"</span>);</span><br><span class="line">    HivePlannerContext confContext = <span class="keyword">new</span> HivePlannerContext(algorithmsConf, registry, calciteConfig,</span><br><span class="line">        corrScalarRexSQWithAgg,</span><br><span class="line">        <span class="keyword">new</span> HiveConfPlannerContext(isCorrelatedColumns, heuristicMaterializationStrategy), statsSource);</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">return</span> HiveVolcanoPlanner.createPlanner(confContext);</span><br><span class="line">  &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在Hive看来，CPU和 IO应该优先级比行数更高，先比较这俩，如果相等，才去看行数。而CPU和IO就不用分那么清楚了，合一起就行，怎么合呢，直接相加。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HiveVolcanoPlanner</span> <span class="keyword">extends</span> <span class="title">VolcanoPlanner</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">boolean</span> ENABLE_COLLATION_TRAIT = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">boolean</span> isHeuristic;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Creates a HiveVolcanoPlanner. */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">HiveVolcanoPlanner</span><span class="params">(HivePlannerContext conf)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>(HiveCost.FACTORY, conf);</span><br><span class="line">    isHeuristic = conf.unwrap(HiveConfPlannerContext<span class="class">.<span class="keyword">class</span>).<span class="title">isHeuristicMaterializationStrategy</span>()</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> RelOptPlanner <span class="title">createPlanner</span><span class="params">(HivePlannerContext conf)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> VolcanoPlanner planner = <span class="keyword">new</span> HiveVolcanoPlanner(conf);</span><br><span class="line">    planner.addRelTraitDef(ConventionTraitDef.INSTANCE);</span><br><span class="line">    <span class="keyword">if</span> (ENABLE_COLLATION_TRAIT) &#123;</span><br><span class="line">      planner.addRelTraitDef(RelCollationTraitDef.INSTANCE);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> planner;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//其他省略</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/***</span></span><br><span class="line"><span class="comment"> * <span class="doctag">NOTE:</span>&lt;br&gt;</span></span><br><span class="line"><span class="comment"> * 1. Hivecost normalizes cpu and io in to time.&lt;br&gt;</span></span><br><span class="line"><span class="comment"> * 2. CPU, IO cost is added together to find the query latency.&lt;br&gt;</span></span><br><span class="line"><span class="comment"> * 3. If query latency is equal then row count is compared.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// <span class="doctag">TODO:</span> This should inherit from VolcanoCost and should just override isLE</span></span><br><span class="line"><span class="comment">// method.</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HiveCost</span> <span class="keyword">implements</span> <span class="title">RelOptCost</span> </span>&#123;</span><br><span class="line"> <span class="comment">// ~ Instance fields --------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> <span class="keyword">double</span>                          cpu;</span><br><span class="line">  <span class="keyword">final</span> <span class="keyword">double</span>                          io;</span><br><span class="line">  <span class="keyword">final</span> <span class="keyword">double</span>                          rowCount;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// ~ Constructors -----------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">  HiveCost(<span class="keyword">double</span> rowCount, <span class="keyword">double</span> cpu, <span class="keyword">double</span> io) &#123;</span><br><span class="line">    <span class="keyword">assert</span> rowCount &gt;= <span class="number">0</span>d;</span><br><span class="line">    <span class="keyword">assert</span> cpu &gt;= <span class="number">0</span>d;</span><br><span class="line">    <span class="keyword">assert</span> io &gt;= <span class="number">0</span>d;</span><br><span class="line">    <span class="keyword">this</span>.rowCount = rowCount;</span><br><span class="line">    <span class="keyword">this</span>.cpu = cpu;</span><br><span class="line">    <span class="keyword">this</span>.io = io;</span><br><span class="line">  &#125;</span><br><span class="line">    </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isLe</span><span class="params">(RelOptCost other)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> ( (<span class="keyword">this</span>.cpu + <span class="keyword">this</span>.io &lt; other.getCpu() + other.getIo()) ||</span><br><span class="line">          ((<span class="keyword">this</span>.cpu + <span class="keyword">this</span>.io == other.getCpu() + other.getIo()) &amp;&amp;</span><br><span class="line">          (<span class="keyword">this</span>.rowCount &lt;= other.getRows()))) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isLt</span><span class="params">(RelOptCost other)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> isLe(other) &amp;&amp; !equals(other);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h2><p>Flink 以Calcite Catalog 为核心，上面承载了 Table API 和 SQL API 两套表达方式，最后殊途同归，统一生成为 Calcite Logical Plan（SqlNode 树）；随后验证、优化为 RelNode 树，最终通过 Rules（规则）和 Convention（转化特征）生成具体的 DataSet Plan（批处理）或 DataStream Plan（流处理），即 Flink 算子构成的处理逻辑。</p>
<p><img src="/blog/7dec2e4/flink.png" alt></p>
<h1 id="未来发展"><a href="#未来发展" class="headerlink" title="未来发展"></a>未来发展</h1><p>Calcite的未来工作将集中在新功能的开发以及其适配器体系结构的扩展上：</p>
<ul>
<li>改进Calcite的设计以进一步支持其使用独立引擎，这将需要对数据定义语言（DDL），物化视图，索引和约束的支持；</li>
<li>不断改进计划程序的设计和灵活性，包括使其更具模块化，从而使用户Calcite可以提供计划程序（规则的集合或合并为计划阶段）以执行；</li>
<li>将新的参数化方法[多目标参数化查询优化]纳入优化器的设计；</li>
<li>支持扩展的SQL命令，功能和实用程序集，包括完全符合OpenGIS；</li>
<li>用于非关系数据源的新适配器，例如用于科学计算的阵列数据库；</li>
<li>改进了性能分析和检测。</li>
</ul>
<h1 id="扩展阅读"><a href="#扩展阅读" class="headerlink" title="扩展阅读"></a>扩展阅读</h1><ul>
<li><a href="http://matt33.com/2019/03/07/apache-calcite-process-flow/" target="_blank" rel="noopener">Apache Calcite 处理流程详解（一）</a></li>
<li><a href="http://matt33.com/2019/03/17/apache-calcite-planner/" target="_blank" rel="noopener">Apache Calcite 优化器详解（二）</a></li>
<li><a href="https://mp.weixin.qq.com/mp/homepage?__biz=MzI3MDU3OTc1Nw==&amp;hid=6&amp;sn=573b51439d7f9b0662c54462490df5c2&amp;scene=1&amp;devicetype=Windows+7+x64&amp;version=63030073&amp;lang=zh_CN&amp;nettype=3gnet&amp;ascene=1&amp;session_us=gh_f3c6b41464e9&amp;wx_header=1&amp;uin=&amp;key=&amp;fontgear=2" target="_blank" rel="noopener">Calcite系列</a> | 麒思妙想</li>
</ul>
]]></content>
      <categories>
        <category>计算引擎</category>
      </categories>
      <tags>
        <tag>Calcite</tag>
        <tag>优化器</tag>
        <tag>CBO</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式事务与一致性</title>
    <url>/blog/4c70dee6.html</url>
    <content><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>如果近几年从业于软件工程，特别是服务器端和后端系统开发，那么您很有可能已经被大量关于数据存储和处理的时髦词汇轰炸过了： NoSQL！大数据！Web-Scale！分片！最终一致性！ACID！ CAP定理！云服务！MapReduce！实时！ 在最近十年中，我们看到了很多有趣的进展，关于数据库，分布式系统，以及在此基础上构建应用程序的方式。</p>
<p>本文首先介绍什么是分布式系统，跟集群有什么区别。其次，引入事务ACID的概念以及在单机过渡到分布式后产生哪些问题和解决方式，包括CAP/BASE理论、2PC、3PC等。最后，介绍分布式事务中协调者的共识问题，从而引入Paxos、Raft分布式一致性协议算法。</p>
<a id="more"></a>
<h1 id="分布式与集群"><a href="#分布式与集群" class="headerlink" title="分布式与集群"></a>分布式与集群</h1><p>这里我们可以拿一个电商网站来举例，有登录、商品、下单、支付、物流5个功能。分别说明单体模式、集群模式和分布式的区别。</p>
<ul>
<li>单体模式：该方式，相当于一个整体的单机系统，将5个功能整合在一起对外提供服务；</li>
<li>集群模式：当用户量上涨后，单体模式不能支撑时。可以将单体模式扩展出多个实例出来，通过负载均衡的方式来提高请求的并发；</li>
<li>分布式：将单体模式中的5个功能，拆解为用户服务、商品服务、订单服务、交易服务、物流服务5个服务独立部署，即业务按照面向服务（SOA）的架构拆分整个网站系统，对外提供服务。</li>
</ul>
<p>由此可以看出，分布式是指通过网络连接的多个组件，通过交换信息协作而形成的系统。而集群是指同一种组件的多个实例，形成的逻辑上的整体。</p>
<h1 id="分布式事务"><a href="#分布式事务" class="headerlink" title="分布式事务"></a>分布式事务</h1><p>分布式事务就是指事务的资源分别位于不同的分布式系统的不同节点之上的事务。这里还是拿上一节的例子来说明，比如订单创建成功后同时要修改库存，订单服务和商品服务的操作必须是一个事务型操作，要么全部成功，要么全部失败。除此以外，<strong>产生分布式事务还有分库分表、业务微服务拆分等</strong>。下面从单体模式的ACID理论到分布式CAP理论演进存在的问题展开介绍。</p>
<h2 id="ACID"><a href="#ACID" class="headerlink" title="ACID"></a>ACID</h2><p>ACID是数据库事务的四大特性，包含：原子性（<strong>Atomicity</strong>）、一致性（<strong>Consistency</strong>）、隔离性（<strong>Isolation</strong>）、持久性（<strong>Durability</strong>）。其中：</p>
<ul>
<li>原子性：一个事务内的多个操作共同组成一个原子操作，要么全部执行成功，要么全部执行失败；</li>
<li>一致性：一个原子性操作过程中的多个操作产生的中间状态对外是不可见的，永远只有事务提交前、后的两种状态，即从事务提交前的一致性状态到事务提交后的一致性状态；</li>
<li>隔离性：不同事务的原子操作之间是可以并发执行、相互隔离、互不影响的，如果多个事务操作同一个资源，会基于数据库锁来实现（比如悲观锁、乐观锁）；</li>
<li>持久性：事务一旦提交成功后，数据库的状态变化，即使机器宕机，也不会发生变化（比如写入WAL）。</li>
</ul>
<p>在分布式模式下，引入全局事务管理器（协调器）来管理每个单体模式的事务提交或回滚，最终确定一个完整分布式事务是成功还是失败。</p>
<p>而在分布式环境下，每个单体服务与协调器的通信依靠网络，同时协调器以及单体服务本身，也有宕机不可用的可能，在这样的分布式环境该如何保证事务中的各单体服务的状态一致性，是需要解决的问题。</p>
<h2 id="CAP"><a href="#CAP" class="headerlink" title="CAP"></a>CAP</h2><p>CAP理论起源于2000年，由加州大学伯克利分校的Eric Brewer教授在分布式计算原理研讨会（PODC）上提出，因此 CAP定理又被称作布鲁尔定理（Brewer’s theorem）。2年后，麻省理工学院的Seth Gilbert和Nancy Lynch 发表了布鲁尔猜想的证明，<strong>CAP理论正式成为分布式领域的定理</strong>。</p>
<p>CAP 指 <strong>Consistency</strong>（一致性）、<strong>Availability</strong>（可用性）、<strong>Partition Tolerance</strong>（分区容错性）。其中：</p>
<ul>
<li>一致性 : 所有节点访问同一份最新的数据副本；</li>
<li>可用性: 非故障的节点在合理的时间内返回合理的响应（不是错误或者超时的响应）；</li>
<li>分区容错性 : 分布式系统出现<strong>网络分区</strong>的时候，仍然能够对外提供服务。</li>
</ul>
<p>补充说明：在分布式系统中，多个节点之前的网络本来是连通的，但是因为某些故障（比如部分节点网络出了问题）某些节点之间不连通了，整个网络就分成了几块区域，这就叫<strong>网络分区</strong>。</p>
<p>由此可见，CAP理论中分区容错性P是一定要满足的，在此基础上，只能满足可用性 A 或者一致性 C。因此，<strong>分布式系统理论上不可能选择 CA 架构，只能选择 CP 或者 AP 架构。</strong></p>
<p>当P发生时，要根据业务场景来考虑选择CP还是AP，当P没有发生时，要考虑如何保证CA。</p>
<ul>
<li>CA：放弃分区容错性，加强一致性和可用性，其实就是传统的单机数据库选择；</li>
<li>AP：<strong>放弃强一致性</strong>，加强分区容错性和可用性，如Eureka，服务注册中心；</li>
<li>CP：放弃高可用性，加强一致性和分区容错性，如Zookeeper，集群选主。</li>
</ul>
<h2 id="BASE"><a href="#BASE" class="headerlink" title="BASE"></a>BASE</h2><p>BASE理论起源于2008年， 由eBay的架构师Dan Pritchett在ACM上发表。<strong>BASE理论是对 CAP 中一致性C和可用性A权衡的结果</strong>，其来源于对大规模互联网系统分布式实践的总结，<strong>是基于CAP 定理逐步演化而来的</strong>，它大大降低了我们对系统的要求。</p>
<p>分布式一致性的3种级别：</p>
<ul>
<li>强一致性：系统写入了什么，读出来的就是什么；</li>
<li>弱一致性 ：不一定可以读取到最新写入的值，也不保证多少时间之后读取到的数据是最新的，只是会尽量保证某个时刻达到数据一致的状态；</li>
<li>最终一致性：弱一致性的升级版，系统会保证在一定时间内达到数据一致的状态。</li>
</ul>
<p>BASE 理论本质上是对 CAP 的延伸和补充，更具体地说，是对 CAP 中 AP 方案的一个补充。即，AP 方案只是在系统发生分区的时候放弃一致性，而不是永远放弃一致性。在分区故障恢复后，系统应该达到<strong>最终一致性</strong>。</p>
<p>BASE理论三要素：基本可用（Basically Available）、软状态（Soft-state）、最终一致性（Eventually Consistent）。其中：</p>
<ul>
<li>基本可用：指分布式系统在出现不可预知故障的时候，允许损失部分可用性。但是，这绝不等价于系统不可用。比如系统出现故障，响应时间变慢但还是可以访问。比如系统访问量剧增，保证核心功能可用，非核心功能暂不可用。</li>
<li>软状态：系统中的数据存在中间状态（即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时，不一致），该状态不影响系统的整体可用性。比如服务注册中心，如果读取不一致，顶多服务请求有些许倾斜，但至少不会出现服务不可用。</li>
<li>最终一致性：强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。业界比较推崇是最终一致性级别，但是某些对数据一致要求十分严格的场景比如银行转账还是要保证强一致性。</li>
</ul>
<h2 id="一致性C"><a href="#一致性C" class="headerlink" title="一致性C"></a>一致性C</h2><p>一致性这个词在不同的上下文中具有不同的含义，这个概念理解会有一些混淆：</p>
<ul>
<li><strong>在事务的上下文中</strong>，这个一致性就是常规的概念，即所有参与事务的节点状态保持一致，要么全部成功提交，要么全部失败回滚，不会出现一些节点成功一些节点失败的情况；</li>
<li><strong>在分布式系统上下文中</strong>，这个一致性是指线性一致性，即多副本的系统能够对外表现地像只有单个副本一样，且所有操作都以原子的方式生效。</li>
</ul>
<p>以上两种一致性是有区别的，在实现分布式事务时，需要引入协调者，由于是单点的，就会涉及可用性问题，因此分布式事务的节点需要在协调者故障时就新协调者选取达成<strong>共识（consensus）</strong>，尽快恢复协调者的可用性，从而恢复分布式事务的运转，从而恢复分布式系统本身对外的可用性。</p>
<p><strong>总结来说，分布式事务中的一致性是通过协调者内部的原子操作与多阶段提交协议保证的，不需要共识。但解决分布式事务一致性带来的可用性问题需要用到共识。</strong></p>
<h1 id="实现方案"><a href="#实现方案" class="headerlink" title="实现方案"></a>实现方案</h1><h2 id="2PC"><a href="#2PC" class="headerlink" title="2PC"></a>2PC</h2><p>两阶段提交是一种使分布式系统中所有节点在进行事务提交时保持一致性而设计的一种协议，现在很多数据库都是采用的两阶段提交协议来完成 <strong>分布式事务</strong> 的处理。换句话说，我们要解决的是在分布式系统中，我们所有服务的数据处理要么都成功、要么都失败，即所有服务的 <strong>原子性问题</strong> 。</p>
<p>由于，每个节点可以知道本身的执行状态，不知道其他节点的执行状态，因此需要引入<strong>协调者</strong>组件来统一管理全部节点。</p>
<p>2PC分为投票阶段和提交阶段：</p>
<ul>
<li>第一阶段投票：协调者Coordinator向参与者Cohort询问是否可以执行操作请求并等待响应，Cohort执行事务操作并记录重做和回滚日志，并向coordinator发送agreement或abort响应表示操作结果；相当于每个cohort向coordinator投票，所有cohort投完票后，进入提交阶段；</li>
<li>第二阶段提交：cohort都是agreement，coordinator会向每个cohort发送commit请求，cohort完成操作并释放资源后向协调者返回完成消息，coordinator收到每个cohort完成消息后结束整个事务。一旦有cohort是abort，则coordinator向所有cohort发起rollback请求，cohort根据回滚日志对操作回滚，并向协调者返回完成消息。</li>
</ul>
<p>2PC缺点有：</p>
<ul>
<li>单点故障：coordinator存在挂掉可能性；</li>
<li>同步阻塞：由于单点故障，则部分cohort一直等待coordinator的commit或ollback请求，处于阻塞状态，资源没有释放；</li>
<li>数据不一致：由于单点故障，部分cohort事务提交了，部分cohort事务没有提交。</li>
</ul>
<h2 id="3PC"><a href="#3PC" class="headerlink" title="3PC"></a>3PC</h2><p>三阶段提交为了解决2pc中由于coordinator<strong>单点故障导致的阻塞问题</strong>（<strong>数据不一致的问题并没有，下面描述</strong>），3pc引入<strong>超时机制和准备阶段</strong>，coordinator和cohort在规定时间内没有收到响应，根据当前状态选择commit或abort。如下图示：</p>
<p><img src="/blog/4c70dee6/3pc.png" alt="image-20210303125735991"></p>
<ul>
<li>第一阶段CanCommit：协调者Coordinator向参与者Cohort询问是否可以执行操作请求并等待响应YES或NO。</li>
<li>第二阶段PreCommit：在上一个阶段，如果Coordinator收到所有Cohort返回YES，则Coordinator发送PreCommit请求给Cohort，后续操作如2PC第一阶段。如果Cohort返回NO或者部分Cohort响应超时，则Coordinator向所有Cohort发送abort请求来中断事务，同时Cohort没有收到abort请求或其他来自Coordinator的消息，则也会自动中断事务。</li>
<li>第三阶段DoCommit：在上一个阶段，如果Coordinator收到所有Cohort的ACK消息后，则Coordinator发送doCommit请求给Cohort，后续操作如2PC第二阶段。如果Cohort返回NO或者部分Cohort响应超时，则发送中断请求操作给Cohort进行日志回滚并响应给Coordinator中断事务。其中，即使Cohort没有收到Coordinator的doCommit请求，也会在一定时间内自动提交。</li>
</ul>
<p>3PC缺点有：</p>
<ul>
<li>虽然3pc解决了阻塞问题，但当第二阶段，preCommit发出后，有的Cohort完成事务处于待提交状态，但是Coordinator挂了没有发送请求或Cohort没有收到请求，此时有的Cohort事务中断了，有的Cohort事务自动提交了，从而导致数据不一致问题。</li>
</ul>
<h2 id="TCC"><a href="#TCC" class="headerlink" title="TCC"></a>TCC</h2><p>2PC、3PC都要求Cohort本身支持事务的能力，业务微服务拆分后的场景，一些服务并具备事务，需要通过业务逻辑来控制提交或回滚。所以说事务的提交和回滚就得提升到业务层面而不是数据库层面了，而 TCC 就是一种业务层面或者是应用层的两阶段提交。</p>
<p>TCC是Try、Confirm、Cancel 3个操作的缩写。Try操作对应2PC的一阶段Try，Confirm对应2PC的二阶段commit，Cancel对应2PC的二阶段rollback。</p>
<p>TCC事务的处理流程与2PC两阶段提交类似，不过2PC通常都是在跨库的DB层面，而TCC本质上就是一个应用层面的2PC，需要通过业务逻辑来实现。这种分布式事务的实现方式的优势在于，可以让<strong>应用自己定义数据库操作的粒度，使得降低锁冲突、提高吞吐量成为可能</strong>。</p>
<p>TCC需要几个条件约束：</p>
<ul>
<li>由于网络超时导致不停重试，此时try、commit、cancel对应的操作需要具备<strong>幂等性</strong>；</li>
<li>没有执行try的时候，执行cancel，要具备空回滚能力。</li>
</ul>
<h2 id="Seata"><a href="#Seata" class="headerlink" title="Seata"></a>Seata</h2><p><a href="http://seata.io/zh-cn/" target="_blank" rel="noopener">Seata</a>是一款开源的分布式事务解决方案，致力于在微服务架构下提供高性能和简单易用的分布式事务服务。具有特性：</p>
<ul>
<li>微服务框架支持：目前已支持 Dubbo、Spring Cloud、Sofa-RPC、Motan 和 grpc 等RPC框架，其他框架持续集成中；</li>
<li>AT 模式：提供无侵入自动补偿的事务模式，目前已支持 MySQL、 Oracle 、PostgreSQL和 TiDB的AT模式，H2 开发中；</li>
<li>TCC 模式：支持 TCC 模式并可与AT混用，灵活度更高；</li>
<li>SAGA 模式：为长事务提供有效的解决方案；</li>
<li>XA 模式：支持已实现 XA 接口的数据库的 XA 模式；</li>
<li>高可用：支持基于数据库存储的集群模式，水平扩展能力强。</li>
</ul>
<h1 id="共识算法"><a href="#共识算法" class="headerlink" title="共识算法"></a>共识算法</h1><p>为了提高事务管理器的可用性，防止事务信息的丢失，可以采用多副本机制提高系统的容错性，类似一个高容错、高可用性的分布式存储系统，解决事务管理器或协调器的单点故障问题。</p>
<p>在分布式存储系统中，通常以多副本冗余的方式实现数据的可靠存储。同一份数据的多个副本必须保证一致，而数据的多个副本又存储在不同的节点中，这里的分布式一致性问题就是存储在不同节点中的数据副本（或称为变量）的取值必须一致。</p>
<p>在单机版的数据库系统中，一般通过事务来保证数据的一致性和完整性。而在分布式系统中，为了保证分布式一致性，就需要引入分布式共识算法来保证各个节点之间的一致性。</p>
<p>共识算法保证的就是节点间的强一致性，与应用层面感知的强一致性、最终一致性是有所区别的。</p>
<h2 id="Paxos和Raft算法"><a href="#Paxos和Raft算法" class="headerlink" title="Paxos和Raft算法"></a>Paxos和Raft算法</h2><p><strong>Paxos算法诞生于 1900 年</strong>，这是一种解决分布式系统一致性的经典算法 。但是，由于Paxos算法非常难以理解和实现，不断有人尝试简化这一算法。到了<strong>2013 年才诞生了一个比 Paxos 算法更易理解和实现的分布式一致性算法Raft 算法</strong>。</p>
<ul>
<li>Paxos：Google的Chubby分布式锁服务，采用了Paxos算法；</li>
<li>Raft（muti-paxos）：etcd分布式键值数据库，采用了Raft算法，还有braft, tikv 等；</li>
<li>ZAB（muti-paxos）：ZooKeeper分布式应用协调服务，Chubby的开源实现，采用ZAB算法，但没有抽象成通用的 library。</li>
</ul>
<p>通过 RAFT 提供的一致性状态机，可以解决复制、修复、节点管理等问题，极大的简化当前分布式系统的设计与实现，让开发者只关注于业务逻辑，将其抽象实现成对应的状态机即可。</p>
<p><img src="/blog/4c70dee6/raft.png" alt></p>
<p>基于这套框架，可以构建很多分布式应用：</p>
<ul>
<li>分布式锁服务，比如 Zookeeper；</li>
<li>分布式存储系统，比如分布式消息队列、分布式块系统、分布式文件系统、分布式表格系统等；</li>
<li>高可靠元信息管理，比如各类 Master 模块的 HA。</li>
</ul>
<h2 id="JRAFT介绍"><a href="#JRAFT介绍" class="headerlink" title="JRAFT介绍"></a>JRAFT介绍</h2><h3 id="JRAFT设计"><a href="#JRAFT设计" class="headerlink" title="JRAFT设计"></a>JRAFT设计</h3><p><img src="/blog/4c70dee6/jraft.png" alt></p>
<h3 id="Raft-Group"><a href="#Raft-Group" class="headerlink" title="Raft Group"></a>Raft Group</h3><p><img src="/blog/4c70dee6/jraft_group.png" alt></p>
<h3 id="Multi-Raft-Group"><a href="#Multi-Raft-Group" class="headerlink" title="Multi Raft Group"></a>Multi Raft Group</h3><p><img src="/blog/4c70dee6/jraft_multi_group.png" alt></p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><a href="https://www.oreilly.com/library/view/designing-data-intensive-applications/9781491903063/" target="_blank" rel="noopener">《Designing Data-Intensive Application》</a></li>
<li><a href="https://www.sofastack.tech/projects/sofa-jraft/consistency-raft-jraft/" target="_blank" rel="noopener">分布式一致性 Raft 与 JRaft</a></li>
<li><a href="https://work-jlsun.github.io/2016/11/30/DistributedSystem-Learning-Roadmap.html" target="_blank" rel="noopener">Distributed System Learning Plan</a></li>
<li><a href="https://pingcap.com/blog-cn/linearizability-and-raft/" target="_blank" rel="noopener">线性一致性和 Raft</a></li>
<li><a href="https://github.com/maemual/raft-zh_cn/blob/master/raft-zh_cn.md" target="_blank" rel="noopener">raft论文中文翻译</a></li>
<li><a href="https://raft.github.io/" target="_blank" rel="noopener">raft网站</a></li>
<li><a href="https://github.com/sofastack/sofa-jraft" target="_blank" rel="noopener">java版raft协议实现sofa-jraft</a></li>
<li><a href="https://github.com/baidu/braft" target="_blank" rel="noopener">c++版raft协议实现braft</a></li>
<li><a href="https://mp.weixin.qq.com/s/GhI7RYBdsrqlkU9o9CLEAg" target="_blank" rel="noopener">深入剖析共识性算法 Raft</a>， vivo互联网技术</li>
</ul>
]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式事务</tag>
        <tag>ACID</tag>
        <tag>CAP</tag>
        <tag>Raft</tag>
        <tag>分布式一致性</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-DTCC-参会分享</title>
    <url>/blog/75c48487.html</url>
    <content><![CDATA[<p><img src="/blog/75c48487/IMG_20201221_130843.jpg" alt></p>
<p>本届大会的主题围绕“架构革新、高效可控”为主题，历时三天，涵盖内容包括数据库底层内核代码开发，数据库的架构设计，数据库技术云平台实践以及上层AI和大数据应用。</p>
<h4 id="数据库新趋势"><a href="#数据库新趋势" class="headerlink" title="数据库新趋势"></a>数据库新趋势</h4><p>大会主会场分两天展开，内容起到了提纲挈领的作用。</p>
<p>来自达梦数据库副总经理冯源谈古说今，介绍了数据库从早期到现在的发展，都是围绕实际需求来的。其中，谈到了分布式数据库概念在当时就已经提出来并研究，为啥没有发展起来？主要是当时的通信方式还是以电话线作为连接，网络通信成本很高。后来硬件的发展非常快速，使得数据库机器在scale up层面就可以提升数据库的性能。到了如今，随着数据量的激增，单机数据库的局限性越来越明显，比如谷歌全球性服务、异地多活等需求的出现，使得分布式数据库技术再次发展起来。</p>
<p>总结来说，数据库技术发展遵循了“需求第一性”的原则，要有走向业务的意识。最后，冯总还提到了当下数据库产品处于百花齐放的势头。来自贝壳找房技术总监侯圣文，作为主持人总结到当下我们要把握住数据库技术发展的变与不变的核心问题。我的理解在这么多数据库产品当中，以业务需求或解决业务问题为出发点，才能选择到合适的数据库产品。</p>
<p>从当下业务场景来看，比如电商大促、国际化云服务等，对数据库资源的弹性扩缩容、夸AZ/Region，甚至全球的服务能力提出了强烈的诉求。伴随着云计算技术的发展，提供了资源池化，计算、存储、网络等资源解耦的能力，促使数据库上云或云原生数据库的发展走上舞台。来自阿里集团副总裁、阿里云智能数据库事业部总负责人李飞飞(飞刀）带来他的分享主题“企业级云原生分布式数据库与数据仓库系统：挑战与机遇”。会上，他给出了在云计算加速数据库系统演进的整个业界趋势，这里列一下在不同阶段的代表性数据库产品：</p>
<ul>
<li>1980-1990商业起步阶段：Oracle、IBM DB2、Sybase、SQL Server、Infomix；</li>
<li>1990-2000开源阶段：Postgres、MySQL；</li>
<li>1990-2000分析阶段：Teradata、Sybase IQ、Greenplum；</li>
<li>2000-2010异构NoSQL：Hadoop、HBase、SAP Hana、MongoDB、Redis；</li>
<li>2010-2019云原生、一体化分布式、多模、HTAP：AWS Aurora、Redshift、Azure SQL Database、Google Spanner、Snowflake</li>
</ul>
<p>根据上诉不同阶段的数据库产品，从产品技术以及处理问题维度可以归纳为如下几部分：</p>
<ul>
<li>结构化数据在线处理：RDBMS[SQL+OLTP]；</li>
<li>海量数据计算与分析：数仓Data warehouse、Data Cube[ETL+OLAP];</li>
<li>异构数据类型：结构化、时序、时空、图数据、向量数据、文本数据；</li>
<li>形态变化：关系型数据库、NoSQL/NewSQL数据库、云原生分布式/软硬一体化、多模/HTAP。</li>
</ul>
<p>由此可见，当下数据库新技术趋势就是<strong>以云原生、分布式为基准，具备计算分析一体化(减少数据移动)、存储计算分离(资源池化、解耦)的能力</strong>。另外，飞飞老师从阿里云产品角度阐述了云原生关系数据库PolarDB、云原生分布式数据库PolarDB-X、云原生数仓AnalyticDB以及云原生数据湖DLA(Data Lake Analytics)。这里，我想分享一下令我影响比较深刻或者算是解答我心中疑惑的点：</p>
<ul>
<li><p>存算分离后，降低了计算本地性，是如何规避性能损耗的？</p>
<p>存算分离带来的好处就是计算实例可以按需扩容，在PolarDB或ADB中多个存储实例间采用了Raft同步Redo Log，对计算实例提供基于用户态IO+NVM+RDMA的存储高速访问能力。而存储实例本身使用OSS(存冷数据）作为共享存储，同时加入ESSD(或者类似的加速卡来缓存热数据)，通过高速网络与计算实例交互。上述各种黑科技的加持，可见大厂为了让<strong>公有云</strong>提供给用户更好的产品和服务投入很多。</p>
</li>
<li><p>数据仓库和数据湖的区别？</p>
<p>简单理解的话，数仓类似现实生活中的物品仓库，需要按照一定类目来摆放物品，就是说数据仓库中的数据是经过加工提炼之后的数据，并且按照一定的组织方式来存放。而数据湖中的数据是杂乱无章的，这些数据都有各自的组织方式，但是它们都有明确的元数据信息来描述它们的位置和模式等相关信息，能方便我们通过数据湖内置的计算引擎（比如presto、spark等）进行按需获取加工处理再次以新的数据和模式存储在湖中，从而形成一个一体化的数据分析系统。</p>
</li>
<li><p>云原生在数仓和数据湖中起动什么作用？</p>
<p>当下K8s已经基本成为云操作系统的标准，云原生能力通过K8s来提供FPGA、GPU、CPU、内存和网络的虚拟化资源。因此，在云原生能力的加持下，使得数据库具备弹性扩展能力。不过，这对数据库具备存算分离的能力提出了要求。</p>
</li>
</ul>
<p>同样来自华为云数据库技术专家彭立勋，提出了GaussDB在云原生下存算分离的理念。基于华为多年的存储和网络的优势，提供了分布式一致性可扩展存储能力，在存储层提供元数据管理以及一致性的视图访问能力。而计算层的SQL和索引层，都有各自不同的生态、事务、存储管理方式，比如B-Tree\LSM Tree\KV等。最终，达到了计算层无状态、存储深度融合提供强一致性。</p>
<p>再说说来自阿里云智能数据库产品管理与运营部总经理叶正盛“数据库2025”的分享，他提到Gartner已经将数据库OLTP、OLAP以及大数据能力整合到一起来评估数据库能力。结合百度云数仓和AI平台总架构师马如悦的“百度数据平台发展趋势”的分享，我的理解是作为一个DBMS，重点是要做好数据的管理，用户关心的是数据，至于管理数据的手段，比如传统数据库、大数据技术、云原生、ACID、智能化、软硬件融合等能力，甚至在此之上构建的平台、中台能力，已然成为一个工具或系统需要逐步具备的，只有管理好了数据，才能提供给用户更好的数据服务。最后，叶正盛老师提出对2025数据库的展望，就是迎接数据时代，全面普及云原生数据库以及数据仓库，数据库自动驾驶，国产数据库全面崛起。</p>
<p>个人感觉整个基调就是当前数据库技术的发展，<strong>云原生是个大的趋势</strong>，同时在技术发展过程中要贴近业务的需求，去理解业务，赋能业务，发挥数据库技术在数字化转型中的重要作用。</p>
<h4 id="数据库上云之路"><a href="#数据库上云之路" class="headerlink" title="数据库上云之路"></a>数据库上云之路</h4><p>上面抛出了那么多概念，那各个厂商的上云之路走得又如何呢？</p>
<p>首先在上云方面，在分会场“大数据架构设计”中，来自金山云大数据平台基础架构技术负责人关海南带来了“金山云大数据架构与容器化实践”的分享，传达了上云之路的现状。比如，针对无状态的业务应用，在k8s上运行基本问题不大。而针对有状态的分布式应用，在k8s上存在一些坑。比如zk在k8s网络如果发生抖动时会导致死锁，在kafka中会导致主从不一致性的问题。因此，针对一些有状态的组件以及核心业务目前还在k8s之外。另外，金山云在spark on k8s上也进行了一些尝试，采用社区spark2.x的版本并进行了一些改动。事实上，spark on k8s在社区还支持得不怎么好，主流还是spark on yarn方式。spark on yarn, yarn on k8s的方式其实显得并不那么云原生。来自HashData的CEO简丽荣也提到，在大数据时代是HDFS+YARN，而在云原生时代应该是k8s+oss。</p>
<p>与此同时，在分会场“云原生数据库开发与实践”中，来自京东云专家架构、云数据库技术负责人张成远在“云数据库建设实践”也提出了自己的一些想法。从传统基础软件，到基础软件上云，到云原生模式，意味着不是软件部署在k8s上了就表示具备云原生了，云原生在更大范围内应该是基于从传统IDC到云之后变化，在此基础上构建出的云原生应用，具备了serverless化，到最终数据库价值就应该在于数据了而不是数据库本身，不用考虑以往存储不够怎么办，并发能力不够怎么等之类的问题，而是追求如何发挥数据的价值。而在当下，做得比较云原生的典范，并且在大会中多次被老师提到的，应该就是<strong>snowflake</strong>了。</p>
<p>来自沃趣科技技术中心负责人魏兴华，在“构建新一代数据库平台基础设施”中谈到，他们已经具备管理上万节点的容器平台，对k8s的未来非常看好。他做了一个比较形象的类比，k8s中推出了operator组件能力，相当于传统linux机器之上应用rpm包，同时operator还可以内置各种自检、恢复、备份等运维人员的大量经验，从而实现自动化处理。比如，tidb-operator就可以极大方便在k8s上部署、运维tidb。<strong>他强调，云原生就是一切要以应用为中心的思想，支撑业务应用的快速迭代和创新，比如负载均衡、限流熔断、弹性伸缩、资源调度等等，都交给云来解决就可以了</strong>。</p>
<h4 id="数据-库-架构设计"><a href="#数据-库-架构设计" class="headerlink" title="数据[库]架构设计"></a>数据[库]架构设计</h4><p>另外，想分享的点就是<strong>“技术演进≠产品消亡”</strong>。企业要以”数据“为中心，不仅仅要在数据库本身投入精力，更应该做好数据架构的设计。</p>
<p>从大会多个议题上可以看出，mysql、pg等关系数据库仍然是众多讨论的话题。比如，基于MySQL的星环KunDB、工行MySQL治理实践、MySQL容器化实践、MySQL中间件思考与实践、MySQL高可用之路，就连快手春晚红包支撑百万TPS的TP数据库也是MySQL。为了解决大数据量下关系数据库的正常交易，基于关系型数据的分库分表、垂直水平扩容、分布式事、多副本、高可用的能力需求依旧存在。另外，数据库中间件<a href="http://shardingsphere.apache.org/index_zh.html" target="_blank" rel="noopener">Apache ShardingSphere</a>已于2020年4月16日成为 Apache 软件基金会的顶级项目，也可见一斑。</p>
<p>很多厂商为了追求HTAP的能力，从架构、产品本身做了很多事情。我的理解是主要分为以下三类：</p>
<ul>
<li>一种是平台层面打造TP+AP的整合能力；</li>
<li>一种是数据库本身的TP和AP能力；</li>
<li>一种是满足特定业务场景的多个组件整合、业务特性固化下沉能力。</li>
</ul>
<p>下面列举几个参会中一些厂家的数据架构：</p>
<p><img src="/blog/75c48487/IMG_20201223_092734.jpg"></p>
<center>图二 快手红包场景数据架构</center>

<p><img src="/blog/75c48487/IMG_20201222_144915.jpg" alt></p>
<center>图三 星环KunDB数据架构</center>

<p><img src="/blog/75c48487/tidb-architecture-1.png" alt></p>
<center>图四 TiDB架构</center>

<p>图二、三采用了DTS数据同步工具，将TP中数据同步到AP库中，针对DTS的能力提出了很高的要求，比如数据一致性问题、schema变化等等，这要根据业务的使用场景来定。图四提出了一个比较新颖的方式，将TP中的行存结构，在多副本同步(raft group)的过程中，形成一份列存的副本，用于AP场景的分析。</p>
<p><img src="/blog/75c48487/wx_20201226164145.png" alt></p>
<center>图五 京东时空分析JUST架构</center>

<p>图五，展示了实现时空数据的查询分析能力的技术组件整合能力和关键技术（比如轨迹数据存储）的突破能力。</p>
<h4 id="总结与收获"><a href="#总结与收获" class="headerlink" title="总结与收获"></a>总结与收获</h4><p>通过参与本次2020年度DTCC大会，发现当前数据库、数据仓库、数据湖技术的发展，都在强调对外的生态，比如兼容客户端通信协议，降低迁移的门槛。同时使用的开源软件技术栈也相对比较集中，比如mysql、postgresql、redis、hbase、mongodb、kafka、flink、spark、orc/parquet、presto、clickhouse、oss、k8s等，关注的技术难点比如分布式事务、raft/paxos协议、DTS同步性能、异地多活、有状态应用容器化的坑、大规模集群实践等。</p>
<p>那么除了头部厂商闭源自研能力外，以开源系为基石的很多厂商在数据库产品竞争力也是不容小觑的。能够满足企业级的业务场景，打造适合自己的数据库产品，各家都有自己的产品创新、技术整合能力，甚至会回馈到社区，发挥出更大的价值。关键是要有场景的验证与沉淀，才能孵化并打磨出一款好的产品，优美的技术架构，从而给客户一个更好的数据赋能业务的一整套解决方案。</p>
<p>这也进一步验证了数据库技术发展过程中的变与不变的辩证关系。作为该领域的从业者，要牢牢把握不变的地方，来从容面对日益变化的数据库产品迭代和业务需求。</p>
<p>以上就是本次参会的一些分享，由于个人能力有限，以上内容表述可能会有不当，欢迎交流指正。</p>
<h4 id="扩展阅读"><a href="#扩展阅读" class="headerlink" title="扩展阅读"></a>扩展阅读</h4><ol>
<li>黄东旭, PingCAP CTO, <a href="https://pingcap.com/blog-cn/new-ideas-for-designing-cloud-native-database/" target="_blank" rel="noopener">云原生数据库设计新思路</a>.</li>
<li>李瑞远，博士，京东城市时空数据组负责人，京东城市时空数据引擎 <a href="http://just.urban-computing.cn/" target="_blank" rel="noopener">JUST</a>.</li>
</ol>
]]></content>
      <categories>
        <category>个人日志</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>SparkSQL窗口函数实操</title>
    <url>/blog/8fd5028b.html</url>
    <content><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>我们在数据分析过程中，会需要在数据集中按照某个维度进行分组，并且需要在组内进行一个类似排名、统计、相邻数据提取等分析操作。因此，在SQL 2003标准中，引入了窗口函数的能力。本文将基于Spark 3.0 版本的Spark SQL的<a href="http://spark.apache.org/docs/3.0.0/sql-ref-syntax-qry-select-window.html" target="_blank" rel="noopener">窗口函数能力</a>进行介绍，并给出一些实验示例。最后，基于主要的窗口函数能力，来介绍能够解决哪些业务场景的分析需求。</p>
<h3 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h3><h4 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">window_function OVER</span><br><span class="line">( [  &#123; PARTITION | DISTRIBUTE &#125; BY partition_col_name = partition_col_val ( [ , ... ] ) ]</span><br><span class="line">  &#123; ORDER | SORT &#125; BY expression [ ASC | DESC ] [ NULLS &#123; FIRST | LAST &#125; ] [ , ... ]</span><br><span class="line">  [ window_frame ] )</span><br></pre></td></tr></table></figure>
<h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><ul>
<li><p>窗口函数</p>
<ul>
<li>排名 ： <code>RANK | DENSE_RANK | PERCENT_RANK | NTILE | ROW_NUMBER</code></li>
<li>分析：<code>CUME_DIST | LAG | LEAD</code></li>
<li>聚合：<code>MAX | MIN | COUNT | SUM | AVG | ...</code></li>
</ul>
</li>
<li><p>窗口大小</p>
<ul>
<li>指定窗口的起始行和结束行语法</li>
</ul>
<p><code>{ RANGE | ROWS } { frame_start | BETWEEN frame_start AND frame_end }</code></p>
<ul>
<li><p>窗口起始或结束的语法</p>
<p><code>UNBOUNDED PRECEDING | offset PRECEDING | CURRENT ROW | offset FOLLOWING | UNBOUNDED FOLLOWING</code></p>
<p><code>offset:</code> 基于current row位置的偏移量</p>
</li>
</ul>
<p>备注: frame_end没有设置，默认为<code>CURRENT ROW</code>.</p>
</li>
</ul>
<h3 id="实验示例"><a href="#实验示例" class="headerlink" title="实验示例"></a>实验示例</h3><h4 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> employees (<span class="keyword">name</span> <span class="keyword">STRING</span>, dept <span class="keyword">STRING</span>, salary <span class="built_in">INT</span>, age <span class="built_in">INT</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> employees <span class="keyword">VALUES</span> (<span class="string">"Lisa"</span>, <span class="string">"Sales"</span>, <span class="number">10000</span>, <span class="number">35</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> employees <span class="keyword">VALUES</span> (<span class="string">"Evan"</span>, <span class="string">"Sales"</span>, <span class="number">32000</span>, <span class="number">38</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> employees <span class="keyword">VALUES</span> (<span class="string">"Fred"</span>, <span class="string">"Engineering"</span>, <span class="number">21000</span>, <span class="number">28</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> employees <span class="keyword">VALUES</span> (<span class="string">"Alex"</span>, <span class="string">"Sales"</span>, <span class="number">30000</span>, <span class="number">33</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> employees <span class="keyword">VALUES</span> (<span class="string">"Tom"</span>, <span class="string">"Engineering"</span>, <span class="number">23000</span>, <span class="number">33</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> employees <span class="keyword">VALUES</span> (<span class="string">"Jane"</span>, <span class="string">"Marketing"</span>, <span class="number">29000</span>, <span class="number">28</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> employees <span class="keyword">VALUES</span> (<span class="string">"Jeff"</span>, <span class="string">"Marketing"</span>, <span class="number">35000</span>, <span class="number">38</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> employees <span class="keyword">VALUES</span> (<span class="string">"Paul"</span>, <span class="string">"Engineering"</span>, <span class="number">29000</span>, <span class="number">23</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> employees <span class="keyword">VALUES</span> (<span class="string">"Chloe"</span>, <span class="string">"Engineering"</span>, <span class="number">23000</span>, <span class="number">25</span>);</span><br></pre></td></tr></table></figure>
<h4 id="RANK函数"><a href="#RANK函数" class="headerlink" title="RANK函数"></a>RANK函数</h4><p>rank(): 计算值在一组值中的排名。结果是1加上当前行前面的行数或当前行相等的行数。这些值在序列中会产生间隙。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,dept,salary,<span class="keyword">rank</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> dept <span class="keyword">order</span> <span class="keyword">by</span> salary) <span class="keyword">as</span> <span class="keyword">rank</span> <span class="keyword">from</span> employees;</span><br><span class="line">+<span class="comment">--------+--------------+---------+-------+</span></span><br><span class="line">|  name  |     dept     | salary  | rank  |</span><br><span class="line">+<span class="comment">--------+--------------+---------+-------+</span></span><br><span class="line">| Lisa   | Sales        | 10000   | 1     |</span><br><span class="line">| Alex   | Sales        | 30000   | 2     |</span><br><span class="line">| Evan   | Sales        | 32000   | 3     |</span><br><span class="line">| Fred   | Engineering  | 21000   | 1     |</span><br><span class="line">| Tom    | Engineering  | 23000   | 2     |</span><br><span class="line">| Chloe  | Engineering  | 23000   | 2     |</span><br><span class="line">| Paul   | Engineering  | 29000   | 4     |</span><br><span class="line">| Jane   | Marketing    | 29000   | 1     |</span><br><span class="line">| Jeff   | Marketing    | 35000   | 2     |</span><br><span class="line">+<span class="comment">--------+--------------+---------+-------+</span></span><br></pre></td></tr></table></figure>
<p>dense_rank(): 计算值在一组值中的排名。结果是1加上先前分配的排名值。在排名序列中不会产生间隙。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,dept,salary, <span class="keyword">dense_rank</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> dept <span class="keyword">order</span> <span class="keyword">by</span> salary) <span class="keyword">as</span> <span class="keyword">dense_rank</span> <span class="keyword">from</span> employees;</span><br><span class="line">+<span class="comment">--------+--------------+---------+-------------+</span></span><br><span class="line">|  name  |     dept     | salary  | dense_rank  |</span><br><span class="line">+<span class="comment">--------+--------------+---------+-------------+</span></span><br><span class="line">| Lisa   | Sales        | 10000   | 1           |</span><br><span class="line">| Alex   | Sales        | 30000   | 2           |</span><br><span class="line">| Evan   | Sales        | 32000   | 3           |</span><br><span class="line">| Fred   | Engineering  | 21000   | 1           |</span><br><span class="line">| Chloe  | Engineering  | 23000   | 2           |</span><br><span class="line">| Tom    | Engineering  | 23000   | 2           |</span><br><span class="line">| Paul   | Engineering  | 29000   | 3           |</span><br><span class="line">| Jane   | Marketing    | 29000   | 1           |</span><br><span class="line">| Jeff   | Marketing    | 35000   | 2           |</span><br><span class="line">+<span class="comment">--------+--------------+---------+-------------+</span></span><br></pre></td></tr></table></figure>
<p>row_number(): 窗口内一行一行有序编号。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,dept,salary, row_number() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> dept <span class="keyword">order</span> <span class="keyword">by</span> salary) <span class="keyword">as</span> row_number <span class="keyword">from</span> employees;</span><br><span class="line">+<span class="comment">--------+--------------+---------+-------------+</span></span><br><span class="line">|  name  |     dept     | salary  | row_number  |</span><br><span class="line">+<span class="comment">--------+--------------+---------+-------------+</span></span><br><span class="line">| Lisa   | Sales        | 10000   | 1           |</span><br><span class="line">| Alex   | Sales        | 30000   | 2           |</span><br><span class="line">| Evan   | Sales        | 32000   | 3           |</span><br><span class="line">| Fred   | Engineering  | 21000   | 1           |</span><br><span class="line">| Tom    | Engineering  | 23000   | 2           |</span><br><span class="line">| Chloe  | Engineering  | 23000   | 3           |</span><br><span class="line">| Paul   | Engineering  | 29000   | 4           |</span><br><span class="line">| Jane   | Marketing    | 29000   | 1           |</span><br><span class="line">| Jeff   | Marketing    | 35000   | 2           |</span><br><span class="line">+<span class="comment">--------+--------------+---------+-------------+</span></span><br></pre></td></tr></table></figure>
<p>percent_rank():计算值在一组值中的百分比排名 (rank - 1)/(total_row - 1)。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,dept,salary, <span class="keyword">percent_rank</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> dept <span class="keyword">order</span> <span class="keyword">by</span> salary) <span class="keyword">as</span> <span class="keyword">percent_rank</span> <span class="keyword">from</span> employees;</span><br><span class="line">+<span class="comment">--------+--------------+---------+---------------------+</span></span><br><span class="line">|  name  |     dept     | salary  |    percent_rank     |</span><br><span class="line">+<span class="comment">--------+--------------+---------+---------------------+</span></span><br><span class="line">| Lisa   | Sales        | 10000   | 0.0                 |</span><br><span class="line">| Alex   | Sales        | 30000   | 0.5                 |</span><br><span class="line">| Evan   | Sales        | 32000   | 1.0                 |</span><br><span class="line">| Fred   | Engineering  | 21000   | 0.0                 |</span><br><span class="line">| Chloe  | Engineering  | 23000   | 0.3333333333333333  |</span><br><span class="line">| Tom    | Engineering  | 23000   | 0.3333333333333333  |</span><br><span class="line">| Paul   | Engineering  | 29000   | 1.0                 |</span><br><span class="line">| Jane   | Marketing    | 29000   | 0.0                 |</span><br><span class="line">| Jeff   | Marketing    | 35000   | 1.0                 |</span><br><span class="line">+<span class="comment">--------+--------------+---------+---------------------+</span></span><br></pre></td></tr></table></figure>
<p>ntile(n): 将窗口内的值分为n组。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,dept,ntile(<span class="number">3</span>) <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">rand</span>()) <span class="keyword">as</span> <span class="keyword">group</span> <span class="keyword">from</span> employees;</span><br><span class="line">+<span class="comment">--------+--------------+--------+</span></span><br><span class="line">|  name  |     dept     | group  |</span><br><span class="line">+<span class="comment">--------+--------------+--------+</span></span><br><span class="line">| Chloe  | Engineering  | 1      |</span><br><span class="line">| Jeff   | Marketing    | 1      |</span><br><span class="line">| Paul   | Engineering  | 1      |</span><br><span class="line">| Alex   | Sales        | 2      |</span><br><span class="line">| Evan   | Sales        | 2      |</span><br><span class="line">| Jane   | Marketing    | 2      |</span><br><span class="line">| Tom    | Engineering  | 3      |</span><br><span class="line">| Lisa   | Sales        | 3      |</span><br><span class="line">| Fred   | Engineering  | 3      |</span><br><span class="line">+<span class="comment">--------+--------------+--------+</span></span><br></pre></td></tr></table></figure>
<h4 id="分析函数"><a href="#分析函数" class="headerlink" title="分析函数"></a>分析函数</h4><p>lag(col,offset[,default]): 从指定列的前offset位置的值，如果超出了起始位置，返回default值；<br>lead(col,offset[,default]): 从指定列的后offset位置的值，如果超出了结束位置，返回default值。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,dept,salary, </span><br><span class="line">	salary-lag(salary,<span class="number">1</span>,salary) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> dept <span class="keyword">order</span> <span class="keyword">by</span> salary) <span class="keyword">as</span> <span class="keyword">add</span>, </span><br><span class="line">	<span class="keyword">lead</span>(salary,<span class="number">1</span>,salary) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> dept <span class="keyword">order</span> <span class="keyword">by</span> salary) - salary <span class="keyword">as</span> <span class="keyword">minus</span> </span><br><span class="line"><span class="keyword">from</span> employees;</span><br><span class="line"></span><br><span class="line">+<span class="comment">--------+--------------+---------+--------+--------+</span></span><br><span class="line">|  name  |     dept     | salary  |  add   | minus  |</span><br><span class="line">+<span class="comment">--------+--------------+---------+--------+--------+</span></span><br><span class="line">| Lisa   | Sales        | 10000   | 0      | 20000  |</span><br><span class="line">| Alex   | Sales        | 30000   | 20000  | 2000   |</span><br><span class="line">| Evan   | Sales        | 32000   | 2000   | 0      |</span><br><span class="line">| Fred   | Engineering  | 21000   | 0      | 2000   |</span><br><span class="line">| Tom    | Engineering  | 23000   | 2000   | 0      |</span><br><span class="line">| Chloe  | Engineering  | 23000   | 0      | 6000   |</span><br><span class="line">| Paul   | Engineering  | 29000   | 6000   | 0      |</span><br><span class="line">| Jane   | Marketing    | 29000   | 0      | 6000   |</span><br><span class="line">| Jeff   | Marketing    | 35000   | 6000   | 0      |</span><br><span class="line">+<span class="comment">--------+--------------+---------+--------+--------+</span></span><br></pre></td></tr></table></figure>
<p>cume_dist(): 计算小于等于当前行值的行数/分组内总行数。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,dept,salary, <span class="keyword">cume_dist</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> dept <span class="keyword">order</span> <span class="keyword">by</span> salary) <span class="keyword">as</span> <span class="keyword">cume_dist</span> <span class="keyword">from</span> employees;</span><br><span class="line">+<span class="comment">--------+--------------+---------+---------------------+</span></span><br><span class="line">|  name  |     dept     | salary  |      cume_dist      |</span><br><span class="line">+<span class="comment">--------+--------------+---------+---------------------+</span></span><br><span class="line">| Lisa   | Sales        | 10000   | 0.3333333333333333  | 1/3</span><br><span class="line">| Alex   | Sales        | 30000   | 0.6666666666666666  | 2/3</span><br><span class="line">| Evan   | Sales        | 32000   | 1.0                 | 3/3</span><br><span class="line">| Fred   | Engineering  | 21000   | 0.25                |</span><br><span class="line">| Chloe  | Engineering  | 23000   | 0.75                |</span><br><span class="line">| Tom    | Engineering  | 23000   | 0.75                |</span><br><span class="line">| Paul   | Engineering  | 29000   | 1.0                 |</span><br><span class="line">| Jane   | Marketing    | 29000   | 0.5                 |</span><br><span class="line">| Jeff   | Marketing    | 35000   | 1.0                 |</span><br><span class="line">+<span class="comment">--------+--------------+---------+---------------------+</span></span><br></pre></td></tr></table></figure>
<h4 id="聚合函数"><a href="#聚合函数" class="headerlink" title="聚合函数"></a>聚合函数</h4><p>unbounded: 无边界，没有限制<br>preceding: 前多少行<br>following: 后多少行<br>frame_end不设置默认为current row</p>
<p>range: 逻辑窗口，指定区间的范围取值<br>rows: 物理窗口，指定区间的行数取值</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,dept,salary,</span><br><span class="line">	<span class="keyword">sum</span>(salary) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> dept <span class="keyword">order</span> <span class="keyword">by</span> salary <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">unbounded</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> rows_total,</span><br><span class="line">	<span class="keyword">sum</span>(salary) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> dept <span class="keyword">order</span> <span class="keyword">by</span> salary <span class="keyword">range</span> <span class="keyword">between</span> <span class="keyword">unbounded</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> range_total </span><br><span class="line"><span class="keyword">from</span> employees;</span><br><span class="line"></span><br><span class="line">+<span class="comment">--------+--------------+---------+-------------+--------------+</span></span><br><span class="line">|  name  |     dept     | salary  | rows_total  | range_total  |</span><br><span class="line">+<span class="comment">--------+--------------+---------+-------------+--------------+</span></span><br><span class="line">| Lisa   | Sales        | 10000   | 10000       | 10000        |</span><br><span class="line">| Alex   | Sales        | 30000   | 40000       | 40000        |</span><br><span class="line">| Evan   | Sales        | 32000   | 72000       | 72000        |</span><br><span class="line">| Fred   | Engineering  | 21000   | 21000       | 21000        |</span><br><span class="line">| Chloe  | Engineering  | 23000   | 44000       | 67000        |</span><br><span class="line">| Tom    | Engineering  | 23000   | 67000       | 67000        |</span><br><span class="line">| Paul   | Engineering  | 29000   | 96000       | 96000        |</span><br><span class="line">| Jane   | Marketing    | 29000   | 29000       | 29000        |</span><br><span class="line">| Jeff   | Marketing    | 35000   | 64000       | 64000        |</span><br><span class="line">+<span class="comment">--------+--------------+---------+-------------+--------------+</span></span><br></pre></td></tr></table></figure>
<h3 id="业务场景"><a href="#业务场景" class="headerlink" title="业务场景"></a>业务场景</h3><ol>
<li><a href="https://mp.weixin.qq.com/s/ZBcGtW3rz5i7BQfuEf_x1w" target="_blank" rel="noopener">经典的SparkSQL/Hive-SQL/MySQL面试-练习题</a></li>
<li><a href="https://mp.weixin.qq.com/s/5vHtv_4xM9NLhwjWS8APwA" target="_blank" rel="noopener">使用SQL窗口函数进行商务数据分析</a></li>
<li><a href="https://mp.weixin.qq.com/s/K2TA_PhNzGEkucYxBXqhLw" target="_blank" rel="noopener">Hive开窗函数实战</a></li>
<li><a href="https://mp.weixin.qq.com/s/f4NdIyFpm03u5tYUH5sWqA" target="_blank" rel="noopener">经典Hive-SQL面试题</a></li>
</ol>
]]></content>
      <categories>
        <category>数据库与大数据</category>
      </categories>
      <tags>
        <tag>SparkSQL</tag>
        <tag>窗口函数</tag>
      </tags>
  </entry>
  <entry>
    <title>浅谈OLAP系统核心技术点</title>
    <url>/blog/d3143feb.html</url>
    <content><![CDATA[<h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p><strong>OLAP</strong>系统广泛应用于<strong>BI, Reporting, Ad-hoc, ETL数仓分析等场景</strong>，本文主要从体系化的角度来分析OLAP系统的核心技术点，从业界已有的OLAP中萃取其共性，分为<strong>谈存储，谈计算，谈优化器，谈趋势</strong>4个章节。</p>
<h3 id="存储"><a href="#存储" class="headerlink" title="存储"></a>存储</h3><h4 id="列存的数据组织形式"><a href="#列存的数据组织形式" class="headerlink" title="列存的数据组织形式"></a>列存的数据组织形式</h4><p>行存，可以看做<strong>NSM</strong>（N-ary Storage Model）组织形式，一直伴随着关系型数据库，对于OLTP场景友好，例如innodb[1]的B+树聚簇索引，每个Page中包含若干排序好的行，可以很好的支持tuple-at-a-time式的点查以及更新等；而列存（Column-oriented Storage），经历了早期的DSM（Decomposition Storage Model）[2]，以及后来提出的PAX（Partition Attributes Cross）尝试混合NSM和DSM，在C-Store论文[3]后逐渐被人熟知，用于OLAP，分析型不同于交易场景，存储IO往往是瓶颈，而列存可以只读取需要的列，跳过无用数据，避免IO放大，同质数据存储更紧凑，编码压缩友好，这些优势可以减少IO，进而提高性能。</p>
<h4 id="列存的数据组织形式-1"><a href="#列存的数据组织形式-1" class="headerlink" title="列存的数据组织形式"></a>列存的数据组织形式</h4><p>对于基本类型，例如数值、string等，列存可以使用合适的编码，减少数据体积，在<strong>C-Store</strong>论文中对于是否排序、<strong>NDV</strong>（Number of Distince Values）区分度，这4种排列组合，给出了一些方案，例如数值类型，无序且NDV小的，转成bitmap，然后bit-packing编码。其他场景的编码还有 varint、delta、RLE（Run Length Encoding）、字符串字典编码（Dictionary Encoding）等，这些轻量级的编码技术仅需要多付出一些CPU，就可以节省不小的IO。对于复杂类型，嵌套类型的可以使用Google Dremel论文[4]提出Striping/Assembly算法（开源 Parquet），使用Definition Level+Repetition Level做编解码。一些数值类型有时也可以尝试大一统的用bitshuffle [14]做转换，配合压缩效果也不错，例如KUDU [7]和百度Palo（Doris）中有应用。在编码基础上，还可以进行传统的压缩，例如lz4、snappy、zstd、zlib等，一般发现压缩率不理想时可以不启用。</p>
<p>一些其他的选项，包括HBase，实际存储的是纯二进制，仅支持Column Family，实际不是columnar format，一些序列化框架和Hadoop融合比较好的，例如 Avro，也不是列式存储。</p>
<h4 id="储存格式"><a href="#储存格式" class="headerlink" title="储存格式"></a>储存格式</h4><p>现代的OLAP往往采用<strong>行列混存</strong>的方案，采用<strong>Data Block + Header/Footer</strong>的文件结构，例如parquet、orc，Data Block使用 Row Group（parquet的叫法，orc叫做Stripe） -&gt; Column Chunk -&gt; Page三层级，每一层又有metadata，Row Group meta包含row count，解决暴力 count(*)，Column Chunk meta包含max、min、sum、count、distinct count、average length等，还有字典编码，解决列剪枝，并且提供基础信息给优化器，Page meta同样可以包含max、min等，跳页用于加速计算。</p>
<h4 id="存储索引"><a href="#存储索引" class="headerlink" title="存储索引"></a>存储索引</h4><p>在parquet、orc中，除了列meta信息外，不提供其他索引，在其他存储上，支持了更丰富的索引，索引可以做单独的块（Index Block），或者形成独立的文件。例如阿里云ADB [5]，对于cardinality较小的，可以做<strong>bitmap索引</strong>，多个条件下推使用and/or。<strong>倒排索引也是可选的</strong>，需要在空间和性能上有所折中，还可以支持全文检索Bloom Filter可以按照page粒度做很多组，加速”in”, “=”查询，快速做page剪枝。另外，假设数据按照某个列或者某几个列是有序的，这样可以减少数据随机性，好处在于相似的数据对编码压缩有利，而且可以基于Row Group、Column Chunk、Page的meta做有效的过滤剪枝，有序列可以使用<strong>B-Tree</strong>、<strong>Masstree</strong> [6]（例如KUDU[7]），或者借鉴<strong>LevelDB</strong>的思想，在Index Block内对有序列做<strong>稀疏索引</strong>，方便<strong>二分查找</strong>，Index Block可以用<strong>LRU Cache</strong>尽量常驻内存，这样有利于按照排序列做点查（point query）和顺序扫描的范围查询（range query）。另外其他列也可以做稀疏有序索引。有序列如果是唯一，可以看做OLTP中主键的概念。</p>
<h4 id="分布式存储"><a href="#分布式存储" class="headerlink" title="分布式存储"></a>分布式存储</h4><p>DAC（Divide And Conquer）在分布式领域也是屡试不爽，要突破单机存储大小和IO限制，就需要把一个文件划分为若干小分片（sharding），以某个列做round-robin、constant、random、range、hash 等，分布在不同的文件或者机器，形成分布式存储。</p>
<p><strong>第一类，存储计算一体的架构</strong></p>
<p>基于单机磁盘（SATA、SSD、NVM），例如Greenplum基于PostgreSQL，还有ClickHouse、百度 Palo（Doris）等，是<strong>Share Nothing架构</strong>，可实现多副本，扩容需要reshard往往比较耗时。</p>
<p><strong>第二类，存储计算分离</strong></p>
<p>文件存在分布式存储（GFS、HDFS）或者对象存储（S3、OSS、GCS），是<strong>Share Everthing（Share Storage）架构</strong>，好处在于扩展性和可用性的提高，由于存储网络延迟，所以一般都做批量、追加写，而非随机写，这把双刃剑也加大了OLAP在实时更新上难度，所以很多都放弃了实时写和ACID能力。</p>
<p>存储计算分离的架构上，例如文件如果存在HDFS上，每个分片是一个HDFS block（例如128MB 大小），便于高吞吐大块 IO 顺序读，一个Row Group大小等于block size，便于上层计算引擎，例如Spark SQL作业并行计算。</p>
<p>存储计算一体架构，可以更专心的设计文件和分片管理系统，采用Centralized Master +多个Tablet架构，例如KUDU以及OLTP新兴的Tikv，分片的多副本依赖于一致性协议Multi-Paxos或者支持乱序提交的Raft协议，多个分片组成Raft-Group，这样可以打散一个表（文件）到多分片多副本的架构上，既保证了扩展性又保证了高可用。Centralized Master管理分片存放的位置，元数据，便于负载均衡、分裂合并等。</p>
<p><strong>示例：</strong>数据按 uid range 分片。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">shard1              shard2</span><br><span class="line">+<span class="comment">---------------+  +---------------+  </span></span><br><span class="line">|uid|   date    |  |uid|   date    | </span><br><span class="line">+<span class="comment">---------------+  +---------------+ </span></span><br><span class="line">| 1 | 2020-11-11|  | 3 | 2020-11-13|</span><br><span class="line">| 2 | 2020-11-12|  | 4 | 2020-11-14|</span><br><span class="line">+<span class="comment">---------------+  +---------------+</span></span><br></pre></td></tr></table></figure>
<p><strong>示例：</strong>数据按uid hash分片，f(uid) = uid mod 2</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">shard1              shard2</span><br><span class="line">+<span class="comment">---------------+  +---------------+  </span></span><br><span class="line">|uid|   date    |  |uid|   date    | </span><br><span class="line">+<span class="comment">---------------+  +---------------+ </span></span><br><span class="line">| 1 | 2020-11-11|  | 2 | 2020-11-12|</span><br><span class="line">| 3 | 2020-11-13|  | 4 | 2020-11-14|</span><br><span class="line">+<span class="comment">---------------+  +---------------+</span></span><br></pre></td></tr></table></figure>
<h4 id="数据进一步分区"><a href="#数据进一步分区" class="headerlink" title="数据进一步分区"></a>数据进一步分区</h4><p>数据分片的基础上，可以进行更细粒度的分区（partition），便于做分区剪枝（partition prune），直接跳过不需要扫描的文件。分片（sharding）策略按照range，可以优化OLAP的范围查询和快速点查；按照hash分区，可以充分打散，有效解决hotspot热点。将二者结合，做二级分区（two-level），例如阿里云ADB、ClickHouse、KUDU，支持DISTRIBUTED BY HASH再PARTITION BY RANGE，而百度Palo（Doris）一般先按时间一级分区，更好做冷热数据区分，二级分区分桶采用hash。</p>
<p><strong>示例：</strong>数据按照二级分区，一级分区uid hash分片，二级分区按date，形成4个文件</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">shard1              shard2</span><br><span class="line">+<span class="comment">---------------+  +---------------+  </span></span><br><span class="line">|uid|   date    |  |uid|   date    | </span><br><span class="line">+<span class="comment">---------------+  +---------------+ </span></span><br><span class="line">| 1 | 2020-11-11|  | 2 | 2020-11-12|</span><br><span class="line">+<span class="comment">---------------+  +---------------+</span></span><br><span class="line"></span><br><span class="line">+<span class="comment">---------------+  +---------------+  </span></span><br><span class="line">|uid|   date    |  |uid|   date    | </span><br><span class="line">+<span class="comment">---------------+  +---------------+ </span></span><br><span class="line">| 3 | 2020-11-13|  | 3 | 2020-11-14| </span><br><span class="line">+<span class="comment">---------------+  +---------------+</span></span><br></pre></td></tr></table></figure>
<h4 id="实时写入和-ACID"><a href="#实时写入和-ACID" class="headerlink" title="实时写入和 ACID"></a>实时写入和 ACID</h4><p>随着实时数仓和<strong>HTAP，HSAP</strong> [8]等概念的兴起，对于传统数据处理的Lambda架构弊端就凸显出来，链路长，数据冗余，数据一致性不好保证等。</p>
<p>融合OLTP的能力，第一点就是在之前所述的immutable table file上做实时增删改，要保证低延迟，高吞吐，可以借鉴<strong>LSM-Tree</strong>思想，优化写吞吐，将流式的低延迟随机写，最终变成聚批mini-batch的group commit顺序写，依赖<strong>Write-ahead Log</strong>保证持久性，最终形成<strong>Base + Delta</strong>的文件结构，读流程包括点查或者扫描，基于Base的同时，还需要Merge Delta的变化，另外后台通过<strong>minor compaction</strong>和<strong>major compaction</strong>不断的合并Delta和Base，可以不断优化读性能，在阿里云ADB，KUDU，Google MESA [9]里面都采用了类似的方案。</p>
<p>在读写一致性层面，需要提供<strong>ACID</strong>和<strong>事务隔离</strong>特性，比较好保证单行和mini-batch的原子性，持久性不言而喻，对于一致性和事务隔离，可以采用<strong>MVCC</strong>机制，每个写都带有version，很简单的实现带版本查一致性，快照一致性（snapshot isolation）。</p>
<h3 id="计算"><a href="#计算" class="headerlink" title="计算"></a>计算</h3><h4 id="查询步骤"><a href="#查询步骤" class="headerlink" title="查询步骤"></a>查询步骤</h4><p>SQL 语言是 OLAP 的标配，一个完整的 SQL 查询步骤包括：</p>
<ol>
<li>SQL词法解析，语法解析</li>
<li>形成抽象语法树 (AST)</li>
<li>校验检查</li>
<li>AST转成关系代数表达式 (relational algebra)；</li>
<li>根据关系代数表达式生成执行计划，先生成逻辑执行计划 (logical plan)</li>
<li>经过优化器生成最优的执行计划</li>
<li>根据执行计划生成物理执行计划 (physical plan)</li>
<li>最终交由执行器执行并返回结果</li>
</ol>
<p>由SQL到AST的过程，类库和工具较多，C++可用Lex/Yacc，Java可用JavaCC/ANTLR，也可以自己手写实现。由AST到关系代数表达式，可以使用visitor模式遍历。下一章节谈优化器，本节聚焦在物理执行计划后的执行阶段。</p>
<h4 id="OLAP-数据建模分类"><a href="#OLAP-数据建模分类" class="headerlink" title="OLAP 数据建模分类"></a>OLAP 数据建模分类</h4><p><strong>Relational OLAP (ROLAP)</strong> </p>
<p>对SQL支持好，查询灵活，使用组合模型，雪花或者星型模型组织多张表。ROLAP计算的数据规模往往小于离线大数据计算（Hive/Spark），ROLAP产品很多，包括传统的Greenpulm、Vertica、Teradata，SQL-on-Hadoop系的Presto、Impala、Spark SQL、HAWQ，云计算厂商的阿里云ADB、Google BigQuery，AWS RedShift，有学术界出品的 MonetDB [10]，还有新兴的 <strong>ClickHouse</strong>。如果把查询阶段分为</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">                   cache</span><br><span class="line">                     /\</span><br><span class="line">                     |</span><br><span class="line">pre-computing -&gt; computing -&gt; post computing</span><br></pre></td></tr></table></figure>
<p>上面的提到的存储技术更多是为了ROLAP在computing阶段优化考虑的，如果把计算中的熵前置到pre-computing阶段做预计算，也可以大幅优化computing阶段。</p>
<p><strong>Multi-dimensional OLAP (MOLAP)</strong> </p>
<p>可以把数据预计算，有些场景下不一定需要细粒度的fact，可以严格区分维度列和指标列，使用<strong>Kylin、Druid</strong>等，利用上卷 (roll-up) 做数据立方体 (data cube)，这样可以大大减少OLAP场景下聚合查询的IO，另外<strong>百度Palo</strong>、<strong>Google MESA</strong>，基于上卷操作做物化视图，也减少了IO消耗，所以他们对于高并发查询支持普遍较好，但是缺点就在于查询不够灵活，数据有冗余。下文主要针对ROLAP谈计算。</p>
<h4 id="计算引擎分类"><a href="#计算引擎分类" class="headerlink" title="计算引擎分类"></a>计算引擎分类</h4><p>物理执行计划往往是一个DAG，每个节点都是一种operator，最下游的叶子节点一般都是TableScan operator，这个DAG的分布式执行器就是计算引擎 (Query Engine)，分为两个流派。</p>
<p><strong>第一类是基于离线计算引擎，</strong>例如<strong>Hive on MR，Spark SQL，阿里云MaxCompute</strong>，支持超大规模的数据，进行了容错保证，多个stage落盘(spill to disk)，使用resource manager调度和queueing，作业可能持续非常长的时间，占用大量资源，并发低。</p>
<p><strong>第二类是MPP，</strong>例如<strong>Greenplum、Presto、Impala、阿里云ADB，RedShift</strong>支持大规模数据，不需要resource manager耗时的分配资源和调度任务，long-running的task manager，只需要轻量级的调度，查询一般不容错，算子并行执行，并行度有限制避免straggler node影响TP99，相比基于离线的计算引擎往往是短任务，查询耗时不会太长。</p>
<p><strong>Presto、Impala</strong>属于SQL-on-Hadoop MPP，利用 Hive metastore，直接读取<strong>Parquet</strong>、<strong>ORC</strong>等文件格式，<strong>Greenplum</strong>、<strong>RedShift</strong>基于<strong>PostgreSQL</strong>，阿里云<strong>ADB</strong>采用私有的数据存储技术，计算存储分离的架构，存储表到分布式存储盘古上。</p>
<h4 id="MPP-架构"><a href="#MPP-架构" class="headerlink" title="MPP 架构"></a>MPP 架构</h4><p>通用的MPP架构组成由coordinator，worker，metastore，scheduler组成，各个产品名称不同而已。通过metastore可以获取表元信息、分区/分片位置、辅助coordinator做校验等。</p>
<p>coordinator负责从SQL到物理执行计划的生成以及执行，一个计划往往被切分为多个plan fragment，plan fragment之间通过添加ExchangeOperator来传递数据（例如 shuffle），逻辑上plan fragment等同于stage，scheduler 管理所有worker节点，coordinator调用scheduler分发stage到不同的worker节点执行，就形成了很多task。</p>
<p>一个task，包含一个或者多个operator算子，最简单的算子实现就是解释执行(interpreted)的模式。算子包括Project、Filter、TableScan、HashJoin、Aggregation 等，叶子节点一般是TableScan，拉取存储中数据。</p>
<p>MPP架构就是充分利用分布式的特性，让算子分布式的并行计算，同时task内部也可以做并行处理，加速查询。</p>
<h4 id="计算执行"><a href="#计算执行" class="headerlink" title="计算执行"></a>计算执行</h4><p><strong>数据流</strong></p>
<p><strong>DAG</strong>在进行数据流动时，采用pipeline方式，也就是上游stage不用等下游stage完全执行结束就可以拉取数据并执行计算。数据不落盘，算子之间通过内存直接拷贝到socket buffer发送，需要保证内存足够大，否则容易OOM。</p>
<p><strong>火山模型 (Volcano-style)</strong></p>
<p>是一种Row-Based Streaming Iterator Model算子的实现，只需要 open、next、close三个函数，就可以实现数据从底向上的“拉”取，驱动计算进行。</p>
<p><strong>向量化执行 (Vectorized query)</strong></p>
<p><strong>MonetDB</strong>论文提出了火山模型的改进方案——向量化执行，火山模型tuple-at-a-time的实现，每个算子执行完传递一行给上游算子继续执行，函数调用过多，且大量的虚函数调用，条件分支预测失败，直接现象就是CPU利用率低(low IPC)。</p>
<p>而现代的CPU有多级流水线可以实现指令级并行，超标量(super scalar)实现乱序执行，对于forloop可以有效优化，超线程还能实现线程级并行，而CPU多级的Cache，以及cache line的有效利用避免cache miss，再配合编译器的优化，都会大大加速计算过程。</p>
<p>向量化执行的思想就是算子之间的输入输出是一批（Batch，例如上千行）数据，这样可以让计算更多的停留在函数内，而不是频繁的交互切换，提高了CPU的流水线并行度，而且还可以使用SIMD指令，例如AVX指令集来实现数据并行处理。</p>
<p>实际实现中，例如Impala各个算子的input虽然是RowBatch，但除了TableScan算子，其他的也是火山模型执行式的row by row 处理，TableScan读存储，列式内存布局加速pushdown的filter执行，aggregation下推后还可以使用SIMD指令加速聚合。但是向量化也会带来额外的开销，就是物化中间结果(materlization)，以牺牲物化的开销换取更高的计算性能。</p>
<p><strong>动态代码生成 (codegen)</strong></p>
<p>解释执行(interpreted)的算子，因为面向通用化设计，大数据集下往往效率不高，可以使用codegen动态生成算子逻辑，例如<strong>Java使用ASM 或者Janino</strong>，<strong>C++使用LLVM IR</strong>，这样生成的算子更贴近计算，减少了冗余和虚函数调用，还可以多个算子糅合成一个函数。另外表达式计算的codegen还可以做的更极致，一些简单的计算可以做成汇编指令，进一步加速。</p>
<p>关于向量化或者codegen，孰优孰劣，论文Everything You Always Wanted to Know About Compiled and Vectorized Queries But Were Afraid to Ask [11]进行了深入的对比。二者也可以融合，通过codegen生成向量化执行代码，另外也不一定做wholestage codegen，和解释执行也可以一起配合。</p>
<p>计算的耗时有一部分会损耗在IO、CPU的闲置上。内存的布局和管理，行式布局还是列式布局，对于CPU Cache是否友好，内存池还是按需分配，都会影响着系统的吞吐，C++可自行维护Arena或者使用jemalloc等框架，而Java的heap memory比较低效还影响GC，因此<strong>使用Unsafe API操作堆外内存</strong>。另外<strong>Arrow</strong>的兴起，也对于跨进程通信后，不必进行数据反序列化、内存分配再拷贝，就可以读取列式的数据，也进一步加速了计算。</p>
<h4 id="常见算子实现"><a href="#常见算子实现" class="headerlink" title="常见算子实现"></a>常见算子实现</h4><p>TableScan算子直读底层数据源，例如Presto，抽象了很好了connector，可对接多种数据源（Hadoop，对象存储等），一般都支持projection、filter，因此可以做filter pushdown和projection pushdown到TableScan，另外在做predicate的时候可以使用lazy materialization（延迟物化）的技巧去short circuit掉先不需要的列。</p>
<p>Join算子的实现，如果两个表都很小，最简单的利用in-memory hash join、simple nested loop join；一大一小，可以广播小表 (broadcast)，一般维度表都比较小，如果大表有索引，扫描小表，根据大表做index lookup join，否则基于小表做build table，大表做probe table，实现hash join；两个大表，如果两个表的join key的一级分区策略相同，则可以很好的对齐，避免大表shuffle，直接在大表的shard 做local join，如果不能对齐，则两个表按照join key shuffle到其他节点，重分布式后再做join；另外如果两个表的join key有序，还可以使用sort-merge join。</p>
<h4 id="资源管理与调度"><a href="#资源管理与调度" class="headerlink" title="资源管理与调度"></a>资源管理与调度</h4><p>MPP架构下coordinator需要scheduler调度task到worker节点，对于长计算任务或者ETL任务，会占用很多资源，导致OLAP的并发度受限，其他请求需要排队，因此很难服务对外在线请求，为了迎合混合负载，传统scheduler简单粗暴的调度和资源管理已经无法满足要求，因此可以进行任务的<strong>fine grained schedule</strong>避免空闲资源，请求间对资源的使用尽量的隔离，避免bad query吃满资源，简单的策略可以通过label化集群，或者用SQL hint实现，区分长短计算任务，让更多的短任务也可以快速得到响应。当OLAP系统足够高性能后，更好的资源管理和调度，将会提升OLAP为一个支持高并发、低延迟的，可对外提供在线服务的系统，而不仅仅是一个in-house的分析系统。</p>
<h3 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h3><p>查询优化器不光是传统数据库DB2、Oracle、MySQL的核心，在OLAP里也至关重要。AST转为SQL形式化表达语言——关系代数表达式 (relational algebra)，代码实现就是一颗关系运算符组成的树，查询优化主要是围绕着“等价交换”的原则做相应的转换，优化关系代数表达式。关系代数的基本运算包括投影(project)、选择 (select)、并 (union)、差 (set difference)、连接(join)等。优化器分为Rule-Based Optimizer (RBO)和Cost-Based Optimizer (CBO)两类。</p>
<h4 id="RBO"><a href="#RBO" class="headerlink" title="RBO"></a>RBO</h4><p>会将原有表达式裁剪掉，遍历一系列规则(Rule)，只要满足条件就转换，生成最终的执行计划。一些常见的规则包括分区裁剪(Partition Prune)、列裁剪、谓词下推(Predicate Pushdown)、投影下推(Projection Pushdown)、聚合下推、limit下推、sort下推、常量折叠 (Constant Folding)、子查询内联转join等。</p>
<h4 id="CBO"><a href="#CBO" class="headerlink" title="CBO"></a>CBO</h4><p>会将原有表达式保留，基于统计信息+代价模型，尝试探索生成等价关系表达式，最终取代价最小的执行计划。CBO的实现有两种模型，<strong>Volcano</strong>模型，<strong>Cascades</strong>模型，很流行的<strong>Calcite</strong> [12] 使用<strong>Volcano</strong>模型，比如<strong>Flink</strong>、<strong>Hive</strong>都基于此，<strong>Orca</strong>使用<strong>Cascades</strong>模型，在<strong>Greenpulm</strong>中使用。</p>
<p>优化器需要尽量的高效，高效的生成搜索空间、动态规划遍历搜索空间(top down、bottom up、depth-first等)，高效的剪枝策略等都可以加速优化过程。统计信息包括表数据大小，row count。查询列的trait metadata (min、max、cardinality等)，sortness、可利用的索引，直方图(Histogram)分布统计等。</p>
<p>Join是OLAP最消耗吞吐的算子之一，也是ROLAP对于分析最强大的地方，可以进行多表的关联查询，常见的优化手段包括join reorder，使用left-deep tree还是bushy tree执行join，以及如何选择join算法实现（上节提到的各种join实现的选择），结合高效索引结构实现的index join，group by 下推、top-n下推等。</p>
<h3 id="趋势"><a href="#趋势" class="headerlink" title="趋势"></a>趋势</h3><p>OLAP领域经历了从 RDBMS 建立起来的SQL + OLAP，到ETL +专有OLAP的数仓阶段，目前仍在不断演进，更多的云厂商也加入这个领域，展示出、也正经历着如下的趋势。</p>
<p><strong>实时分析</strong></p>
<p>传统的OLAP需要做各种pipeline、ETL导入数据，这样的架构会存储多份数据，冗余并且一致性不好保证，也引入过多的技术栈和复杂度，也不能满足实时分析，即使mini-batch的处理仍然需要最快数分钟。</p>
<p>业界的趋势在于赋予OLAP高吞吐实时写，提供实时查询能力，例如上游数据源，经过流计算系统，老的架构基于lambda，写历史数据到存储再清洗，实时数据入一些NoSQL，使用方需要做各种数据源merge操作，流行的方式是流计算系统直接写OLAP，这样避免了数据孤岛，保证了链路简单，阿里云<strong>hologres</strong>团队提出的<strong>HSAP</strong>(Hybrid Serving/Analytical Processing) [8]正是这种理念。</p>
<p><strong>HTAP</strong></p>
<p>事务处理和分析处理在一个数据库中提供，是最理想的状态，但是二者的技术体系往往又很难融合，因此现在很多数据库厂商都在做这方面的尝试，保证数据一致性是很大的挑战，一种思路是从OLTP到OLAP，多副本存储时，有些副本是专门为OLAP定制的，使用专用的OLAP引擎提供查询，另外就是赋予ACID和事务能力到OLAP系统中，使得OLAP也支持INSERT/DELETE/UPDATE操作。</p>
<p><strong>云原生</strong></p>
<p>传统的OLAP，例如Exadata等依赖于高端硬件，很多on-premise的解决方案也面临扩展性和成本问题，云原生的架构通过虚拟化技术，可实现更好的弹性计算，如果采用存储计算分离的架构还可以实现弹性存储，这些水平扩展的机制可以很好的兼顾高性能、成本和扩展性。</p>
<p><strong>多模数据结构分析</strong></p>
<p>不仅限于结构化数据，半结构化、非结构化的数据分析也逐渐在OLAP中应用，包括向量检索，JSON、ARRAY检索等。</p>
<p><strong>软硬一体化</strong></p>
<ul>
<li>计算方面，更好利用多核并行，使得查询满足<strong>NUMA-aware</strong>，亲和性(affinity)可以进一步榨干系统的吞吐，使用<strong>FPGA</strong>、<strong>GPU</strong>硬件加速，利用这些硬件提供的超高带宽和深度流水线可以加速一些向量计算和聚合操作；</li>
<li>存储方面，随着存储查询带宽增大、延迟降低，可以应用更多新存储，例如Intel傲腾<strong>NVM 3D-XPoint SSD</strong> [13]提供2.6G/s的顺序读吞吐，高并发点查延迟可控制在10几个us；</li>
<li>网络方面，基于<strong>RDMA</strong>网络，DPDK等技术可替换传统的tcp，做kernel bypass，降低网络延迟。上层的OLAP软件可以基于这些新硬件做更深度的定制，提供更极致的性能。</li>
</ul>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>[1] <a href="从MySQL InnoDB物理文件格式深入理解索引">从MySQL InnoDB物理文件格式深入理解索引</a></p>
<p>[2] <a href="inf.ufpr.br/eduardo/ens">A DECOMPOSITION STORAGE MODEL</a></p>
<p>[3] <a href="vldb.org/archives/websi">C-Store: A Column-oriented DBMS</a></p>
<p>[4] <a href="static.googleusercontent.com">Dremel: Interactive Analysis of Web-Scale Datasets</a></p>
<p>[5] <a href="vldb.org/pvldb/vol12/p2">AnalyticDB: Real-time OLAP Database System at Alibaba Cloud</a></p>
<p>[6] <a href="pdos.csail.mit.edu/pape">Cache craftiness for fast multicore key-value storage</a></p>
<p>[7] <a href="kudu.apache.org/kudu.pd">Kudu: Storage for Fast Analytics on Fast Data</a></p>
<p>[8] <a href="阿里云Hologres：数据仓库、数据湖、流批一体，终于有大神讲清楚了！">数据仓库、数据湖、流批一体，终于有大神讲清楚了</a></p>
<p>[9] <a href="static.googleusercontent.com">Mesa: Geo-Replicated, Near Real-Time, Scalable Data Warehousing</a></p>
<p>[10] <a href="w6113.github.io/files/p">MonetDB/X100: Hyper-Pipelining Query Execution</a></p>
<p>[11] <a href="vldb.org/pvldb/vol11/p2">Everything You Always Wanted to Know About Compiled and Vectorized Queries But Were Afraid to Ask</a></p>
<p>[12] <a href="arxiv.org/pdf/1802.1023">Apache Calcite: A Foundational Framework for Optimized Query Processing Over Heterogeneous Data Sources</a></p>
<p>[13] <a href="Intel® Optane™ DC SSD Series">Intel Optane Series</a></p>
<p>[14] <a href="github.com/kiyo-masui/b">bitshuffle</a></p>
<blockquote>
<p> 转载自 <a href="https://zhuanlan.zhihu.com/p/163236128" target="_blank" rel="noopener">知乎专栏- 存储与索引</a></p>
</blockquote>
]]></content>
      <categories>
        <category>数据库与大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>OLAP</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>Paper翻译 Spark SQL Relational Data Processing in Spark</title>
    <url>/blog/8973d72a.html</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Spark SQL是Apache Spark中的一个新模块，用于将关系处理和Spark的函数编程API相集成。基于Shark的经验，Spark SQL使得Spark开发者能够充分利用关系处理（声明式查询和优化后的存储）的优势以及让SQL用户调用Spark中复杂的分析库（如机器学习）。与之前的系统相比，Spark SQL主要有两个优势：一是在关系型和过程处理间集成更加紧密，通过一个声明式的<strong>DataFrame API</strong>来集成过程式Spark代码。二是包含了一个高扩展性的优化器<strong>Catalyst</strong>，使用Scala编程语言特性构建，使得添加可组合的规则、控制代码生成和定义扩展点比较容易。使用Catalyst，我们已经构建了许多特性（如JSON方式的模式引用，机器学习类型以及外部数据库的联邦查询）来满足现代数据分析的复杂需求。我们认为Spark SQL是Spark上的SQL和Spark本身的一种进化，它提供了更丰富的API和优化，同时保留了Spark编程模型的优点。</p>
<h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><p>数据库；数据仓库；机器学习；Spark；Hadoop</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>大数据应用要求处理技术、数据源以及存储格式的融合。早期设计的系统，是针对这些workloads，比如MapReduce，给用户提供了一个强大的、底层的、过程式编程接口。编程这样的系统是让人费力的，要求用户人工优化来获得更高的性能。因此，许多新的系统都在寻求提供一个更加有效的用户体验，通过为大数据提供一个关系型接口。比如Pig、Hive、Dremel以及Shark[29,36,25,38]这些系统，都利用了声明式查询优势来提供更加丰富的自动优化。</p>
<p>虽然关系型系统的流行表明用户通常更喜欢编写声明式查询，但是关系型方法对于许多大数据应用程序来说是不够的。首先，用户想从多个数据源中执行ETL，也许是半结构化或非结构化的，这要求自定义开发。其次，用户想执行高级的分析，比如机器学习和图形处理，这些在关系型系统中表达出来一个挑战。实际上，我们发现大部分数据管道可以理想地通过关系查询和复杂过程算法的组合来表达。不幸地是，这两类系统-关系型和过程型，到目前为止，它们在很大程度上仍然是分离的，迫使用户选择一种模式或另一种模式。</p>
<p>本论文描述了我们在Spark SQL上组合多个模型的工作与努力，作为Apache Spark[39]中一个关键的组件。Spark SQL的构建基于中早期的成果，叫做Shark。不用强制用户在关系型和过程式API中做出选择，而是在两者之间无缝混合使用。</p>
<p>Spark SQL通过两方面努力，架起了两种模型之间的桥梁。首先，Spark SQL提供了一个<em>DataFrame API</em>，可以在任何外部数据源和Spark内置分布式集合上执行关系型操作。这个API类似R[32]中被广泛使用的data frame的概念，但是会延迟计算操作，以便它可以执行关系优化。其次，<strong>为了支持大量数据源和大数据算法，Spark SQL引入了一个新式的可扩展优化器，叫做Catalyst</strong>。Catalyst可以使得Spark SQL更容易添加数据源，优化规则以及领域内数据类型，如机器学习。</p>
<p>DataFrame API在Spark程序中提供了丰富的关系型和过程型集成。DataFrames是一种结构化记录集合，通过Spark的过程式API或者关系型API（具备丰富的优化）来管理。它们可以通过Spark内置的分布式Java或Python对象来创建，使得在Spark程序中可以进行关系型处理。其他Spark组件，比如机器学习库，也可以获取和创建DataFrames。在许多场景下，DataFrames比起Spark的过程式API更加方便和高效。例如，可以使用一个SQL描述来计算多个阶段的聚合，而在传统的函数API方式表达就比较困难。也可以自动将数据以列式的格式存储，这个比Java或Python对象有更好的压缩。最后，不像已有的R和Python中的data frame，Spark SQL中的DataFrame操作是经过关系优化器Catalyst的。</p>
<p>为了在Spark SQL中支持更加广泛的数据源和分析workloads，我们设计了一个可扩展的查询优化器Catalyst。Catalyst使用Scala编程语言的特性，如模式匹配，以图灵完备语言来表达可组合的规则。提供了一个通用框架用于转换trees，并用来执行分析、计划以及运行时codegen。通过这个框架，Catalyst能够基于新数据源扩展，包含半结构化数据，比如JOSN以可以谓词下推的高效存储（如HBase）；用户自定义函数或领域自定义类型，如机器学习。函数式语言适合用于构架编译器[37],所以用于很容易构建一个可扩展的优化器，并不感到惊讶。我们发现Catalyst确实能够有效地快速添加Spark SQL的能力，因为它的发布，我们已经看到很多外部开发者也很容易添加它。</p>
<p>Spark SQL是在2014年5月发布，现在是Spark组件中最活跃的开发组件之一。在写这篇文章的时候，Apache Spark是大数据处理中最活跃的开源，过去一年达到了400个贡献者。Spark SQL已经在很多大规模环境下部署。例如，最大的Internet公司使用Spark SQL构建数据管道，在8000个节点、超100PB数据的集群上运行查询。每个查询通常要操作数十TB数据。而且，许多用户采用Spark SQL不仅仅为了SQL查询，而是在程序中和过程式处理组合使用。例如，2/3的Databricks Cloud的客户，运行Spark的托管服务，在其他编程语言中使用Spark SQL。性能方面，我们发现Spark SQL比其他Hadoop上关系查询的纯SQL系统更有竞争力。同时，<strong>用SQL表示的计算要比原生Spark代码快10倍同时内存使用更加高效</strong>。</p>
<p>一般来说，我们认为Spark SQL是核心Spark API的一个重要发展。虽然，Spark传统的函数式编程API是很普遍的，但这使得自动优化的机会非常有限。Spark SQL同时使Spark可以被更多的用户访问，并改进了对现有用户的优化。在Spark中，社区正在将Spark SQL合入更多的API中，针对机器学习，DataFrames是标准的数据表现方式在新的“ML pipeline”的API中。以及我们希望扩展到其他组件中，比如GraphX和Streaming。</p>
<p>第二章节，我们以Spark为背景，Spark SQL为目标，开始本篇论文。然后，在第三章节描述DataFrame API，第四章节Catalyst优化器以及第五章节基于Catalyst构建的高级特性。在第六章，给出Spark SQL评测。在第七章，描述外部研究。最后，第八章节给出相关工作。</p>
<h2 id="2-背景与目标"><a href="#2-背景与目标" class="headerlink" title="2. 背景与目标"></a>2. 背景与目标</h2><h3 id="2-1-Spark-概述"><a href="#2-1-Spark-概述" class="headerlink" title="2.1 Spark 概述"></a>2.1 Spark 概述</h3><p>Spark是一个通用的集群计算引擎，支持Scala、Java以及Python API，以及streaming、graph处理和机器学习[6]的库。在2010年发布，是广泛使用的语言集成API系统，类似DryadLINQ[20]，同时也是大数据处理中，最活跃的开源项目之一。Spark在2014就有超过400个贡献者，被多个软件供应商集成。</p>
<p>Spark和其他系统[20,11]类似，提供一个函数式编程API，让用户操作分布式集合，叫做弹性分布式数据集（RDDS）[39]。每个RDD就是一个夸集群的Java或Python对象分区集合。RDDS可以通过类似map、filter和reduce的操作来管理，在编程语言中使用这些函数，将会把它们分发至集群的各个节点上。例如，下面统计文本文件中ERROR打头的行数的Scala代码：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">lines = spark.textFile(<span class="string">"hdfs://..."</span>)</span><br><span class="line">errors = lines.filter(s=&gt; s.contains(<span class="string">"ERROR"</span>))</span><br><span class="line">println(erros.count())</span><br></pre></td></tr></table></figure>
<p>这段代码通过读取HDFS文件，创建RDD的lines，使用filter来转换得到另一个RDD，errors。然后在数据上执行count操作。</p>
<p>RDDs是一个具备容错的，可以使用RDD的血缘图来恢复丢失的数据（通过重新运行操作，比如重新执行filter来重建丢失的分区）。它们也可以显式地缓存在内存或磁盘中来支持迭代计算[39]。</p>
<p>RDD APIs的最后一点说明是延迟计算的。每个RDD表达了一个“逻辑计划”来计算数据集，但是Spark会等待特定的输出操作，比如count，来发起一个计算。这使得引擎可以做一些简单的查询优化，比如管道化操作。例如，在上面的例子中，Spark将从HDFS文件中管道式读取行数，应用filter和计算一个count统计，使得它不用物化中间的lines和errors结果。虽然这样的优化非常有用，但是非常有限因为引擎不理解RDD中的数据结构（任意的JAVA或Python对象）或者用户函数的语义（包含任意的代码）。</p>
<h3 id="2-2-Spark上早期关系系统"><a href="#2-2-Spark上早期关系系统" class="headerlink" title="2.2 Spark上早期关系系统"></a>2.2 Spark上早期关系系统</h3><p>我们早期在Spark构建的关系型接口是Shark[38]，这个是修改了Apache Hive的系统运行在Spark上，实现了传统的RDBMS优化，比如列式处理，在Spark引擎之上。虽然Shark显示一个好的性能，以及和Spark集成的好机会，但是还是具有三个重要的挑战：</p>
<ul>
<li>首先，Shark仅仅用于查询存储在Hive Catalog中的外部数据，因此对于Spark程序中的数据（比如上述手动创建的errors RDD），进行关系型查询是没有用的；</li>
<li>其次，从Spark程序调用Shark的唯一方法是将一个SQL字符串组合在一起，这在模块化程序中是不方便和容易出错的；</li>
<li><strong>最后，Hive的优化是针对MapReduce的，很难扩展，构建一个新特性也比较困难，比如数据类型用于机器学习或支持新的数据源</strong>。</li>
</ul>
<h3 id="2-3-Spark-SQL目标"><a href="#2-3-Spark-SQL目标" class="headerlink" title="2.3 Spark SQL目标"></a>2.3 Spark SQL目标</h3><p>基于Shark的体验，我们想扩展关系型处理，来覆盖Spark中的原生RDD以及更广泛的数据源。目标如下：</p>
<ul>
<li>在Spark程序中支持关系型处理（在原生RDD上），以及在外部数据源上使用开发者友好的API；</li>
<li>使用成熟的DBMS技术来提供更高的性能；</li>
<li>更加容易地支持新的数据源，包含半结构化数据和外部数据库，满足联邦查询；</li>
<li>支持高级分析算法扩展，比如图计算和机器学习。</li>
</ul>
<h2 id="3-编程接口"><a href="#3-编程接口" class="headerlink" title="3. 编程接口"></a>3. 编程接口</h2><p>Spark SQL作为一个库运行在Spark之上，如图1所示。表示SQL接口可以通过JDBC/ODBC或者命令行控制台访问，也可以通过集成到Spark中，来支持编程语言的DataFrame API，让用户可以混合使用过程式和关系型代码。然而，高级函数也可以通过UDFs在SQL中表示，通过BI工具调用。这个我们将在3.7章节讨论。</p>
<p><img src="/blog/8973d72a/spark_sql.png" alt></p>
<center>图1 Spark SQL和Spark集成</center>

<h3 id="3-1-DataFrame-API"><a href="#3-1-DataFrame-API" class="headerlink" title="3.1 DataFrame API"></a>3.1 DataFrame API</h3><p>在Spark SQL的API中最主要抽象就是DataFrame，即带有同样schema的rows分布式集合。一个DataFrame等价于关系型数据库中的一个表，也可以通过类似Spark中原生分布式集合RDD的方式来处理。与RDD不同，DataFrames含有数据的schema，以及支持各种关系型操作能够得到进一步地优化执行。</p>
<p>DataFrames可以通过系统catalog中的表（外部数据源）或者已有的原生Java/Python对象RDD（3.5章节）来构建。一旦构建成功，就可以使用各种关系型操作，比如where和groupBy，可以接受DSL中的各种表达式，类似R语言和Python中[32, 30]的data frames。每个DataFrame也可以被视为是一个Row对象的RDD，允许用户调用过程式Spark API，比如map。</p>
<p>最后，与传统的data frames的API不同，Spark的DataFrames式懒加载的，在每个DataFrame对象中，都表示一个逻辑计划来计算dataset，但不会执行直到用户调用的特定的输出操作，比如save。这使得可以跨越所有操作的丰富优化，能够使用在DataFrame的构建中。</p>
<p>为了说明这一点，如下的Scala代码从一个Hive表中定义了一个DataFrame，基于它获得了另外一个DataFrame，并打印结果：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">ctx = <span class="keyword">new</span> <span class="type">HiveContext</span>()</span><br><span class="line">users = ctx.table(<span class="string">"users"</span>)</span><br><span class="line">young = user.where(users(<span class="string">"age"</span>) &lt; <span class="number">21</span>)</span><br><span class="line">println(young.count())</span><br></pre></td></tr></table></figure>
<p>在这段代码中，users和young是DataFrames。<code>users(&quot;age&quot;) &lt; 21</code>的代码片段是data frame DSL中的一个表达式，作为一个抽象语法树，而不是代表传统Spark API的Scala函数。最后，每个DataFrame都代表了一个逻辑表达式（比如，读取users表和过滤age &lt; 21）。当用户调用count时，是一个输出操作，Spark SQL会构建一个物理表达式来计算最终的结果。这也许会包含一个游湖，比如仅仅scan数据的age列，假如它的存储是列式的，或者使用数据源中索引来统计匹配的行。</p>
<h3 id="3-2-数据模型"><a href="#3-2-数据模型" class="headerlink" title="3.2 数据模型"></a>3.2 数据模型</h3><p>对于表和DataFrames，Spark SQL使用基于Hive[19]的内嵌数据模型。支持所有主要的SQL数据类型，包含boolean、integer、double、decimal、string、date和timestamp以及复杂的数据类型（非原子的）：structs、arrays、maps和unions。复杂的数据类型也可以嵌套在一起，来创建一个更强大的类型。与许多传统的DBMS不同，Spark SQL为查询语言和API中的复杂数据类型提供一流的支持。<strong>而且，Spark SQL也支持用户自定义类型，这个在4.4.2章节说明</strong>。</p>
<p>使用这个类型系统，我们能够准确地模型化来自各种数据源和文件格式的数据，包括Hive、关系型数据库、JSON以及Java/Scala/Python的原生对象。</p>
<h3 id="3-3-DataFrame操作"><a href="#3-3-DataFrame操作" class="headerlink" title="3.3 DataFrame操作"></a>3.3 DataFrame操作</h3><p>用户能够在DataFrames中使用DSL来执行关系型操作，类似R语言中的data frames[32]和Python中的Pandas[30]。DataFrames支持所有的通用关系型操作（算子），包括projection（select），filter（where)，join和aggregations（groupBy）。这些算子在有限的DSL中都可以表达出来，是的Spark能够获取表达式的结构。例如，如下的代码，计算每个部门的女雇员的数量：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">employees</span><br><span class="line">.join(dept, employees(<span class="string">"deptid"</span>) === dept(<span class="string">"id"</span>))</span><br><span class="line">.where(employees(<span class="string">"gender"</span>) === <span class="string">"female"</span>)</span><br><span class="line">.groupBy(dept(<span class="string">"id"</span>), dept(<span class="string">"name"</span>))</span><br><span class="line">.agg(count(<span class="string">"name"</span>))</span><br></pre></td></tr></table></figure>
<p>这里，employees是一个DataFrame，employees(“deptid”)是一个表示deptId列的表达式。表达式对象有许多操作能够返回一个新的表达式，包含常用的比较操作符（例如，=== 等值判断，&gt; 大于）以及数学操作（+，- 等）。所有这些操作都构建在一个表达式的抽象语法树AST上，然后传递给Catalyst进行优化。与原生Spark API中使用函数包含任意Scala/Java/Python代码不同，这些代码对于runtime引擎来说是不透明的。针对详细的API列表，我们可以参考阅读Spark的官方文档[6]。</p>
<p>除了关系型DSL，DataFrames可以被注册为一个系统catalog中的临时表，然后使用SQL来查询。如下代表给出了示例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">users.where(users(<span class="string">"age"</span> &lt; <span class="number">21</span>)).registerTempTabel(<span class="string">"young"</span>)</span><br><span class="line">ctx.sql(<span class="string">"SELECT count(*), avg(age) FROM young"</span>)</span><br></pre></td></tr></table></figure>
<p>SQL有时候用于简单计算多个聚合是非常方便的，也允许程序通过JDBC/ODBC的方式暴露datasets。在catalog中注册的DataFrames，还是一个未物化的食物，使得优化操作可以发生在SQL和原始DataFrame表达式中。但是，DataFrames也是可以被物化的，这个在3.6章节讨论。</p>
<h3 id="3-4-DataFrames和关系型查询语言对比"><a href="#3-4-DataFrames和关系型查询语言对比" class="headerlink" title="3.4 DataFrames和关系型查询语言对比"></a>3.4 DataFrames和关系型查询语言对比</h3><p>虽然，表面上DataFrames提供了类似关系型查询语言的同样的操作，如SQL和Pig[29]，但是我们发现让用户更容易地使用，多亏了他们在整个编程语言中的集成。例如，用户能够把他们的代码拆解为Scala，Java或Python函数，在他们之间传递DataFrames来构建逻辑计划，同时还可以在运行输出操作时在整个计划上执行优化。同样，开发人员可以使用控制结构（如if语句和循环）来构造工作。一位用户说，DataFrame API“像SQL一样简洁、声明性强，但我可以命名中间结果”，指的是如何更容易构造计算和调试中间步骤。</p>
<p>为了更加简化DataFrames中的编程，我们使得API可以尽早地分析逻辑计划（比如，识别表达式中的列名是否在底层表中，以及它们的数据类型是否正确），即使查询结果是延迟计算的。因此，只要用户输入一个无效代码行，Spark SQL就会报告一个错误，而不是等到执行时，这比起一个大SQL同样容易使用。</p>
<h3 id="3-5-查询原生Datasets"><a href="#3-5-查询原生Datasets" class="headerlink" title="3.5 查询原生Datasets"></a>3.5 查询原生Datasets</h3><p>实际场景的数据pipelines是从异构数据源中抽取数据，运行来自不同程序库的各种算法。为了和过程式Spark代码交互，Spark SQL允许用户基于原生RDD对象直接构建DataFrames。Spark SQL可以自动使用发射来获取这些对象的schema。在Scala和Java中，类型信息可以从语言类型系统中抽取（JavaBeans和Scala Case类）。在Python中，由于动态类型系统，Spark SQL对dataset进行采样以执行模式推断。</p>
<p>例如，如下Scala代码从一个User对象RDD定义了一个DataFrame。Spark SQL自动检测列的名称（name和age）和数据类型（string和int）。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">usersRDD</span> </span>= spark.parallelize(<span class="type">List</span>(<span class="type">User</span>(<span class="string">"Alice"</span>, <span class="number">22</span>), <span class="type">User</span>(<span class="string">"Bob"</span>, <span class="number">19</span>)))</span><br><span class="line">usersDF = usersRDD.toDF</span><br></pre></td></tr></table></figure>
<p>内部，Spark SQL创建一个逻辑数据scan操作，指向RDD。这个被编译为物理操作，访问原生对象的字段。这个完全不同于传统的ORM。ORM一般会出现昂贵的转换操作，将整个对象翻译为不同格式的对象。相反，Spark SQL访问原生对象，仅仅提取每个查询中使用的列。</p>
<p>查询原生dataset的能力可以让用户运行优化后的关系操作，在已有的Spark程序中。而且，将RDD和外部结构化整合更加简单。例如，我们将users RDD和Hive表进行Join操作：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">views = ctx.table(<span class="string">"pageviews"</span>)</span><br><span class="line">usersDF.join(views, usersDF(<span class="string">"name"</span>) === views(<span class="string">"user"</span>))</span><br></pre></td></tr></table></figure>
<h3 id="3-6-内存缓存"><a href="#3-6-内存缓存" class="headerlink" title="3.6 内存缓存"></a>3.6 内存缓存</h3><p>和之前的Shark一样，Spark SQL可以使用列存储将热数据物化（通常称为“缓存”）<strong>。与Spark原生cache相比（简单地存储数据为JVM对象），列缓存可以将内存占用减少一个数量级，因为可以应用列式压缩机制，如字典编码和行程编码</strong>。缓存对于交互查询和机器学习的迭代算法是非常有用的。可以通过DataFrame的cache()方法来调用。</p>
<h3 id="3-7-用户自定义函数"><a href="#3-7-用户自定义函数" class="headerlink" title="3.7 用户自定义函数"></a>3.7 用户自定义函数</h3><p>用户自定义函数（UDF）是数据库系统很重要的扩展。例如，MySQL依赖UDF提供JSON数据的基本支持。更高级的例子，MADLib使用UDF为Postgres和其他数据库系统[12]实现机器学习算法。然而，数据库系统要求UDF在一个独立编程环境下定义，不同于基础的查询接口。Spark SQL的DataFrame API支持内联UDF的定义，不需要复杂的打包以及注册流程。事实证明，这个特性对于API的采用至关重要。</p>
<p>在Spark SQL中，能够通过Scala、Java或Python函数来内联注册UDF，这些函数可以在内部完整地使用Spark API。例如，给定一个机器学习模型的model对象，我们可以将预测函数注册为UDF：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> model : <span class="type">LogisticRegreesionModel</span> = ...</span><br><span class="line">ctx.udf.register(<span class="string">"predict"</span>, (x:<span class="type">Float</span>, y:<span class="type">Float</span>) =&gt; model.predict(<span class="type">Vector</span>(x,y)))</span><br><span class="line">ctx.sql(<span class="string">"SELECT predict(age, weight) FROM users"</span>)</span><br></pre></td></tr></table></figure>
<p><strong>一旦注册，BI工具就可以通过JDBC/ODBC接口来使用这个UDF</strong>。除了对标量值进行操作的UDF（如本文所示）之外，还可以定义对整个表进行操作的UDF，如MADLib[12]中所示，并在其中使用分布式Spark API，从而公开高级的<br>SQL用户的分析功能。最后，由于UDF定义和查询执行是使用相同的通用语言（例如Scala或Python）表示的，所以用户可以使用标准工具调试或分析整个程序。</p>
<p>上面的示例演示了许多pipelines中的一个常见用例，即同时使用关系型运算符和高级分析方法的用例，这些方法在SQL中是难以表达的。DataFrame API允许开发人员无缝地混合这些方法。</p>
<h2 id="4-Catalyst优化器"><a href="#4-Catalyst优化器" class="headerlink" title="4. Catalyst优化器"></a>4. Catalyst优化器</h2><p>为了实现Spark SQL，我们设计了一个新的可扩展优化器Catalyst，基于Scala的函数式编程构建。Catalyst的可扩展设计有两个目的：一是，在Spark SQL中更加容易地添加新的优化技术和特性，尤其是在大数据中要解决遇到的各种问题（比如，半结构化数据和高级分析）。二是，让外部开发者能够扩展优化器，例如通过加入数据源的特定规则，能够将过滤或聚合下推到外部存储系统中，或者支持新的数据类型。Catalyst对RBO和CBO都支持。</p>
<p>虽然，可扩展优化器在过去已经被提出，但一般要求比较复杂的DSL来指定规则，和一个优化器编译器来将规则翻译为可执行代码[17, 16]。这将导致一个很重的学习曲线和维护负担。相反，<strong>Catalyst使用Scala函数式编程的标准特性，比如模式匹配[14]，让开发人员使用完整的编程语言，同时使规则易于指定。函数式语言的设计部分是为了构建编译器，所以我们发现Scala非常适合这个任务</strong>。然而，据我们所知，Catalyst是第一个基于这种语言构建的生产级质量的查询优化器。</p>
<p>在核心代码中，Catalyst包含了一个通用的库，用于表示trees，和应用rules来看你管理trees。在这个框架之上，针对关系查询处理，我们已经建了许多的库（比如expressions、逻辑查询计划），以及都许多规则用于处理查询执行的不同阶段，包括分析、逻辑优化、物理计划以及codegen来将部分查询编译为Java字节码。针对最后一个，我们使用了另外一个Scala特性，quasiquotes[34]，使得运行时从可组合的表达式生成代码更加容易。最后，Catalyst提供了许多扩展点，包括外部数据与啊以及用户自定义类型。</p>
<h3 id="4-1-Trees"><a href="#4-1-Trees" class="headerlink" title="4.1 Trees"></a>4.1 Trees</h3><p>在Catalyst中最重要的数据类型就是由多个node对象构成的tree。每个node有一个类型、0或多个子节点。新的node类型通过Scala定义，作为一个TreeNode的子类。这些对象是不可变的，可以通过函数式转换来控制，这个在后续章节讨论。</p>
<p>正如下面的例子，假设针对一个简单的表达式语言，我们有三个node类：</p>
<ul>
<li>Literal(value: Int)： 一个常量值；</li>
<li>Attribute（name: String)： 一个输入row的属性，例如，”x”；</li>
<li>Add(left: TreeNode, right: TreeNode)： 两个表达式求和。</li>
</ul>
<p>这些类用来构建tree，例如，表达式x+(1+2)，如图2所示，将通过如下Scala代码表示：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">Add</span>(<span class="type">Attribute</span>(x), <span class="type">Add</span>(<span class="type">Literal</span>(<span class="number">1</span>), <span class="type">Literal</span>(<span class="number">2</span>)))</span><br></pre></td></tr></table></figure>
<p><img src="/blog/8973d72a/catalyst_simple_exp.png" alt></p>
<center>图2 x+(1+2)表达式</center>

<h3 id="4-2-Rules"><a href="#4-2-Rules" class="headerlink" title="4.2 Rules"></a>4.2 Rules</h3><p>Trees是可以通过Rules来控制的，规则一中可以将一个tree转化为另一个tree的functions。虽然一个规则在输入的tree上（假定tree就是一个Scala对象）可以执行任意代码，但大部分方式是通过一个模式匹配functions集合，来检索和使用特定的结构来替换子树。</p>
<p>模式匹配是许多函数式语言的一个特性，可以从关系代数数据类型的内嵌结构中抽取相关的信息。在Catalyst中，trees提供了一个transform的方法，可以在tree的所有节点上递归应用模式匹配函数，将每个符合模式的节点tranform成一个结果。例如，我们可以实现一个规则，折叠常量的Add操作，如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">tree.transform &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Add</span>(<span class="type">Literal</span>(c1), <span class="type">Literal</span>(c2) =&gt; <span class="type">Literal</span>(c1+c2)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>将这个规则，应用在x+(1+2)的树上，在图2中，合并成一个新的tree为x+3。case关键词是Scala中标准模式匹配的语法[14]，可以用来匹配一个对象的类型，以及根据给定的名称来抽取值（如c1和c2）。</p>
<p>这个传递给transform的模式匹配表达式是一个partial function（偏序函数），意味着它只需要匹配所有可能的输入树的子集。Catalyst将测试给定规则适用于树的哪些部分，自动跳过并降序到不匹配的子树中。这个能力使得规则只需要应用在给定优化的树，没有匹配的部分不用考虑。因此，rules不需要被修改为算子的新类型，加入到系统中。</p>
<p>在同样的transform调用中，rules（以及Scala的模式匹配）能够匹配多个模式，使得它可以同时实现多个transformations：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">tree.transform &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Add</span>(<span class="type">Literal</span>(c1), <span class="type">Literal</span>(c2) =&gt; <span class="type">Literal</span>(c1+c2)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Add</span>(left, <span class="type">Literal</span>(<span class="number">0</span>)) =&gt; left</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Add</span>(<span class="type">Literal</span>(<span class="number">0</span>), right) =&gt; right</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>实际上，rules也许需要执行多次才能完全transform一个tree。Catalyst将rules分组为batches，执行每个batch直到达到一个固定点，也就是在应用这些规则之后，直到tree停止改变。将规则运行到固定点意味着每个规则都可以是简单和自包含的，但最终仍然会对树产生更大的全局影响。在上述示例中，重复应用规则将会常量折叠一个更大的trees，比如(x+0)+(3+3)。在另一个例子中，第一个batch也会分析一个表达式，给所有attributes分配类型，而第二个batch也许会使用这些类型进行常量折叠。在每个batch处理之后，开发人员还可以对new tree运行健全性检查（例如，查看所有属性是否都被分配了类型），通常也是通过递归匹配来编写的。</p>
<p>最后，rule的条件和它们的内部包含任意Scala代码。这使得Catalyst比起DSL在优化器方面更加强大，同时保持了简单规则的简洁性。</p>
<p>在我们的经验中，在可变trees上的函数式transformations使得整个优化器更加容易推理和调试。还可以做到在优化器中实现并行化，虽然我们还没有使用到它。</p>
<h3 id="4-3-在SparkSQL中使用Catalyst"><a href="#4-3-在SparkSQL中使用Catalyst" class="headerlink" title="4.3 在SparkSQL中使用Catalyst"></a>4.3 在SparkSQL中使用Catalyst</h3><p>我们在4个阶段使用Catalyst的通用tree transformation框架，如图3所示：</p>
<ul>
<li>分析逻辑计划来解析引用；</li>
<li>逻辑计划优化；</li>
<li>物理计划；</li>
<li>代码生成，将查询部分逻辑编译为Java字节码。</li>
</ul>
<p>在物理计划阶段，Catalyst也会生成多个计划，然后基于代价进行比较，其他所有阶段都是基于规则的。每个阶段使用不同类型的tree nodes；Catalyst包含了表达式、数据类型、逻辑和物理算子的。node库。</p>
<p><img src="/blog/8973d72a/phase_planning.png" alt></p>
<center>图3 SparkSQL查询阶段。圆角矩形表示Catalyst trees</center>

<h4 id="4-3-1-分析"><a href="#4-3-1-分析" class="headerlink" title="4.3.1 分析"></a>4.3.1 分析</h4><p>Spark SQL从一个SQLParser返回的AST或者使用API创建DataFrame对象开始关系计算。在这两个场景中，这个关系包含了unresovled的attribute引用或者关系：例如，在SQL查询<code>SELECT col FROM sales</code>中，col的类型或者列名是否有效都是未知的直到我们找到表sales。如果我们不知道它的类型或者还没有匹配到一个输入的表（或者别名），那么attribute就被视为unresovled。<strong>Spark SQL使用Catalyst的rule和一个Catalog对象，在所有数据源上跟踪这个表来resolve这些attributes</strong>。首先，构建一个带有未绑定的attributes和数据类型的unresolved逻辑计划，然后按照如下流程应用rules：</p>
<ul>
<li>从catalog中，通过name查找relations；</li>
<li>将命名了的attributes映射为输入提供给指定算子的children；</li>
<li>决定哪些attributes引用同一个值，给它们一个unique ID（之后可以优化表达式，比如col = col）；</li>
<li>通过表达式，传播和强制类型：例如，我们不知道1 + col的类型，直到resolved了col和可能将子表达式转为兼容的类型。</li>
</ul>
<p><strong>总之，分析器的rule大概有1000行代码。</strong></p>
<h4 id="4-3-2-逻辑优化"><a href="#4-3-2-逻辑优化" class="headerlink" title="4.3.2 逻辑优化"></a>4.3.2 逻辑优化</h4><p>逻辑优化阶段，将应用标准的基于规则的优化操作在逻辑计划上。这些包含了常量折叠、谓词下推、投影剪枝、null传播、布尔表达式简化以及其他rules。通常，我们发现针对各种场景，我们添加一些规则是非常简单的。例如，当我们在Spark SQL中，添加一个固定精度的DECIMAL类型时，我们想优化聚合操作，如在小精度的DECIMAL上的sums和averages操作；大概要12行代码来写一个规则，在SUM和AVG表达式中寻找decimals，之后把它们cast为一个unscaled 64位long，做完聚合计算后，再把结果转换回去。改规则的一个简单的版本，仅仅用于优化SUM表达式，如下再表示一下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DecimalAggregates</span> <span class="keyword">extends</span> <span class="title">Rule</span>[<span class="type">LogicalPlan</span>] </span>&#123;</span><br><span class="line">    <span class="comment">//Long中十进制最大位数</span></span><br><span class="line">    <span class="keyword">val</span> <span class="type">MAX_LONG_DIGITS</span> = <span class="number">18</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(plan: <span class="type">LogicalPlan</span>): <span class="type">LogicalPlan</span> = &#123;</span><br><span class="line">        plan transformAllExpressions &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="type">Sum</span>(e<span class="meta">@DecimalType</span>.<span class="type">Expreesion</span>(prec, scale))</span><br><span class="line">               <span class="keyword">if</span> prec + <span class="number">10</span> &lt;= <span class="type">MAX_LONG_DIGITS</span> =&gt; </span><br><span class="line">            		<span class="type">MakeDecimal</span>(<span class="type">Sum</span>(<span class="type">LongValue</span>(e)), prec + <span class="number">10</span>, scale)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>另外一个例子，一个12行代码的规则，将带有一个简单的正则表达式的LIKE表达式优化为一个String.startWith或者String.contains的调用。在rule中可以自由使用任意Scala代码，来实现这些优化，它超越了模式匹配的子树结构，易于表达。</p>
<p><strong>总之，逻辑优化的rule有800行代码</strong>。</p>
<h4 id="4-3-3-物理计划"><a href="#4-3-3-物理计划" class="headerlink" title="4.3.3 物理计划"></a>4.3.3 物理计划</h4><p>在物理计划阶段，Spark SQL使用逻辑计划，生成一个或多个物理计划，使用能够匹配Spark执行引擎的物理算子。<strong>使用代价模型来选择一个计划</strong>。当前，基于代价的优化器仅仅用于选择JOIN算法：针对一个较小的relations，Spark SQL会使用broadcast join，使用Spark中点对点广播的能力。然而，该框架支持更广泛地使用基于代价的优化，因为可以使用规则可以递归地估算整个tree的代价。在未来，我们打算实现更加丰富的基于代价的优化。</p>
<p><strong>物理planner也可以执行基于规则的物理优化，比如将投影或过滤操作pipelining到Spark的map操作中</strong>。而且，可以将逻辑计划中的操作下推到支持谓词或投影下推的数据源中。我们将在4.4.1章节描述这些数据源的API。</p>
<p><strong>总之，物理计划的rule大概有500行代码</strong>。</p>
<h4 id="4-3-4-代码生成"><a href="#4-3-4-代码生成" class="headerlink" title="4.3.4 代码生成"></a>4.3.4 代码生成</h4><p>查询优化的最后一个阶段，涉及到生成Java字节码，运行在每个机器上。因为，<strong>Spark SQL经常在内存的Datasets上执行操作，处理是受限于CPU的，因此我们想支持codegen来加速执行</strong>。尽管如此，codegen引擎通常很难构建，实质上相当于一个编译器。Catalyst基于Scala语言的特殊特性，<strong>quasiquotes</strong>[34]，使得codegen变得容更加简单。Quasiqutoes允许在Scala语言中以编程方式构造抽象语法树（AST），然后可以在运行时将其提供给Scala编译器以生成字节码。我们使用Catalyst将SQL中tree表达式转换为Scala代码的AST，来计算这个表达式，之后编译和运行生成的代码。</p>
<p>正如章节4.2中介绍的例子，Add、Attribute以及Literal的tree node，可以用来编写一个表达式(x+y)+1。如果没有codegen，这个表达式将针对每行数据，向下遍历tree中的Add、Attribute和Literal节点。这将会引入大量的分析和虚函数的调用，降低执行的性能。通过codegen，我们能够编写一个函数，将特定的expression  tree翻译为Scala AST，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compile</span></span>(node:<span class="type">Node</span>): <span class="type">AST</span> = node <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Literal</span>(value) =&gt; <span class="string">q"<span class="subst">$value</span>"</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">Attribute</span>(name) =&gt; <span class="string">q"row.get(<span class="subst">$name</span>)"</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">Add</span>(left, right) =&gt; <span class="string">q"<span class="subst">$&#123;compile(left)&#125;</span> + <span class="subst">$&#123;compile(right)&#125;</span>"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>以q开头的字符串就是quasiquotes，虽然它们看起来像一个普通的字符串，实际上是可以通过Scala Compiler解析的，来表示内部代码的AST。Quasiquotes可以使用变量或者其他AST‘s的片段，通过$来表示。例如，Literal(1)转为Scala的AST为1，而Attribute(“x”)转为row.get(“x”)。最后，类似Add(Literal(1), Attribute(“x”))的tree，转为Scala代码的AST为1+row.get(“x”)。</p>
<p>Quasiquotes在编译时进行类型检测，以确保仅有正确的ASTs或者literal被替换，这个比起字符串拼接更有使用意义，它们将直接生成Scala AST，而不是在运行执行Scala解析器。而且，它们是高度可组装的，因为针对每个node的codegen规则并不需要知道它的孩子nodes是如何构建返回的。最后，生成的code会进一步通过Scala编译器来优化，以防Catalyst错过表达式级优化。图4展示了，quasiquotes生成的代码性能和手动调整的程序性能类似。</p>
<p><img src="/blog/8973d72a/quasiquotes_hand.png" alt></p>
<center>图4 表达式x+x+x计算10亿次性能比较，x为integer类型</center>

<p>我们发现，quasiquotes在codegen方面使用非常直接，许多新的Spark SQL contributors能够快速地为新的表达式类型添加规则。Quasiquotes也可以和原生Java对象一起使用：当从这些对象中访问fields时，我们能够直接通过codegen来访问需要的field，而不是将对象拷贝到Spark SQL Row中并使用Row的accessor方法。最后，也可以直接组合codegen计算和interpreted计算（还不能生成codegen的表达式），因为我们编译的Scala代码可以直接调用表达式interpreter（解释器）。</p>
<p><strong>总之，Catalyst的code生成器大概有700行代码</strong>。</p>
<h3 id="4-4-扩展点"><a href="#4-4-扩展点" class="headerlink" title="4.4 扩展点"></a>4.4 扩展点</h3><p>Catalyst的设计围绕可组装的rules，使得它更容易被用户和第三方库扩展。开发者们可以添加一批rules到运行时查询优化的每个阶段，只要它们遵循每个阶段的约定（例如，保证分析阶段可以解析所有attributes）。然而，<strong>为了在不用理解Catalyst rules情况下，添加一些扩展类型更加简单，我们需要定义两个较小的公共扩展点：数据源和用户自定义类型</strong>。这些还需要依赖core引擎的基础设施和优化器的其余部分进行交互。</p>
<h4 id="4-4-1-数据源"><a href="#4-4-1-数据源" class="headerlink" title="4.4.1 数据源"></a>4.4.1 数据源</h4><p>开发者们为Spark SQL开发一个新的数据源，可以使用许多API，这些API提供了各种程度的优化。所有数据源必须实现一个带有kv参数集合的createRelation函数，和返回一个BaseRelatoin对象，仅当relation的可以成功加载。每个BaseRelation包含一个schema和一个可选的计算字节大小。例如，一个表示MySQL的数据源，带有一个表名作为一个参数，向MySQL查询表大小的估计值。</p>
<p>为了能让Spark SQL读取数据，一个BaseRelation需要实现众多接口中的一个，这些接口允许它们公开不同程度的复杂性。最简单的是<strong>TableScan</strong>，要求relation返回一个Row对象（表中的数据）的RDD。另外一个更高级的是<strong>PrundedScan</strong>（带有列名数组），返回的Rows仅仅包含这些列。第三个接口，<strong>PrunedFilteredScan</strong>（带有需访问的列名和过滤对象数组），它是Catalyst表达式语法的子集，可以进行谓词下推。过滤器是建议性的，即数据源应该尝试返回符合过滤条件的rows，但是如果过滤器不能被计算，则需要返回false。最后一个接口是<strong>CatalystScan</strong>（带有完整地Catalyst表达式trees集合），用于谓词下推。</p>
<p>这些接口，使得数据源可以实现各种程度的优化，同时也使开发人员能够方便地添加几乎任何类型的简单数据源。我们以及其他人已经使用这个接口实现了如下数据源：</p>
<ul>
<li>CSV文件，简单地scan整个文件，用户可以指定schema；</li>
<li>Avro[4]，一个用于嵌套数据的自描述二进制格式；</li>
<li>Parquet[5]，一个列式文件格式，支持列剪枝和过滤；</li>
<li>JDBC数据源，从RDBMS中并行scan一个表的范围，同时下推filters到RDBMS以最小化通信。</li>
</ul>
<p>为了使用这些数据源，程序员们在SQL语句中指定它们的包名，配置选项通过KV对传递。例如，使用Avro数据源，持有一个指向文件的path:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TEMPORARY</span> <span class="keyword">TABLE</span> messages</span><br><span class="line"><span class="keyword">USING</span> com.databricks.spark.avro</span><br><span class="line">OPTIONS(<span class="keyword">path</span> <span class="string">"message.avro"</span>)</span><br></pre></td></tr></table></figure>
<p>所有的数据源还会抛出网络本地性的信息，即数据的每个分区从哪个机器上读取是最高效的。这个是通过返回的RDD对象抛出的，RDDs有一个内置的API，用于数据本地性（data locality)[39]。</p>
<p>最后，也有类似的接口用于写数据到已有的或者新的表中。这些更加简单，因为Spark SQL仅仅提供了一个Row objects的RDD来写。</p>
<h4 id="4-4-2-用户自定义类型（UDTs）"><a href="#4-4-2-用户自定义类型（UDTs）" class="headerlink" title="4.4.2 用户自定义类型（UDTs）"></a>4.4.2 用户自定义类型（UDTs）</h4><p>可以在Spark SQL执行高级分析的一个特性就是用户自定义类型。例如，机器学习应用需要的vector类型，图算法需要的用于表示graph的类型，这些可能已经超出了关系型表[15]。虽然，添加一个新的类型非常有挑战，但数据类型遍及执行引擎的方方面面。例如，Spark SQL中，内置的数据类型以列式的、压缩格式存储在内存cache中（3.6章节），从之前章节的数据源API可以看到，我们需要暴露所有可能的数据类型给数据源拥有者。</p>
<p>在Catalyst中，我们通过将用户自定义类型映射到Catalyst内置类型（3.2章节）组合结构，来解决这个问题。为了注册一个Scala类型为UDT，用户提供一个映射关系，从它们的class对象到一个内置类型的Catalyst Row，以及一个反向转换。在用户code中，它们能够在Spark SQL查询的对象中使用Scala类型，它将会被转为底层的内置类型。同样，它们可以注册直接对其类型进行操作的UDFs（见第3.7节）。</p>
<p>根据一个小示例，假设我们想注册一个两维的点(x, y)作为一个UDT。我们可以通过两个double值表示这个向量。为了注册这个UDT，我们写了如下内容：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PointUDT</span> <span class="keyword">extends</span> <span class="title">UserDefinedType</span>[<span class="type">Point</span>] </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dataType</span> </span>= <span class="type">StructType</span>(</span><br><span class="line">    	<span class="type">Seq</span>(<span class="type">StructField</span>(<span class="string">"x"</span>, <span class="type">DoubleType</span>), <span class="type">StructField</span>(<span class="string">"y"</span>, <span class="type">DoubleType</span>))</span><br><span class="line">    )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">serialize</span></span>(p: <span class="type">Point</span>) = <span class="type">Row</span>(p.x, p.y)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">deserialize</span></span>(r: <span class="type">Row</span>) = </span><br><span class="line">    	<span class="type">Point</span>(r.getDouble(<span class="number">0</span>), r.getDouble(<span class="number">1</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注册完这个类型之后，在Spark SQL中访问并转为DataFrames时，Points将会原生对象中被识别，并传递给在Points上定义的UDFs。而且，Spark SQL将会以列式格式存储Points，当缓存数据时（将x和y作为单独列压缩），Points也是可以被写入到所有的Spark SQL的数据源中，以double值对的形式。在Spark机器学习库中，我们也使用了这样的能力，在5.2章节进行描述。</p>
<h2 id="5-高级分析特性"><a href="#5-高级分析特性" class="headerlink" title="5. 高级分析特性"></a>5. 高级分析特性</h2><p>本章我们将描述在Spark SQL中加入的三大特性来解决大数据环境下的挑战。</p>
<ul>
<li>第一，在大数据环境下，经常出现非结构化或半结构化数据。虽然按程序解析这些数据是可能的，但这会导致冗长的样板代码。为了让用户立刻查询数据，Spark SQL包含了一个模式推理算法，针对JSON以及其他半结构化数据。</li>
<li>第二，大规模处理往往超出了聚合，需要在数据上加入机器学习。我们描述了Spark SQL是如何被整合到Spark机器学习库的一个新的高级API中的。</li>
<li>第三，数据管道data pipelines经常需要从不同存储系统中整合数据。基于在4.4.1章节中的数据源API的构建，Spark SQL支持查询联邦，允许一个单一程序高效地读取不同数据源。</li>
</ul>
<p>以上特性，都是基于Catalyst框架构建。</p>
<h3 id="5-1-半结构化数据schema推理"><a href="#5-1-半结构化数据schema推理" class="headerlink" title="5.1 半结构化数据schema推理"></a>5.1 半结构化数据schema推理</h3><p>在大规模环境下半结构化数据很普遍，因为随着时间的推移，很容易生成和添加字段。在Spark用户当中，我们发现JSON作为输入数据的使用率很高。不幸地是，在Spark或MapReduce这样的过程式环境中使用JSON很麻烦：大部分用户使用类ORM的库（如Jackson[21]）,将JSON结构映射为Java对象，或者采用一些更低级别的库来尝试解析每个输入的记录。</p>
<p>在Spark SQL中，我们添加了一个JSON数据源，来自动从一个记录集合中，推断出一个schema。例如，给定JSON对象如图5所示，这个库推断出的schema如图6所示。</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line"><span class="attr">"text "</span>: <span class="string">"This is a tweet about #Spark"</span>,</span><br><span class="line"><span class="attr">"tags "</span>: [<span class="string">"#Spark"</span>],</span><br><span class="line"><span class="attr">"loc "</span>: &#123;<span class="attr">" lat "</span>: <span class="number">45.1</span> , <span class="attr">"long "</span>: <span class="number">90</span>&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"text "</span>: <span class="string">"This is another tweet"</span>,</span><br><span class="line"><span class="attr">"tags "</span>: [],</span><br><span class="line"><span class="attr">"loc "</span>: &#123;<span class="attr">" lat "</span>: <span class="number">39</span>, <span class="attr">"long "</span>: <span class="number">88.5</span>&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"text "</span>: <span class="string">"A #tweet without #location"</span>,</span><br><span class="line"><span class="attr">"tags "</span>: [<span class="string">"#tweet"</span>, <span class="string">"#location"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<center>图5 tweets上的JSON记录示例</center>

<figure class="highlight"><table><tr><td class="code"><pre><span class="line">text STRING NOT NULL,</span><br><span class="line">tags ARRAY&lt;STRING NOT NULL&gt; NOT NULL,</span><br><span class="line">loc STRUCT&lt;lat FLOAT NOT NULL, long FLOAT NOT NULL&gt;</span><br></pre></td></tr></table></figure>
<center>图6 基于图5的tweets的schema推断</center>

<p>用户可以很简单地注册一个JSON文件作为一张表，通过path访问字段的语法来查询它，如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> loc.lat, loc.long </span><br><span class="line"><span class="keyword">FROM</span> tweets</span><br><span class="line"><span class="keyword">WHERE</span> <span class="built_in">text</span> <span class="keyword">LIKE</span> <span class="string">'%Spark%'</span> <span class="keyword">AND</span> tags <span class="keyword">IS</span> <span class="keyword">NOT</span> <span class="literal">NULL</span></span><br></pre></td></tr></table></figure>
<p>//部分省略…</p>
<h3 id="5-2-和机器学习库进行集成"><a href="#5-2-和机器学习库进行集成" class="headerlink" title="5.2 和机器学习库进行集成"></a>5.2 和机器学习库进行集成</h3><p>作为一个Spark SQL能力在其他Spark模块中使用的例子，机器学习lib，引入了一个新的使用DataFrame[26]的high-level API。这个新的API，基于机器学习pipeline的概念，它是一种抽象，类似其他high-level机器学习库Scikit-Learn[33]。一个pipeline就是在数据上进行各种transformations的graph，比如特征提取、归一化、降维以及模型训练，每一个操作都要交换数据集。Pipelines是一个非常有用的抽象，因为机器学习工作流有许多的步骤，把这些步骤表示成可组成的各个元素，使得它更加容易改变pipeline的部分或者检索调优参数，在整个工作流的层面上。</p>
<p>为了在pipeline stages之间交换数据，MLlib的开发者需要一个格式，可以压缩（因为数据集很大）还需具备灵活性，每条记录中可以有多种类型字段。例如，一个用户开始处理的记录，包含文本字段以及数值字段，运行特征算法后如TF-IDF，文本字段转为了一个向量vector，规范化另一个字段，对整个特征集进行降维操作等等。为了表示这个数据集，使用DataFrames的新API，每列表示数据的一个特征。在pipelines中，所有算法都可以使用，给出输入列的名字和输出列，因此可以对字段的任何子集进行调用并生成新的。这使得开发者更加容易构建复杂pipelines，同时保留每条记录的原始数据。为了说明这个API，图7给出了一个简短的pipeline和创建DataFrames的schema。<strong>从text字段提取words，运行词频统计特征器，转为特征向量，进行逻辑回归训练。</strong></p>
<p><img src="/blog/8973d72a/ml_pipeline.png" alt></p>
<center>图7 机器学习pipeline和Python code运行</center>

<p><strong>MLlib中使用Spark SQL的主要方面是为了创建自定义类型向量vectors</strong>。vector的可以存储稀疏和稠密vectors，使用四个基本类型字段表示：</p>
<ul>
<li>一个是boolean类型，表示是稀疏还是稠密；</li>
<li>一个是向量的大小；</li>
<li>一个是indices数组（针对稀疏坐标）；</li>
<li>一个是double值数组（针对稀疏向量的非0坐标或者所有坐标）。</li>
</ul>
<p>除了DataFrame可以跟踪和管理columns的能力，其他实用性的原因为：在Spark支持的编程中，更加容易抛出MLlib的新API。之前，MLlib中的每个算法持有的对象，都是针对特定领域的概念的（比如，打标点分类，用户产品推荐等级），这些类必须使用各种语言实现一遍（比如，从Scala拷贝到Python）。在所有语言中使用DataFrame，可以更加简单地抛出所有算法，因为我们仅仅需要Spark SQL中的数据约定。这个在Spark绑定其他语言非常重要。</p>
<p>最后，在MLlib中使用DataFrame作为存储，也会更加容易使用SQL中的所有算法。我们简单定义一个在3.7章节提到的MADlib-sytle的UDFs，在表上内部调用这些算法。我们也在探索API，在SQL中构建机器学习pipeline。</p>
<h3 id="5-3-外部数据联邦查询"><a href="#5-3-外部数据联邦查询" class="headerlink" title="5.3 外部数据联邦查询"></a>5.3 外部数据联邦查询</h3><p>数据pipelines通常需要整合来自异构数据源的数据。例如，推荐系统的pipeline，需要整合traffic logs和一个user profile数据库以及用户社交媒体流。这些数据源通常处在不同的机器或地理位置，查询它们可能代价很高。Spark SQL数据源任何时候，尽可能利用Catalyst进行下推谓词到数据源中。</p>
<p>例如，下面的例子，使用JDBC数据源和JSON数据源，join两张表，找到最近注册用户的traffic log。方便的是，所有数据源能够自动推断schema不需要用户定义。这个JDBC数据源将下推filter谓词到MySQL，来减少传输数据的数量。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TEMPORARY</span> <span class="keyword">TABLE</span> <span class="keyword">users</span> <span class="keyword">USING</span> jdbc</span><br><span class="line">OPTIONS(driver <span class="string">"mysql"</span> <span class="keyword">url</span> <span class="string">"jdbc:mysql://userDB/users"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TEMPORARY</span> <span class="keyword">TABLE</span> <span class="keyword">logs</span> <span class="keyword">USING</span> <span class="keyword">json</span> </span><br><span class="line">oPTIONS(<span class="keyword">path</span> <span class="string">"logs.json"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> users.id, users.name, logs.message</span><br><span class="line"><span class="keyword">FROM</span> <span class="keyword">users</span> <span class="keyword">JOIN</span> <span class="keyword">logs</span> <span class="keyword">WHERE</span> users.id = logs.userId</span><br><span class="line"><span class="keyword">AND</span> users.registrationDate &gt; <span class="string">"2015-01-01"</span></span><br></pre></td></tr></table></figure>
<p>未来，Spark SQL的发布，我们也会为KV存储添加谓词下推的能力，比如HBase和Cassandra，它们在谓词过滤的形式有限。</p>
<h2 id="6-评估"><a href="#6-评估" class="headerlink" title="6. 评估"></a>6. 评估</h2><p>我们从两个维度评估Spark SQL的性能：SQL查询处理性能和Spark程序性能。特别地，我们将介绍Spark SQL的可扩展性架构，不仅支持丰富的功能，而且还带来持续的性能优化，相比之前基于Spark的SQL引擎。而且，对于Spark应用开发者，DataFrame API的比起原生Spark API具有持续的加速，虽然Spark程序更加具体容易理解。最后，整合了关系型和过程式查询的应用，在集成的Spark SQL引擎上，执行的速度比起单独运行SQL和过程式代码更加快速。</p>
<h3 id="6-1-SQL性能"><a href="#6-1-SQL性能" class="headerlink" title="6.1 SQL性能"></a>6.1 SQL性能</h3><p>我们将Spark SQL和Shark、Impala[23]进行性能比较，基于AMPLab的大数据benchmark[3]，使用Pavlo等人[31]开发的web分析workload。这个负载包含了四种类型的查询带有不同的参数，执行scans、agg、joins以及UDFs的MapReduce作业。我们使用6个EC2 i2.xlarge机器组成的集群（1个master，5个workers），每个机器4core、30Gmem和800GSSD，运行HDFS 2.4，Spark1.3，Shark 0.9.1以及Impala 2.1.1。这个数据集是110GB的数据，是经过列式存储Parquet格式压缩后的。</p>
<p>图8展示了每个查询的结果，通过查询的类型来分组。查询1-3有不同的参数，它们的选择性不同，1a、2a等是最有选择性的，1c、2c等是选择性最低的，并且处理更多的数据。查询4使用基于Python的Hive UDF，在Impala中不能直接支持，很大程度受限于UDF的CPU代价。</p>
<p>在所有查询中，我们看到，Spark SQL一直都比Shark快，和Impala相当。和Shark的主要差异在于Catalyst中的代码生成（4.3.4章节），减少了CPU的开销。这个特性使得Spark SQL与基于C++和LLVM的Impala引擎具有的一定竞争性。在查询3a中和Impala最大的差距就是，Impala选取了一个更好的join计划，因为查询的选择性使得其中一个表很小。</p>
<p><img src="/blog/8973d72a/shark_impala_sparksql.png" alt></p>
<center>图8 Shark、Impala、Spark SQL 大数据基准查询性能对比</center>

<h3 id="6-2-DataFrames对比原生Spark-Code"><a href="#6-2-DataFrames对比原生Spark-Code" class="headerlink" title="6.2 DataFrames对比原生Spark Code"></a>6.2 DataFrames对比原生Spark Code</h3><p>除了运行SQL查询，Spark SQL也能让非SQL开发者写一个更加简单、更加高效Spark Code，通过DataFrame的API。Catalyst可以在DataFrame的操作上执行优化，这个硬编码很难做到的，比如谓词下推、pipelining以及自动join的选择。甚至如果没有这些优化，DataFrame的API也能够做出更加高效地执行，由于codegen的作用。这个对于Python应用也是一样的，因为Python一般是比JVM慢的。</p>
<p>针对这个计算，我们比较了Spark程序的两种实现，做一个分布式的聚合。数据集由10亿的整形对(a, b构成，其中a有10万个唯一值，在同样的5个worker i2.xlarge集群上。我们计算了每个a值对应的b值的平均值，我们衡量这个耗时。首先，我们看一个版本，就是使用Spark的Python API中的map和reduce函数来计算平均值：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sum_and_count = </span><br><span class="line">	data.map(<span class="keyword">lambda</span> x :(x.a, (x.b, <span class="number">1</span>)))</span><br><span class="line">		.reduceByKey(<span class="keyword">lambda</span> x, y: (x[<span class="number">0</span>]+y[<span class="number">0</span>], x[<span class="number">1</span>]+y[<span class="number">1</span>]))			</span><br><span class="line">        .collect()</span><br><span class="line">[(x[<span class="number">0</span>], x[<span class="number">1</span>][<span class="number">0</span>] / x[<span class="number">1</span>][<span class="number">1</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> sum_and_count]</span><br></pre></td></tr></table></figure>
<p>相反，使用DataFrame API的，只需要如下简单的管理：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.groupBy(<span class="string">"a"</span>).avg(<span class="string">"b"</span>)</span><br></pre></td></tr></table></figure>
<p>图9显示了DataFrame版本的代码性能是Python版本的12倍，而且更加简洁。这是因为DataFrame API，仅仅逻辑计划是通过Python构建的，所有的物理执行都被编译为原生Spark代码作为JVM字节码，会得到一个更加高效的执行。实际上，DataFrame版本也比Scala版本的代码性能高出2倍。主要由于codegen：DataFrame的代码版本避免了手写Scala代码中，昂贵的KV对空间申请。</p>
<p><img src="/blog/8973d72a/python_scala_dataframe.png" alt></p>
<center>手写python、scala代码和dataframe的聚合性能</center>

<h3 id="6-3-Pipeline性能"><a href="#6-3-Pipeline性能" class="headerlink" title="6.3 Pipeline性能"></a>6.3 Pipeline性能</h3><p>DataFrame的API也能够在应用中优化性能，通过整合关系型和过程式处理，让开发者将所有的操作写在一个单独的程序中，跨越关系型和过程式代码，形成流水线计算。举个简单的例子，我们考虑一个2阶段的pipeline，从语料库中选择文本消息的子集并计算最常用的单词。虽然很简单，但是能对一些真实世界的pipelines建立模型。例如，计算特定人群在tweet中使用的最流行词汇。</p>
<p>在这个实验中，我们在HDFS中生成了一个包含100亿条消息的合成数据集。每条信息平均包含10个从英语词典中提取的单词。pipeline的第一个阶段就是使用关系型过滤，来初略选择90%的数据，第二阶段就是计算单词数据量。</p>
<p>首先，我们使用一个单独的SQL查询和一个基于Scala的Spark作业来实现管道，这可能发生在运行单独的关系引擎和过程引擎的环境中（例如，Hive和Spark）。然后，我们又实现一个组合的管道，使用DataFrame的关系算子执行过滤，使用RDD的API在结果上执行单词统计。与第一个pipeline相比，第二个pipeline避免了在传递给Spark作业之前，将整个SQL查询结果的保存到HDFS作为中间结果集的成本。因为，Spark SQL将用于单词统计的map操作和用于过滤的关系算子进行管道化操作。图10给出了两种方法的运行性能比较，除了更加容易理解和操作之外，基于DataFrame的pipeline性能也提升2倍。</p>
<p><img src="/blog/8973d72a/sql_sparkjob_dataframe.png" alt></p>
<center>图10 sql+spark作业与dataframe集成方式，在两阶段pipeline的性能对比</center>

<h2 id="7-研究应用"><a href="#7-研究应用" class="headerlink" title="7. 研究应用"></a>7. 研究应用</h2><h3 id="7-1-通用在线聚合"><a href="#7-1-通用在线聚合" class="headerlink" title="7.1 通用在线聚合"></a>7.1 通用在线聚合</h3><p>//省略…</p>
<h3 id="7-2-基因学计算"><a href="#7-2-基因学计算" class="headerlink" title="7.2 基因学计算"></a>7.2 基因学计算</h3><p>//省略…</p>
<h2 id="8-相关工作"><a href="#8-相关工作" class="headerlink" title="8. 相关工作"></a>8. 相关工作</h2><h3 id="8-1-编程模型"><a href="#8-1-编程模型" class="headerlink" title="8.1 编程模型"></a>8.1 编程模型</h3><p>许多系统试图将过程式处理和关系型处理相结合，使用在大规模集群中。Shark[38]是最接近于Spark SQL的，运行在相同的引擎之上，提供同样的关系型查询和高级分析的结合。Spark SQL通过一个丰富的和对开发者更加友好的API，DataFrames，对Shark进行了改进。在宿主编程语言中构造的查询，可以以模块化的方式组合在一起（见3.4章节）。也可以在原生RDDs上直接运行关系型查询，支持除Hive以外的更多数据源。</p>
<p>给Spark SQL的设计带来灵感的系统是DryadLINQ[20]，它是在C#中将语言集成查询编译为分布式DAG执行引擎。LINQ查询也是关系型的，但是能够直接对C#对象进行操作。Spark SQL超越了DryadLINQ，提供一个类似公共数据科学库[32, 30]的DataFrame接口，作为数据源和类型的API，支持在Spark上执行的迭代算法。</p>
<p>其他一些系统只在内部使用数据模型，将过程式代码转为UDFs。例如，Hive和Pig[36, 29]提供关系型查询语言，但是大量使用UDF接口。ASTERIX[8]内部有一个半结构化数据模型。Stratosphere[2]也有一个半结构化模型，但是提供了Scala和Java的API，让用户很容易使用UDFs。PIQL[7]也同样提供了Scala的DSL。与这些系统相比，Spark SQL和原生Spark应用集成更加紧密，能够直接在用户自定义类中查询数据（原生Java/Python对象），让开发者在同一个语言环境下，混合使用过程式和关系型APIs。而且，通过Catalyst优化，Spark SQL实现了优化（例如Codegen）和其他功能（流入JSON和机器学习数据类型的模式推断），这些在大规模计算框架中是没有的。我们相信这些特性从本质上为大数据提供了一个集成的，易于使用的环境。</p>
<p>最后，DataFrame API既可以在单个机器[32, 30]也可以在集群[13, 10]上构建。不像之前的APIs，Spark SQL通过关系优化器，优化了DataFrame的计算。</p>
<h3 id="8-2-扩展的优化器"><a href="#8-2-扩展的优化器" class="headerlink" title="8.2 扩展的优化器"></a>8.2 扩展的优化器</h3><p>Catalyst优化器与可扩展优化器框架，如<strong>EXODUS</strong>[17]和<strong>Cascades</strong>[16]有着相似的目标。虽然，传统上优化器框架要求一个DSL来写规则，同时一个优化器的编译器，将它们翻译为可运行的代码。我们的主要改进就是采用一个函数式编程语言来构建我们的优化器，提供了同样或更好的表达性，同时降低了维护成本和学习曲线。高级语言特性在Catalyst的方方面面有所帮助，例如，<strong>CodeGen采用quasiquotes</strong>（4.3.4章节所述）的方式，是这项任务中最简单、最易组合的方法之一。虽然，扩展性很难衡量质量，但有一个有希望的迹象是，Spark SQL在发布后的头8个月内有超过50个外部贡献者。</p>
<p>针对CodeGen，LegoBase[22]最进提出一个方法，<strong>使用Scala中的生成式编程，有可能是替代quasiquotes</strong>。</p>
<h3 id="8-3-高级分析"><a href="#8-3-高级分析" class="headerlink" title="8.3 高级分析"></a>8.3 高级分析</h3><p>Spark SQL近期的工作主要是在大规模集群中运行高级分析算法，包括平台迭代算法[39]和图分析[15, 24]。</p>
<h2 id="9-总结"><a href="#9-总结" class="headerlink" title="9. 总结"></a>9. 总结</h2><p>//省略…</p>
<h2 id="10-致谢"><a href="#10-致谢" class="headerlink" title="10. 致谢"></a>10. 致谢</h2><p>//省略…</p>
<h2 id="11-参考文献"><a href="#11-参考文献" class="headerlink" title="11. 参考文献"></a>11. 参考文献</h2><p>//待完善…</p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>SparkSQL</tag>
        <tag>Spark</tag>
        <tag>Catalyst</tag>
      </tags>
  </entry>
  <entry>
    <title>Paper翻译 Apache Calcite A Foundational Framework for Optimized Query Processing Over Heterogeneous Data Sources</title>
    <url>/blog/10fa9651.html</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><a href="https://calcite.apache.org" target="_blank" rel="noopener">Apache Calcite</a>是一个基础软件框架，提供了查询处理、优化和查询语言，支持多个主流开源数据处理系统，比如Apache Hive，Apache Storm，Apache Flink，Druid和MapD。Calcite的架构由如下组件构成：</p>
<ul>
<li>模块化、可扩展的优化器，内置了上百个优化规则；</li>
<li>查询处理器，可以处理各种查询语言；</li>
<li>适配器架构设计，用于扩展和支持异构数据源和存储（关系模型、半结构化、流和地理空间）。</li>
</ul>
<p>这个灵活、可内嵌和可扩展的架构，使得Calcite在大数据框架中被应用是一个很好的选择。这是一个很活跃的项目，会持续地引入新的数据类型，查询语言、查询处理和优化的方法。</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>遵循具有重要意义的关系数据库系统，传统关系数据库引擎主导了数据处理领域。然而，早在2005年，Stonebraker和Çetintemel预测[49]，我们将看到一系列特定的引擎，如列式存储、流处理引擎，文本检索引擎等。他们争论着只有特定的引擎才能提供高效的性能，结束“one size fits all”的模式。他们的愿景今天似乎比以往任何时候都更有意义。事实上，许多专门的开源系统已经流行起来，比如Storm[50]和Flink[16]（流处理），Elasticsearch[15] (文本检索)，Apache Spark [47]，Druid [14]等等。</p>
<p>由于各个组织根据他们的需要在定制化数据处理系统上的投资，出现了两个重要的问题：</p>
<ul>
<li>这些特定系统的开发者们已经遇到一些相关的问题，比如查询优化[4,25]，或者查询语言的支持，比如SQL和相关扩展（流式查询[26]），以及受LINQ启发的语言集成查询[33]。没有一个统一的框架，许多工程师独自开发相似的优化逻辑和语言支持，是在浪费精力。</li>
<li>程序员们使用这些特定的系统，不得不把它们集成在一起。一个组织也许依赖Elasticsearch、Spark和Druid。我们需要构建一个系统，能够支持在跨异构数据源[55]上进行优化查询的能力。</li>
</ul>
<p>Apache Calcite的开发就是被用来解决上述问题的。它是一个完整的查询处理系统，支持查询处理、优化和语言，是任何数据库管理系统所需要的，除了数据的存储和管理放置到特定的引擎中。Calcite很快就被Hive，Drill[13]，Storm和许多其他数据处理引擎所采用。比如，Hive[24]是一个主流的基于Hadoop的数仓项目。因为，Hive从批处理看框架转为了交互式SQL查询平台，很显然该项目在它的核心基础之上需要一个强大的优化器。因此，Hive采用Calcite作为它的优化器，它们的集成也一直在发展。许多其他项目或产品，也遵循着这种方式，包括Flink、MapD[12]等等。</p>
<p>因此，Calcite通过暴露公共接口给多个系统，能够进行跨平台的优化。为了提高效率，优化器需要进行全局性地推理。例如，在多个不同系统上，进行物化视图的选择。</p>
<p>构建一个通用的框架并非没有挑战。尤其，该框架需要有足够的可扩展性和灵活性，来满足不同类型系统的集成要求。</p>
<p>我们相信以下特性可以促进Calcite在开源社区和工业界的广泛使用：</p>
<ul>
<li><strong>开源友好</strong>：在过去10年的主要数据处理平台已经是开源的或大部分是基于开源的。Calcite是一个开源框架，由Apache基金会[5]支持，提供一种协作开发项目的方式。而且，该软件由Java编写，更加容易和多个新的数据处理系统交互[12,13,16,24,28,44]，它们本身也是用Java写的（或者是基于JVM的Scala），尤其是Hadoop生态系统中的那些系统。</li>
<li><strong>多数据模型</strong>：Calcite提供查询优化和查询语言的支持，使用流式和传统数据处理模式。Calcite将流看作时间顺序的记录集合或事件，它们没有像传统数据处理系统那样，被持久化在磁盘上。</li>
<li><strong>灵活的查询优化器</strong>：优化器的每个组件都是可插拔和可扩展的，从规则到代价模型。而且，Calcite支持多种计划引擎。因此，优化过程可以被拆解为多个阶段，通过不同优化引擎来处理，这取决于哪个优化引擎更适合这个阶段。</li>
<li><strong>跨系统支持</strong>：Calcite框架能够运行和优化多个查询处理系统和后端数据库。</li>
<li><strong>可靠性</strong>：Calcite是靠的，因为多年的广泛使用，带来了平台的大量的测试。Calcite也包含了一些扩展的测试套件，来验证系统的所有组件，包括查询优化器规则和后端数据源的集成。</li>
<li>SQL支持和扩展：许多系统并不提供它们自己的查询语言，但是更倾向依赖已有的东西，比如SQL。为了这些，Caclite提供了对ANSI标准SQL的支持，以及各种SQL方言和扩展。比如，在流式数据或嵌套数据上的表达式查询。而且，Calcite还包含了符合JDBC标准的驱动。</li>
</ul>
<p>剩余章节的组织如下。第二章讨论相关工作，第三章介绍Calcite架构和主要组件，第四章描述关系代数在Calcite的Core中，第五章阐述Calcite的适配器，定义怎样读取外部数据源的一个抽象。然后，第六章节，描述Calcite的优化器和主要特性。第七章描述处理不同查询处理模式的扩展。第八章，给出了一个已经使用Calcite的数据处理系统概览。第九章讨论该框架的未来扩展，第十章总结。</p>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h2><p>虽然，Calcite是当前Hadoop生态系统中被大数据分析广泛采用的优化器，它背后的思想其实并不新奇。例如，优化器的思想是源于<strong>Volcano</strong>[20]和<strong>Cascades</strong>[19]框架，结合其他广泛使用的优化器技术，比如<strong>物化视图重写</strong>[10,18,22]。还有其他系统尝试为Calcite填补类似的功能。</p>
<p><strong>Orca</strong>[45]是一个模块化查询优化器，使用在数据管理产品中，如GreenPlum和HAWQ。Orca通过实现一个用于在两者之间交换信息的框架（称为数据交换语言），将优化器与查询执行引擎分离。Orca还提供了用于验证生成的查询计划的正确性和性能的工具。相比于Orca，Calcite可以用于独立的查询执行引擎，联合多个存储和后端处理，包括可插拔计划和优化器。</p>
<p><strong>Spark SQL</strong>[3]扩展了Apache Spark来支持SQL查询执行器，也可以在Calcite的多个数据源上执行查询。然而，虽然在Spark中的<strong>Catalyst优化器</strong>也尝试最小化查询执行的成本，但是它缺乏Calcite中使用的动态规划，有陷入局部最小化的风险。</p>
<p><strong>Algebricks</strong>[6]是一个查询编译器架构，为大数据查询处理提供了数据模型无关的代数层和编译器框架。高等级语言被编译为Algebricks的逻辑代数。Algebricks会生成一个优化的作业，用于Hyracks的后端并行处理。Calcite与Algebricks共享模块化方法，还支持基于成本的优化。在当前版本的Calcite中，查询优化器架构，采用的是基于Volcano[20]的动态规划，以及扩展的Orca[45]中多阶段优化。虽然，在原理上Algebricks能够支持多个后端处理（比如，Tez，Spark），但是多年来Calcite为不同的后端提供了良好的测试支持。</p>
<p><strong>Garlic</strong>[7]是一个异构数据管理系统，可以将来自多个系统的数据用统一的对象模型表示。然而，Garlic不支持来自不同系统的查询优化，依赖每个系统优化自己的查询。</p>
<p><strong>FORWARD</strong>[17]是一个联合查询处理器，实现了SQL的超集，即SQL++[38]。SQL++有半结构化数据模型，将JSON和关系数据模型集成进来。而Calcite在查询计划期间，半结构化数据模型是通过关系数据模型表示的。SQL++将联合查询拆解为多个子查询，然后根据相应的查询计划，在底层数据库中执行。最后，FORWARD引擎进行数据的合并。</p>
<p>另一个，联合的数据存储和处理系统是<strong>Big-DAWG</strong>。它抽象了一个广泛的数据模型，包括关系型、时序、流式。在Big-DAWG中的一个抽象单元，叫island of information（信息孤岛）。每一个island of information有一个查询语言、数据模型，能够连接到一个或多个存储系统。在单个island of information的边界中，支持跨存储系统的查询。相反，Calcite采用了一个统一的关系抽象模型，来支持跨多个具有不同数据模型的后端查询。</p>
<p><strong>Myria</strong>是一个通用的大数据分析引擎，对Python语言[21]高度支持。用于为其他后端引擎生成查询计划，比如Spark、PostgreSQL。</p>
<h2 id="3-架构"><a href="#3-架构" class="headerlink" title="3. 架构"></a>3. 架构</h2><p>Calcite包含很多构成典型数据库管理系统的部分。然而，它跳过了一些关键组件，如数据存储、处理数据算法和存储元数据的仓库。这些舍弃是经过深思熟虑的，在具有多个数据存储位置和多个数据处理引擎的应用之间，Calcite作为一个中间媒介是最好的选择。同时，它也是构建定制的数据处理系统的坚实基础。</p>
<p>图1给出了Calcite架构的主要组件，Calcite的优化器使用关系算子树来作为内部表示。这个优化器引擎，主要由三个组件构成：rules、metadata providers以及planner engines。在第六章我们将深入讨论这些组件的细节。途中的虚线表示和其他框架的外部集成，和Calcite集成有很多种方式。</p>
<p><img src="/blog/10fa9651/calcite_archi.png" alt></p>
<center>图1 Calcite架构和交互</center>

<p>首先，Calcite包含一个查询解析器和验证器，能够将SQL查询翻译为关系算子树。因为Calcite不包含一个存储层，但它提供了一个机制，可以通过适配器（第五章讲述）来定义外部存储引擎的表schemas和views，所以它被用于这些引擎之上。</p>
<p>其次，Calcite虽然为那些需要语言支持的数据提供了优化的SQL支持，但也为那些已经有自己的语言解析和解释的系统提供的优化的支持：</p>
<ul>
<li><p>一些支持SQL查询的系统，没有或有限的查询优化。例如，Hive和Spark早期就提供SQL支持，但是不包含优化器。针对这样的场景，一旦查询已经被优化，Calcite能够再次将关系算子树转为SQL。这样的特性，可以使得Calcite可以独立运行在任何有SQL但没有优化器的数据管理系统之上。</p>
</li>
<li><p>Calcite的架构不仅仅专为优化SQL查询。通常，数据处理系统针对它们自己的查询语言使用自己的解析。Calcite也能够有助于优化这些查询。事实上，Calcite允许算子树通过直接实例化来构建，使得构建变得更加容易。有一种是可以使用内置的relational expressions builder接口。例如，假设我们想使用expression builder来表达如下Pig[41]的脚本。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">emp = <span class="keyword">LOAD</span> <span class="string">'empolyee_data'</span> <span class="keyword">AS</span> (deptno, sal);</span><br><span class="line">emp_by_dept = GROUP emp BY (deptno);</span><br><span class="line">emp_agg = FOREACH emp_by_dept GENERATE GROUP as deptno, COUNT(emp.sal) AS c, SUM(emp.sal) as S;</span><br><span class="line">dump emp_agg;</span><br></pre></td></tr></table></figure>
<p>等价表示如下所示：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> RelNode node = builder</span><br><span class="line">    .scan(<span class="string">"employee_data"</span>)</span><br><span class="line">    .aggregate(builder.groupKey(<span class="string">"deptno"</span>),</span><br><span class="line">               builder.count(<span class="keyword">false</span>, <span class="string">"c"</span>),</span><br><span class="line">               builder.sum(<span class="keyword">false</span>, <span class="string">"s"</span>, builder.field(<span class="string">"sal"</span>)))</span><br><span class="line">    .build();</span><br></pre></td></tr></table></figure>
<p>这个接口给出了在构建关系表达式时的主要构建过程。在优化阶段完成之后，应用程序会得到优化后的关系表达式，能够再映射到系统查询处理单元中。</p>
</li>
</ul>
<h2 id="4-查询代数"><a href="#4-查询代数" class="headerlink" title="4. 查询代数"></a>4. 查询代数</h2><p><strong>Operators</strong>。关系代数[11]是Calcite的核心。除了表达常见的数据操作的算子之外，例如<em>filter</em>，<em>project</em>，<em>join</em>等，Calcite还包含一些额外的操作，来满足不同的需求，例如简洁地表达复杂的操作和高效地识别优化的时机。</p>
<p>例如，针对OLAP，决策制定和流式应用，使用window定义来表达复杂分析函数，如数量在一个时间周期或数据行数上的移动平均数，是很常见的。因此，Calcite引入了<em>window</em>算子来封装window定义，即上下边界、分区等，以及在每个窗口内执行聚合函数。</p>
<p><strong>Traits</strong>。Calcite没有使用不同的实体来表示逻辑和物理算子，而是通过使用<em>traits</em>关联一个算子，来描述它的物理属性。这些traits有助于优化器评估不同可选算子的成本。改变trait的值，并不是改变正在评估的逻辑表达式，即给定的算子输出的行还是一样的。</p>
<p>在优化期间，Calcite会尝试在关系表达式上增强某个特定的traits，例如特定字段的排序顺序。关系算子会实现一个<em>converter</em>接口来表明如何将表达式的traits从一个值转为另一个值。</p>
<p>Calcite包含了一些常见的traits，这些traits描述了关系表达式生产数据的物理属性，例如，<em>ordering</em>、<em>grouping</em>和<em>partitioning</em>。和SCOPE优化器[57]类似，Calcite优化器能够分析属性和利用它们寻找计划，来避免不必要的操作。例如，如果sort算子的输入可能已经正确排序，因为底层系统这些数据有同样的排序，则sort操作就可以移除。</p>
<p>除了这些属性，Calcite的重要特性之一就是<em>calling convention</em>的trait。本质上，这个trait表明了该表达式将在相应的数据处理系统中执行。包含<em>calling convention</em>的trait，可以使得Calcite达到透明地优化查询的目标，这些查询也许会横跨不同的引擎，即convention将会被视为任何其他物理属性。</p>
<p>举例来说，MySQL中的Products表和Splunk中的Orders表进行JOIN操作（见图2）。首先，Orders表的scan发生在<em>splunk</em>的convention中，Products表的scan发生在<em>jdbc-mysql</em>的convention中。这两个表必须在他们各自引擎中被scan。这个Join操作是在<em>logical</em>的convention中，意味着没有实现被选择。而且，在图2上的SQL查询中，还包含了filter，通过特定的适配器规则（见第五章节），下推到了<em>splunk</em>中。有一种可能的实现，就是使用Apache Spark作为外部引擎：这个Join操作被转为<em>spark</em>的convention，它的输入是一个converters，从<em>jdbc-mysql</em>和<em>splunk</em>到<em>spark</em>的convention。但是，有一个更加高效的实现：事实上，Splunk能够通过ODBC回查MySQL，一个planner规则通过<em>splunk-to-spark</em>的converter下推Join，然后Join就在<em>splunk</em>的convention中，能够在Splunk引擎中执行。</p>
<p><img src="/blog/10fa9651/optimization_process.png" alt></p>
<center>图2 查询优化的过程</center>

<h2 id="5-适配器"><a href="#5-适配器" class="headerlink" title="5. 适配器"></a>5. 适配器</h2><p>一个适配器是一个架构模式，定义了Calcite是如何跟多种数据源交互，实现统一访问的。图3描述了它的组件。本质上，一个适配器由一个model、一个schema和一个schema factory构成。model是一个数据源被访问的物理属性的描述。schema是在model中可以找到的数据定义（格式和布局）。数据本身物理上是通过tables访问的。Calcite的table接口定义在适配器中，能够读取数据，作为查询被执行。适配器也许会定义一系列规则，添加到planner中。例如，包含一些规则将各种逻辑关系表达式转为适配器约定（convention）的相应关系表达式。schema  factory组件要求来自model的元数据信息来创建schema。</p>
<p><img src="/blog/10fa9651/adapter_design.png" alt></p>
<center>图3 适配器设计</center>

<p>在第四章节，已经讨论过，Calcite使用physical trait，即<em>calling convention</em>，来识别关系操作对应到特定的后端数据库。这些物理算子在每个适配器中实现了底层表的访问路径。当一个查询被解析，转为关系代数表达式，每个表会创建一个算子来表示在表上的scan操作，这是一个最小的接口，一个适配器必须实现。如果一个适配器实现了表的scan算子，这个Calcite优化器能够使用客户端侧的算子，比如sorting，filtering和joins，在这些表上来执行任意SQL的查询。</p>
<p>表的scan操作包含了适配器要求的必要的信息，将scan操作下发到后端数据库上。为了扩展适配器的功能，Calcite定义了一个<em>enumerable</em>的calling convention。带有enumerable calling convention的关系算子能够简化在元组上的操作，通过迭代器接口。这个calling convention允许Calcite实现的算子，不依赖每个适配的后端。例如，EnumerableJoin算子实现Join操作，通过从它的子节点中收集数据行，在需要的属性上进行join操作。</p>
<p>这些查询仅仅是在表中的小数据集，enumerate（枚举）所有的元组，对于Calcite来说是非常低效的。幸运的是，相同的基于规则的优化器，能够被用于特定适配器规则的优化。例如，假设一个查询，涉及filtering和sorting。一个适配器能够在后端上执行filtering，需要实现一个规则，来匹配LogicalFilter，将它转为适配器的calling convention。这个规则将LogicalFilter转为另一个Filter实例。这个新的Filter节点（node)具有较低的关联成本，使得Calcite可以跨适配器来优化查询。</p>
<p>使用适配器是一个强大的抽象，不仅可以针对特定后端进行查询优化，还可以跨多个后端。Calcite能够将查询涉及的多个后端上的表，将所有可能的逻辑下推到每个后端，然后在这个结果数据上执行joins和aggregations。实现一个适配器可以简单提供一个表scan算子或者涉及许多高级的优化设计。<strong>在关系代数上的任意表达式都可以通过优化器规则下推到适配器</strong>。</p>
<h2 id="6-查询处理和优化"><a href="#6-查询处理和优化" class="headerlink" title="6. 查询处理和优化"></a>6. 查询处理和优化</h2><p>查询优化器是Calcite框架中的核心组件。Calcite通过不断重复地在关系表达式上应用planner rules来优化查询。一个代价模型主导这个处理过程，planner引擎尝试生成一个可替代的表达式，它的语义和原始的保持一样，并且代价更低。</p>
<p><strong>在优化器中的每个组件都是可以扩展的，用户可以添加关系算子，规则，代价模型和统计信息。</strong></p>
<p><strong>Planner Rule</strong>。Calcite包含了一系列的planner rules来转换expression trees。具体来说，一个规则匹配了tree中的一个模式，然后执行一个保留expression语义的转换。Calcite包含了数百个优化规则。然而，依赖Calcite的数据处理系统，包含了它们自己的优化规则，允许有特定的重写，也是很常见的一种方式。</p>
<p>例如，Calcite提供了一个Apache Cassandra[29]的适配器，一个宽表存储通过部分列进行分区，然后在每个分区中，基于其他列进行行排序。正如第五章讨论的，一个适配器尽可能高效地将查询处理下推之到每个后端是非常有益的。下推sort到Cassandra的规则需要满足两个条件：</p>
<ul>
<li>表现前已经过滤到单个分区级别（行排序仅仅在一个分区中）；</li>
<li>要求的排序和Cassandra中的分区排序有相同的前缀。</li>
</ul>
<p>要求<code>LogicalFilter</code>被重写为<code>CassanddraFilter</code>，保证分区过滤被下推之数据库中。规则作用很简单（将<code>LogicalSort</code>转为<code>CassandraSort</code>），但是规则匹配的灵活性使后端能够在复杂的场景中下推算子。</p>
<p>例如，一个复杂用途的规则，看如下查询：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> products.name, <span class="keyword">COUNT</span>(*)</span><br><span class="line"><span class="keyword">FROM</span> sales <span class="keyword">JOIN</span> products <span class="keyword">USING</span> (productId)</span><br><span class="line"><span class="keyword">WHERE</span> sales.discount <span class="keyword">IS</span> <span class="keyword">NOT</span> <span class="literal">NULL</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> products.name</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="keyword">COUNT</span>(*) <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure>
<p><img src="/blog/10fa9651/filterIntoJoin.png" alt></p>
<center>图4 FilterIntoJoinRule应用</center>

<p>上述查询相应的关系代数表达式如图4所示。因为，Where仅仅作用在sales表上，我们可以在Join之前移动filter。这个优化能够极大减少查询执行的时间，因为我们不需要执行谓词没有匹配的行的Join操作。甚至，如果sales和products表都在同一个底层存储中，在Join前移动filter使得适配器将filter下推至底层。Calcite通过<code>FilterInoJoinRule</code>实现了这个优化，将filter节点和作为父节点的Join节点进行匹配，检测filter是否可以被join执行。这种优化方式，体现了Calcite优化方式的灵活性。</p>
<p><strong>Metadata providers</strong>。元数据是Calcite的优化器很重要的一部分，主要有两个作用：一是引导planner往减少整个查询计划成本的方向为目标，二是在应用规则提供一些信息。</p>
<p>元数据提供者主要负责给优化器提供信息，特别在calcite的元数据providers的默认实现中包含了一些方法，可以返回在操作树中执行一个子表达式的总体成本，表达式结果的行数、数据大小，以及可以运行的最大并发度。继而，它还可以提供一个查询计划的结构，比如在一个tree node下的filter condition。</p>
<p>Calcite的providers接口可以允许数据处理系统将它们的元数据挂载到框架中。这些系统也许会选择实现providers，包括重写已有的函数或提供它们自己新的元数据函数，在优化阶段使用到。然而，对于它们当中大部分来说，提供输入数据的统计信息就已经足够了。比如，一个表的数据行数和大小，一个给定列的值是否是唯一的，以及Calcite还会使用默认实现做剩下的工作。</p>
<p>由于metdata providers是可插拔的，所以它们可以在运行期间通过使用Janio[27]（一个java轻量级编译器）来进行编译和实例化。它们的实现包含一个元数据结果缓存，可以达到显著的性能提升。比如，当我们需要计算不同类型的元数据时，例如基数、平均行大小和给定联接的选择性，所有这些计算都依赖于输入的基数。</p>
<p><strong>Planner engines</strong>。一个planner engine的主要目标是触发提供给引擎的规则，直到达到给定的目标。此时，Calcite提供了两种不同的引擎。新的引擎在框架中是可插拔的。</p>
<p>第一个是基于代价的planner engine，基于减少表达式执行代价的目标，来触发输入的规则。该引擎使用动态规划算法，类似Volcano[20]，通过触发给定引擎的规则，来创建和跟踪多个可替换的计划。首先，每个表达式都在planner处注册，以及基于表达式属性和它的输入形成一个digest。当一个规则在表达式e1中触发后，就会生成一个新的表达式e2，planner就会将e2加入一个等价表达式集合Sa中，e1也属于该集合。同时，planner会生成一个新表达式的digest，和之前注册在planner中的表达式digest进行比较。如果在Sb中找到一个和表达式e3有类似digest，表示planner找到重复，将合并Sa和Sb成一个新的等价集合。这个处理过程持续到planner达到一个可配置的fix point。尤其是，它能够非常详尽地探索搜索空间直到所有规则已经应用在所有表达式上。或者，使用启发式规则，即当在最后一次计划迭代时超过给定阈值<em>δ</em>也不能提升执行代价时，来停止搜索。由metadata providers提供的代价函数可以让优化器决定选择哪一个计划。<strong>默认的代价函数实现，整合了对给定表达式在CPU、IO和内存资源使用上的评估</strong>。</p>
<p>第二个引擎是一个详尽的planner，尽可能详尽地触发rules直到生成一个不再被任何规则更改的表达式。这个planner在快速执行规则不用考虑每个表达式代价时是非常有用的。</p>
<p>用户选择使用已有的planner引擎，取决于他们具体的需要。当他们的系统需求改变时，可以从一个切换到另一个是很简单的。另外，用户可以选择生成多阶段优化逻辑，在优化过程的连续阶段应用不同的规则集合。重要的是，两种planner允许Calcite用户通过指导搜索不同的查询计划来减少整个优化时间。</p>
<p><strong>Materialized Views</strong>。在数仓中一个用来加速查询处理的强大技术，就是相关摘要数据预计算或者物化视图。多个Calcite适配器和依赖Calcite的项目有它们自己的物化视图的概念。例如，Cassandra允许用户基于已有的表定义物化视图，由系统自动维护。</p>
<p>这些引擎将它们的物化视图暴露给Calcite，优化器就有机会通过使用视图来替换原表，来将接收的查询重写。尤其，<strong>Calcite提供了两种不同的基于物化视图的重写算法</strong>。</p>
<p>第一个方法是基于视图替换（<em>view substitution</em>）[10,18]。这个目的是通过等价表达式（使用物化视图）来替换关系代数树中的一部分，这个算法流程如下：</p>
<ul>
<li>基于物化视图的scan算子和物化视图定义的plan注册到planner中；</li>
<li>用于尝试统一查询计划中的表达式的转换规则会触发；</li>
<li>视图不需要准确地将查询中匹配的表达式进行替换，因为Calcite的重写算法可以创建部分重写，包含了用于计算所需表达式的额外操作，如带有剩余谓词条件的filters。</li>
</ul>
<p>第二个方法是基于栅格（<em>lattices</em>）[22]。一旦一个数据源被声明形成一个lattice，Calcite会将每个物化信息表示成一个<em>tile</em>，从而优化器可以使用它们来匹配进入的查询。一个方面，这个重写算法在用star schema组织的数据源上进行表达式匹配更加高效，通常用于OLAP应用。另一方面，它比视图替换更具限制性，因为它对底层模式施加了限制。</p>
<h2 id="7-扩展Calcite"><a href="#7-扩展Calcite" class="headerlink" title="7. 扩展Calcite"></a>7. 扩展Calcite</h2><p>之前章节我们已经提到，Calcite不仅仅面向SQL处理进行定制。<strong>实际上，Calcite提供了对SQL的扩展，来表达在其他数据抽象上的查询，比如半结构化、流式和地理空间的数据</strong>。它的内部算子适配这些查询。除了对SQL的扩展，Calcite也包含了一种语言集成查询语言。我们将通过本章节来描述这些扩展，并提供一些示例。</p>
<h3 id="7-1-半结构化数据"><a href="#7-1-半结构化数据" class="headerlink" title="7.1 半结构化数据"></a>7.1 半结构化数据</h3><p>Calcite支持许多复杂字段数据类型，使得关系型和半结构化数据混合存储在表中。尤其，当列是ARRAY、MAP和MULITSET类型时。而且，这些复杂类型是可以嵌套的，例如MAP类型的values可以是ARRAY。ARRAY和MAP列中的数据，可以通过<code>[]</code>操作符提取。存储在这些复杂类型中的特殊类型值不需要预先定义。</p>
<p>例如，Calcite包含一个MongoDB适配[36]，一个文档存储，这些文档由类似json文档那个的数据组成。为了将MongoDB数据抛给Calcite，每个文档都创建一个单列（名为_MAP）的表。许多场景下，希望文档具有一个共同的结构。一个表示邮政编码的文档集合，也许每个都包含字段city name, latitude和longitude。将这些数据表示成一个关系表很有用。在Calcite中，可以抽取想要的值和转为正确的类型后，创建一个视图。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">CAST</span>(_MAP[<span class="string">'city'</span>] <span class="keyword">AS</span> <span class="built_in">varchar</span>(<span class="number">20</span>)) <span class="keyword">AS</span> city,</span><br><span class="line"><span class="keyword">CAST</span>(_MAP[<span class="string">'loc'</span>][<span class="number">0</span>] <span class="keyword">AS</span> <span class="built_in">float</span>) <span class="keyword">as</span> longitude,</span><br><span class="line"><span class="keyword">CAST</span>(_MAP[<span class="string">'loc'</span>][<span class="number">1</span>] <span class="keyword">AS</span> <span class="built_in">float</span>) <span class="keyword">as</span> latitude</span><br><span class="line"><span class="keyword">FROM</span> mongo_raw.zips;</span><br></pre></td></tr></table></figure>
<p>以这种方式建立在半结构化数据上的视图，就更容易地同时操作不同半结构化数据源和关系型数据。</p>
<h3 id="7-2-流"><a href="#7-2-流" class="headerlink" title="7.2 流"></a>7.2 流</h3><p>Calcite基于标准SQL进行了特定流式扩展，提供了一流的流式查询[26]，叫做<em>STREAM</em>扩展，windowing扩展，通过在join或其他操作中使用window表达式，来显式地使用流。这些扩展受到持续查询语言[2]的启发，尝试和标准SQL进行有效地集成。主要的扩展就是，通过<em>STREAM</em>声明，告诉系统用户对新入的记录感兴趣，而不是已有的记录。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> STREAM rowtime, productId, units</span><br><span class="line"><span class="keyword">FROM</span> Orders</span><br><span class="line"><span class="keyword">WHERE</span> units &gt; <span class="number">25</span>;</span><br></pre></td></tr></table></figure>
<p>查询流的关键词STREAM去掉后，查询就变为了普通的关系查询，表示系统应该处理已有的记录，即从流中已经接收的记录，而不是新入的记录。</p>
<p>由于流固有的无边界特性，windowing用于解除阻塞运算符，比如Aggregate和Joins。Calcite的流扩展使用SQL分析函数来表达<strong>滑动和级联窗口聚合</strong>，如下示例所示：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> STREAM </span><br><span class="line">  rowtime, </span><br><span class="line">  productId, </span><br><span class="line">  units,</span><br><span class="line">  <span class="keyword">SUM</span>(units) <span class="keyword">OVER</span> (</span><br><span class="line">      <span class="keyword">ORDER</span> <span class="keyword">BY</span> rowtime </span><br><span class="line">      <span class="keyword">PARTITION</span> <span class="keyword">BY</span> productId </span><br><span class="line">      <span class="keyword">RANGE</span> <span class="built_in">INTERVAL</span> <span class="string">'1'</span> <span class="keyword">HOUR</span> <span class="keyword">PRECEDING</span></span><br><span class="line">  ) unitsLastHour</span><br><span class="line"><span class="keyword">FROM</span> Orders;</span><br></pre></td></tr></table></figure>
<p>翻滚（Tumbling）、跳跃（Hopping）和会话（Session）窗口，通过TUMBLE，HOPPING，SESSION函数开启，相关实用函数比如TUMBLE_END和HOP_END。它们可以分别使用在GROUP BY的clauses和projections。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> STREAM</span><br><span class="line">  TUMBLE_END(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'1'</span> <span class="keyword">HOUR</span>) <span class="keyword">as</span> rowtime,</span><br><span class="line">  productId,</span><br><span class="line">  <span class="keyword">COUNT</span>(*) <span class="keyword">AS</span> c,</span><br><span class="line">  <span class="keyword">SUM</span>(units) <span class="keyword">AS</span> units</span><br><span class="line"><span class="keyword">FROM</span> Orders</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> TUMBLE(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'1'</span> <span class="keyword">HOUR</span>), productId;</span><br></pre></td></tr></table></figure>
<p>涉及窗口聚合的流式查询要求在GROUP BY子句中或在ORDER BY子句中存在单调或准单调表达式，以防滑动和级联窗口查询。</p>
<p>涉及更加复杂的流和流JOIN的流式查询，可以通过在JOIN字句中使用隐式窗口表达式来表示。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECt</span> STERAM </span><br><span class="line">  o.rowtime, </span><br><span class="line">  o.productId, </span><br><span class="line">  o.orderId,</span><br><span class="line">  s.rowtime <span class="keyword">AS</span> shipTime</span><br><span class="line"><span class="keyword">FROM</span> Orders <span class="keyword">AS</span> o</span><br><span class="line"><span class="keyword">JOIN</span> Shipments <span class="keyword">AS</span> s </span><br><span class="line"><span class="keyword">ON</span> o.orderId = s.orderId </span><br><span class="line"><span class="keyword">AND</span> s.rowtime <span class="keyword">BETWEEN</span> o.rowtime <span class="keyword">AND</span> o.rowtime + <span class="built_in">INTERVAL</span> <span class="string">'1'</span> <span class="keyword">HOUR</span>;</span><br></pre></td></tr></table></figure>
<p>在这个隐式窗口的例子中，Calcite的query planner会验证这个表达式是单调的。</p>
<h3 id="7-3-地理空间查询"><a href="#7-3-地理空间查询" class="headerlink" title="7.3 地理空间查询"></a>7.3 地理空间查询</h3><p>地理空间支持在Calcite中刚起步，但是正在使用关系代数来实现。核心就是增加一个GEOMETRY的数据类型，来封装不同的几何对象，比如点(point)、曲线(curve)和多边形(polygon)。Calcite将完全兼容OpenGIS Simple Feature Access[39]规范，该规范为访问地理空间数据的SQL接口定义了一个标准。2.下面给出一个列子，查询包含Amsterdam的国家：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">name</span> <span class="keyword">FROM</span>(</span><br><span class="line">  <span class="keyword">SELECT</span> <span class="keyword">name</span>, </span><br><span class="line">    ST_GeomFromTet(<span class="string">'Polygon((4.82 52.543, 4.97 52.43, 4.97 52.33, 4.82 52.33, 4.82 52.33))'</span>) <span class="keyword">AS</span> <span class="string">"Amsterdam"</span>,</span><br><span class="line">    ST_GeomFromText(boundary) <span class="keyword">AS</span> <span class="string">"Country"</span></span><br><span class="line">  <span class="keyword">FROM</span> country</span><br><span class="line">)</span><br><span class="line"><span class="keyword">WHERE</span> ST_Contains(<span class="string">"Country"</span>, <span class="string">"Amsterdam"</span>);</span><br></pre></td></tr></table></figure>
<h3 id="7-4-JAVA语言集成查询"><a href="#7-4-JAVA语言集成查询" class="headerlink" title="7.4 JAVA语言集成查询"></a>7.4 JAVA语言集成查询</h3><p>Calcite可以用于查询多个数据源，不仅仅是关系型数据库，但它的目标也不仅仅是支持SQL语言。虽然，SQL仍然是数据库的主要语言，但是很多程序员喜欢使用语言集成语言，如LINQ[33]。不像SQL是内嵌在JAVA或C++代码中，语言集成查询语言允许程序员使用一种语言来写他们的代码。Calcite提供了针对JAVA的语言集成查询语言（简称LINQ4J），它严格遵循了微软的LINQ对.NET语言的约定。</p>
<h2 id="8-工业和学术界应用"><a href="#8-工业和学术界应用" class="headerlink" title="8. 工业和学术界应用"></a>8. 工业和学术界应用</h2><p>Caclite得到了广泛的应用，尤其是在工业界中使用的开源项目。由于Calcite提供了特定集成的灵活性，这些项目选择在它们的core中内嵌Calcite（即作为library引入）或者实现的一个适配器来联邦查询处理。此外，我们看到研究界越来越有兴趣将Calcite作为开发数据管理项目的基石。下面，我们将介绍使用Calcite的不同项目。</p>
<h3 id="8-1-内嵌Calcite"><a href="#8-1-内嵌Calcite" class="headerlink" title="8.1 内嵌Calcite"></a>8.1 内嵌Calcite</h3><p>表1给出了使用Calcite的软件列表，包含以下几个维度：</p>
<ul>
<li>暴露给用户的查询接口；</li>
<li>是否使用Calcite的JDBC驱动（Avatica）；</li>
<li>是否使用Calcite中的SQL解析和验证；</li>
<li>是否使用Calcite的查询代数来表示在数据上的操作；</li>
<li>是否依赖Calcite引擎来执行，即使用自己的原生引擎还是Calcite算子（enumerable）或者其他项目。</li>
</ul>
<p><img src="/blog/10fa9651/embed_calcite_systems.png" alt></p>
<center>表1 内嵌Calcite的系统列表</center>

<p>Drill[13]是一个基于Dremel系统[34]的灵活数据处理引擎，内部使用无模式JSON数据模型。Drill使用它自己的SQL方言，包括半结构化数据查询表达的扩展，类似SQL++[38]。</p>
<p>Hive[24]第一次作为MapReduce编程模型之上的SQL接口而变得很流行。此后，它逐步转向成为一个交互式SQL查询引擎，采用Calcite作为它的基于规则和代价的优化器，不依赖Calcite的JDBC驱动、SQL解析和校验，采用自己的组件来实现。<strong>查询先翻译为Calcite的算子，经过优化之后，再翻译为Hive的物理代数</strong>。Hive算子能够在多个引擎上执行，主流的有Aapche Tez[43, 51]和Apache Spark[47, 56]。</p>
<p>Apache Solr[46]是一个主流的全文分布式检索平台，基于Apache Lucene库[31]构建。Solr给用户提供了多个查询接口，包扩类似Rest的HTTP/XML和JSON接口。而且，Solr集成Calcite来提供SQL兼容。</p>
<p>Apache Phoenix{40]和Apache Kylin[28]都运行在Apache HBase[23]之上，它是继Bigtable[9]之后的一个分布式KV存储。具体来说，Phoenix提供了一个SQL接口和编排层来查询HBase。<strong>Kylin聚焦于OLAP式查询，通过构建cubes来声明物化视图并存储在HBase中，因此可以通过Calcite的优化器来重写输入的查询，使用cubes来处理查询</strong>。<strong>在Kylin中，查询计划的执行，是结合了Calcite原生算子和HBase的能力</strong>。</p>
<p>近来，Calcite在流处理系统中也越来越流行。比如Apache Apex[1]，<strong>Flink</strong>[16]，Apache Samza[44]以及<strong>Storm</strong>[50]，这些项目选择和Calcite集成，使用它的组件来提供流SQL接口给用户。最后，其他商业系统也有采用Calcite的，比如MapD[32]，Lingual[30]和Qubole Quark[42]。</p>
<h3 id="8-2-Calcite适配"><a href="#8-2-Calcite适配" class="headerlink" title="8.2 Calcite适配"></a>8.2 Calcite适配</h3><p>除了将Calcite作为库来使用，其他系统也可以通过适配器方式集成Calcite，读取他们的数据源。表2给出了Calcite中的适配器列表。实现这些适配器最主要的组件就是<em>converter</em>，负责翻译关系代数表达式，推出给系统支持的查询语言。表中还给出了每个适配器翻译的语言。</p>
<p>JDBC适配器支持多个SQL方言，包括主流的RDBMS，比如PostgreSQL和MySQL。另外，Cassandra[8]适配器生成自己的类SQL语言，叫CQL。而Apache Pig[41]适配器生成的查询使用Pig Latin[37]来表达。Apache Spark[47]的适配器使用JAVA RDD API。最后，Druid[14], ElasticSearch[15]和Splunk[48]通过Rest HTTP API请求查询，通过JSON或XML来表达查询。</p>
<h3 id="8-3-研究使用"><a href="#8-3-研究使用" class="headerlink" title="8.3 研究使用"></a>8.3 研究使用</h3><p>在研究环境中，Calcite已经作为精确医学和临床分析场景的多存储替代方案。此处省略…</p>
<h2 id="9-未来工作"><a href="#9-未来工作" class="headerlink" title="9. 未来工作"></a>9. 未来工作</h2><p>Calcite的未来工作聚焦于新特性的开发和适配器架构的扩展：</p>
<ul>
<li>强化Calcite设计，支持作为一个独立引擎来使用，要求<strong>支持DDL，物化视图、索引和约束</strong>；</li>
<li><strong>继续改进planner的设计和灵活性，包括使它更加模块化，允许用户通过Calcite为执行器提供planner程序（规则集合或组织到各个计划阶段）</strong>；</li>
<li><strong>将新的参数[53]纳入优化器设计</strong>；</li>
<li>支持SQL命令、函数和工具的扩展，<strong>包括OpenGIS的完全兼容</strong>；</li>
<li>针对非关系数据源的新适配器，比如用于科学计算的阵列数据库；</li>
<li><strong>改进性能分析和检测</strong>。</li>
</ul>
<h3 id="9-1-性能测试与评估"><a href="#9-1-性能测试与评估" class="headerlink" title="9.1 性能测试与评估"></a>9.1 性能测试与评估</h3><p>虽然Calcite包含了一个性能测试模块，但是它没有评估查询执行。在评估基于Calcite构建的系统性能时将会有用的。例如，我们比较和Calcite类似的框架。不幸地是，做出公平的比较是比较困难的。例如，像Calcite，Algebricks优化了Hive的查询。Borkar等人[6]将Algebricks和Hyracks调度器跟Hive 0.12进行了比较（无Calcite）。Borkar等人的工作，要在Hive在重要工程和架构变化之前。在时间方面，一种公平的方式来比较Calcite和Algebricks似乎不太可行，因为要确保每个都使用相同的执行引擎。Hive应用主要依赖Apache Tez或Apache Spark作为执行引擎，而Algebricks则依赖于它自己的框架（包括hyracks）。</p>
<p>此外，为了评估基于Calcite的系统性能，我们需要考虑两个不同的使用场景。Calcite可以被用于作为单一系统的一部分或作为一个工具来加速一个系统的构建，甚至是作为一个公共层，整合多个不同系统的更加困难的任务。前者是跟数据处理系统的特点有关，因为Calcite功能多样和使用广泛，需要许多不同的基准。后者受已有异构基准可用性的限制。BigDAWG[55]被用于集成PostgreSQL和Vertica，在标准基准上，有人认为在处理查询时，集成系统是优于将整个表从一个系统拷贝到另一个系统的。基于实际经验，我们相信，更大的目标是集成多个系统，将优于每个系统部分的和。</p>
<h2 id="10-总结"><a href="#10-总结" class="headerlink" title="10. 总结"></a>10. 总结</h2><p>新兴的数据管理实践和关联分析使用的数据，正在朝着越来越多样化和异构场景方向发展。与此同时，通过SQL访问的关系数据源，保留了企业处理数据的本质方式。在这个有点分叉的空间里，Calcite起到了一个独特的作用，来支持传统数据处理和其他包括半结构化、流和地理空间模型的数据源。而且，Calcite聚焦于灵活性、适应性和可扩展性的设计理念，已经成为Calcite在大量开源框架中使用最广泛的查询优化器的另一个因素。Calcite的动态和灵活的查询优化器，以及适配器架构使得嵌入到大量数据处理框架中成为可能，比如Hive，Drill，MapD和Flink。Calcite对异构数据处理的支持，以及关系函数扩展，在功能和性能上都在持续改进。</p>
<h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h1><p>我们感谢Calcite社区，贡献者和用户，是他们构建、维护、使用、测试、写作和持续推动社区项目向前发展。本手稿部分由UT Battelle，LLC根据与美国能源部签订的合同（合同编号：DE-AC05-00OR22725）共同撰写。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] Apex. Apace Apex. <a href="https://apex.apache.org" target="_blank" rel="noopener">https://apex.apache.org</a>. (Nov. 2017).</p>
<p>[2] Arvind Arasu, Shivnath Babu, and Jennifer Widom. 2003. <em>The CQL Continuous</em> <em>Query Language: Semantic Foundations and Query Execution</em>. Technical Report 2003-67. Stanford InfoLab.</p>
<p>[3] Michael Armbrust et al. 2015. Spark SQL: Relational Data Processing in Spark. In <em>Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data (SIGMOD ’15)</em>. ACM, New York, NY, USA, 1383–1394.</p>
<p>[4] Michael Armbrust, Reynold S. Xin, Cheng Lian, Yin Huai, Davies Liu, Joseph K. Bradley, Xiangrui Meng, Tomer Kaftan, Michael J. Franklin, Ali Ghodsi, and Matei Zaharia. 2015. Spark SQL: Relational Data Processing in Spark. In <em>Proceedings of</em> <em>the 2015 ACM SIGMOD International Conference on Management of Data (SIGMOD</em> <em>’15)</em>. ACM, New York, NY, USA, 1383–1394.</p>
<p>[5] ASF. The Apache Software Foundation. (Nov. 2017). Retrieved November 20, 2017 from <a href="http://www.apache.org/" target="_blank" rel="noopener">http://www.apache.org/</a></p>
<p>[6] Vinayak Borkar, Yingyi Bu, E. Preston Carman, Jr., Nicola Onose, Till Westmann, Pouria Pirzadeh, Michael J. Carey, and Vassilis J. Tsotras. 2015. Algebricks: A Data Model-agnostic Compiler Backend for Big Data Languages. In <em>Proceedings of the Sixth ACM Symposium on Cloud Computing (SoCC ’15)</em>. ACM, New York, NY, USA, 422–433.</p>
<p>[7] M. J. Carey et al. 1995. Towards heterogeneous multimedia information systems: the Garlic approach. In <em>IDE-DOM ’95</em>. 124–131.</p>
<p>[8] Cassandra. Apache Cassandra. (Nov. 2017). Retrieved November 20, 2017 from <a href="http://cassandra.apache.org/" target="_blank" rel="noopener">http://cassandra.apache.org/</a></p>
<p>[9] Fay Chang, Jeffrey Dean, Sanjay Ghemawat, Wilson C. Hsieh, Deborah A. Wallach, Michael Burrows, Tushar Chandra, Andrew Fikes, and Robert Gruber. 2006. Bigtable: A Distributed Storage System for Structured Data. In <em>7th Symposium on</em> <em>Operating Systems Design and Implementation (OSDI ’06), November 6-8, Seattle,</em> <em>WA, USA</em>. 205–218.</p>
<p>[10] Surajit Chaudhuri, Ravi Krishnamurthy, Spyros Potamianos, and Kyuseok Shim.1995. <strong>Optimizing Queries with Materialized Views</strong>. In <em>Proceedings of the Eleventh</em> <em>International Conference on Data Engineering (ICDE ’95)</em>. IEEE Computer Society, Washington, DC, USA, 190–200.</p>
<p>[11] E. F. Codd. 1970. A Relational Model of Data for Large Shared Data Banks. <em>Commun. ACM</em> 13, 6 (June 1970), 377–387.</p>
<p>[12] Alex Şuhan. <strong>Fast and Flexible Query Analysis at MapD with Apache Calcite</strong>. (feb 2017). Retrieved November 20, 2017 from <a href="https://www.mapd.com/blog/2017/02/08/fast-and-flexible-query-analysis-at-mapd-with-apache-calcite-2/" target="_blank" rel="noopener">https://www.mapd.com/blog/2017/02/08/fast-and-flexible-query-analysis-at-mapd-with-apache-calcite-2/</a></p>
<p>[13] Drill. Apache Drill. (Nov. 2017). Retrieved November 20, 2017 from <a href="http://drill.apache.org/" target="_blank" rel="noopener">http://drill.apache.org/</a></p>
<p>[14] Druid. Druid. (Nov. 2017). Retrieved November 20, 2017 from <a href="http://druid.io/" target="_blank" rel="noopener">http://druid.io/</a></p>
<p>[15] Elastic. Elasticsearch. (Nov. 2017). Retrieved November 20, 2017 from <a href="https://www.elastic.co" target="_blank" rel="noopener">https://www.elastic.co</a></p>
<p>[16] Flink. <strong>Apache Flink</strong>. <a href="https://flink.apache.org" target="_blank" rel="noopener">https://flink.apache.org</a>. (Nov. 2017).</p>
<p>[17] Yupeng Fu, Kian Win Ong, Yannis Papakonstantinou, and Michalis Petropoulos. 2011. The SQL-based all-declarative FORWARD web application development framework. In <em>CIDR</em>.</p>
<p>[18] Jonathan Goldstein and Per-Åke Larson. 2001. <strong>Optimizing Queries Using Materialized Views: A Practical, Scalable Solution.</strong> <em>SIGMOD Rec.</em> 30, 2 (May 2001), 331–342.</p>
<p>[19] Goetz Graefe. 1995. The Cascades Framework for Query Optimization. <em>IEEE</em> <em>Data Eng. Bull.</em> (1995).</p>
<p>[20] Goetz Graefe and William J. McKenna. 1993. <strong>The Volcano Optimizer Generator: Extensibility and Efficient Search</strong>. In <em>Proceedings of the Ninth International</em> <em>Conference on Data Engineering</em>. IEEE Computer Society, Washington, DC, USA, 209–218.</p>
<p>[21] Daniel Halperin, Victor Teixeira de Almeida, Lee Lee Choo, Shumo Chu, Paraschos Koutris, Dominik Moritz, Jennifer Ortiz, Vaspol Ruamviboonsuk, Jingjing Wang, Andrew Whitaker, Shengliang Xu, Magdalena Balazinska, Bill Howe, and Dan Suciu. 2014. Demonstration of the Myria Big Data Management Service. In <em>Proceedings of the 2014 ACM SIGMOD International Conference on</em> <em>Management of Data (SIGMOD ’14)</em>. ACM, New York, NY, USA, 881–884.</p>
<p>[22] Venky Harinarayan, Anand Rajaraman, and Jeffrey D. Ullman. 1996. Implementing Data Cubes Efficiently. <em>SIGMOD Rec.</em> 25, 2 (June 1996), 205–216.</p>
<p>[23] HBase. Apache HBase. (Nov. 2017). Retrieved November 20, 2017 from <a href="http://hbase.apache.org/" target="_blank" rel="noopener">http://hbase.apache.org/</a></p>
<p>[24] Hive. Apache Hive. (Nov. 2017). Retrieved November 20, 2017 from <a href="http://hive.apache.org/" target="_blank" rel="noopener">http://hive.apache.org/</a></p>
<p>[25] Yin Huai, Ashutosh Chauhan, Alan Gates, Gunther Hagleitner, Eric N. Hanson, Owen O’Malley, Jitendra Pandey, Yuan Yuan, Rubao Lee, and Xiaodong Zhang. 2014. Major Technical Advancements in Apache Hive. In <em>Proceedings of the 2014</em> <em>ACM SIGMOD  International Conference on Management of Data (SIGMOD ’14)</em>. ACM, New York, NY, USA, 1235–1246.</p>
<p>[26] <strong>Julian Hyde</strong>. 2010. Data in Flight. <em>Commun. ACM</em> 53, 1 (Jan. 2010), 48–52.</p>
<p>[27] Janino. <strong>Janino: A super-small, super-fast Java compiler</strong>. (Nov. 2017). Retrieved November 20, 2017 from <a href="http://www.janino.net/" target="_blank" rel="noopener">http://www.janino.net/</a></p>
<p>[28] Kylin. Apache Kylin. (Nov. 2017). Retrieved November 20, 2017 from <a href="http://kylin.apache.org/" target="_blank" rel="noopener">http://kylin.apache.org/</a></p>
<p>[29] Avinash Lakshman and Prashant Malik. 2010. Cassandra: A Decentralized Structured Storage System. <em>SIGOPS Oper. Syst. Rev.</em> 44, 2 (April 2010), 35–40.</p>
<p>[30] Lingual. Lingual. (Nov. 2017). Retrieved November 20, 2017 from <a href="http://www.cascading.org/projects/lingual/" target="_blank" rel="noopener">http://www.cascading.org/projects/lingual/</a></p>
<p>[31] Lucene. Apache Lucene. (Nov. 2017). Retrieved November 20, 2017 from <a href="https://lucene.apache.org/" target="_blank" rel="noopener">https://lucene.apache.org/</a></p>
<p>[32] MapD. MapD. (Nov. 2017). Retrieved November 20, 2017 from <a href="https://www.mapd.com" target="_blank" rel="noopener">https://www.mapd.com</a></p>
<p>[33] Erik Meijer, Brian Beckman, and Gavin Bierman. 2006. LINQ: Reconciling Object, Relations and XML in the .NET Framework. In <em>Proceedings of the 2006 ACM</em> SIGMOD International Conference on Management of Data (SIGMOD ’06)*. ACM, New York, NY, USA, 706–706.</p>
<p>[34] Sergey Melnik, Andrey Gubarev, Jing Jing Long, Geoffrey Romer, Shiva Shivakumar, Matt Tolton, and Theo Vassilakis. 2010. <strong>Dremel: Interactive Analysis of Web-Scale Datasets</strong>. <em>PVLDB</em> 3, 1 (2010), 330–339. <a href="http://www.comp.nus.edu.sg/~vldb2010/proceedings/files/papers/R29.pdf" target="_blank" rel="noopener">http://www.comp.nus.edu.sg/~vldb2010/proceedings/files/papers/R29.pdf</a></p>
<p>[35] Marcelo RN Mendes, Pedro Bizarro, and Paulo Marques. 2009. A performance study of event processing systems. In <em>Technology Conference on Performance</em> <em>Evaluation and Benchmarking</em>. Springer, 221–236.</p>
<p>[36] Mongo. MongoDB. (Nov. 2017). Retrieved November 28, 2017 from <a href="https://www.mongodb.com/" target="_blank" rel="noopener">https://www.mongodb.com/</a></p>
<p>[37] Christopher Olston, Benjamin Reed, Utkarsh Srivastava, Ravi Kumar, and Andrew Tomkins. 2008. Pig Latin: a not-so-foreign language for data processing. In <em>SIGMOD</em>.</p>
<p>[38] Kian Win Ong, Yannis Papakonstantinou, and Romain Vernoux. 2014. The SQL++ query language: Configurable, unifying and semi-structured. <em>arXiv preprint</em> <em>arXiv:1405.3631</em> (2014)</p>
<p>[39] Open Geospatial Consortium. OpenGIS Implementation Specification for Geographic information - Simple feature access - Part 2: SQL option. <a href="http://portal.opengeospatial.org/files/?artifact_id=25355" target="_blank" rel="noopener">http://portal.opengeospatial.org/files/?artifact_id=25355</a>. (2010).</p>
<p>[40] Phoenix. Apache Phoenix. (Nov. 2017). Retrieved November 20, 2017 from <a href="http://phoenix.apache.org/" target="_blank" rel="noopener">http://phoenix.apache.org/</a></p>
<p>[41] Pig. Apache Pig. (Nov. 2017). Retrieved November 20, 2017 from <a href="http://pig.apache.org/" target="_blank" rel="noopener">http://pig.apache.org/</a></p>
<p>[42] Qubole Quark. Qubole Quark. (Nov. 2017). Retrieved November 20, 2017 from <a href="https://github.com/qubole/quark" target="_blank" rel="noopener">https://github.com/qubole/quark</a></p>
<p>[43] Bikas Saha, Hitesh Shah, Siddharth Seth, Gopal Vijayaraghavan, Arun C. Murthy, and Carlo Curino. 2015. Apache Tez: A Unifying Framework for Modeling and Building Data Processing Applications. In <em>Proceedings of the 2015 ACM SIGMOD</em> <em>International Conference on Management of Data, Melbourne, Victoria, Australia,</em> <em>May 31 - June 4, 2015</em>. 1357–1369. <a href="https://doi.org/10.1145/2723372.2742790" target="_blank" rel="noopener">https://doi.org/10.1145/2723372.2742790</a></p>
<p>[44] Samza. Apache Samza. (Nov. 2017). Retrieved November 20, 2017 from <a href="http://samza.apache.org/" target="_blank" rel="noopener">http://samza.apache.org/</a></p>
<p>[45] Mohamed A. Soliman, Lyublena Antova, Venkatesh Raghavan, Amr El-Helw, Zhongxian Gu, Entong Shen, George C. Caragea, Carlos Garcia-Alvarado, Foyzur Rahman, Michalis Petropoulos, Florian Waas, Sivaramakrishnan Narayanan, Konstantinos Krikellas, and Rhonda Baldwin. 2014. <strong>Orca: A Modular Query Optimizer Architecture for Big Data</strong>. In <em>Proceedings of the 2014 ACM SIGMOD</em> <em>International Conference on Management of Data (SIGMOD ’14)</em>. ACM, New York, NY, USA, 337–348.</p>
<p>[46] Solr. Apache Solr. (Nov. 2017). Retrieved November 20, 2017 from <a href="http://lucene.apache.org/solr/" target="_blank" rel="noopener">http://lucene.apache.org/solr/</a></p>
<p>[47] Spark. <strong>Apache Spark</strong>. (Nov. 2017). Retrieved November 20, 2017 from <a href="http://spark.apache.org/" target="_blank" rel="noopener">http://spark.apache.org/</a></p>
<p>[48] Splunk. Splunk. (Nov. 2017). Retrieved November 20, 2017 from <a href="https://www.splunk.com/" target="_blank" rel="noopener">https://www.splunk.com/</a></p>
<p>[49] Michael Stonebraker and Ugur Çetintemel. 2005. “One size fits all”: an idea whose time has come and gone. In <em>21st International Conference on Data Engineering</em> <em>(ICDE’05)</em>. IEEE Computer Society, Washington, DC, USA, 2–11.</p>
<p>[50] Storm. Apache Storm. (Nov. 2017). Retrieved November 20, 2017 from <a href="http://storm.apache.org/" target="_blank" rel="noopener">http://storm.apache.org/</a></p>
<p>[51] Tez. Apache Tez. (Nov. 2017). Retrieved November 20, 2017 from <a href="http://tez.apache.org/" target="_blank" rel="noopener">http://tez.apache.org/</a></p>
<p>[52] Ashish Thusoo, Joydeep Sen Sarma, Namit Jain, Zheng Shao, Prasad Chakka, Suresh Anthony, Hao Liu, Pete Wyckoff, and Raghotham Murthy. 2009. Hive: a warehousing solution over a map-reduce framework. <em>VLDB</em> (2009), 1626–1629.</p>
<p>[53] Immanuel Trummer and Christoph Koch. 2017. Multi-objective parametric query optimization. <em>The VLDB Journal</em> 26, 1 (2017), 107–124.</p>
<p>[54] Ashwin Kumar Vajantri, Kunwar Deep Singh Toor, and Edmon Begoli. 2017. <strong>An Apache Calcite-based Polystore Variation for Federated Querying of Heterogeneous Healthcare Sources</strong>. In <em>2nd Workshop on Methods to Manage Heterogeneous</em> <em>Big Data and Polystore Databases</em>. IEEE Computer Society, Washington, DC, USA.</p>
<p>[55] Katherine Yu, Vijay Gadepally, and Michael Stonebraker. 2017. Database engine integration and performance analysis of the BigDAWG polystore system. In <em>2017</em> <em>IEEE High Performance Extreme Computing Conference (HPEC)</em>. IEEE Computer Society, Washington, DC, USA, 1–7.</p>
<p>[56] Matei Zaharia, Mosharaf Chowdhury, Michael J. Franklin, Scott Shenker, and Ion Stoica. 2010. Spark: Cluster Computing with Working Sets. In <em>HotCloud</em>.</p>
<p>[57] Jingren Zhou, Per-Åke Larson, and Ronnie Chaiken. 2010. <strong>Incorporating partitioning and parallel plans into the SCOPE optimizer.</strong> In <em>2010 IEEE 26th International</em> <em>Conference on Data Engineering (ICDE 2010)</em>. IEEE Computer Society, Washington, DC, USA, 1060–1071.</p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>Calcite</tag>
        <tag>优化器</tag>
      </tags>
  </entry>
  <entry>
    <title>工程师如何从技术转型做管理</title>
    <url>/blog/188052f8.html</url>
    <content><![CDATA[<p>转载自 微信公众号 | IT人的职场进阶 （前亚马逊工程师，现58转转技术总监）</p>
<p><strong>“我，程序员，32岁，距离退休，只剩3年了！”</strong></p>
<p>这句话用来形容2019年互联网行业最适合不过了。从18年开始，大大小小的互联网公司开始了不止一轮的裁员，19年网上开始充斥一类文章，专门写互联网公司超过35岁的人，如果到这个年龄，还不是leader，业务又不核心，那么请焦虑吧。</p>
<p>昨天听罗胖的跨年演讲，主题是：基本盘。意思是不要受到人云亦云的情绪影响，而是转过头，看手中的资源，基于基本盘看清自己的努力方向，非常感慨和受启发。中国互联网经过过去十多年野蛮式的发展似乎这2年开始慢下来了，程序员35岁的退休年龄虽然只是贩卖焦虑的一种说法，但是整个行业对人的要求越来越高是不争的事实，要求我们的成长速度必须跟上。<strong>2020年开始，希望自己在技术、管理、业务3个维度再做更深层次的学习，体系化个人的认知，做一个有特点的IT人</strong>。</p>
<p>下面要写的主题是关于『工程师如何从技术转型做管理』，这是我在团队管理上第一篇系统性的总结。之所以选择这个主题，一方面，个人觉得转型做管理是当前环境下大部分程序员会选择的职业路径，另一方面，自己亲身经历了比较漫长的转型过程，应该能写出点心得体会。希望下面的内容对于『正在转型挣扎期』或者『后续有规划往管理转型』的同学，让你们有所启发，内容大概分成以下4个部分：</p>
<ul>
<li>什么样的工程师会被提拔做管理？</li>
<li>你选择做管理的初衷是什么？</li>
<li>转型期你会遇到哪些困惑或者挑战？</li>
<li>转型期应该具备哪些心智？</li>
</ul>
<h3 id="1-什么样的工程师会被提拔做管理？"><a href="#1-什么样的工程师会被提拔做管理？" class="headerlink" title="1.  什么样的工程师会被提拔做管理？"></a>1.  什么样的工程师会被提拔做管理？</h3><p>一般来说，满足这3个条件的工程师会被提拔做管理：<strong>技术能力强、业务熟练、软性素质达标</strong>。（当然还要看公司是否有管理岗位的空缺以及你个人的意愿），下面分别展开说下重点。</p>
<p>技术方面：<strong>常用技术的深度和宽度缺一不可，架构能力非常关键</strong>。否则技术方向都把握不好，技术决策也容易出问题。如果技术能力没达到一定水平，不建议太早转管理（个人感觉能力至少要接近阿里的P7，腾讯的T3-1，百度的T6）。</p>
<p>业务方面：不了解业务，技术没法落地，<strong>不仅要求熟悉业务而且应该具备比较强的业务意识</strong>，（如果能从技术维度提出好想法，帮助业务拿到更好的结果，这种leader是非常受欢迎的）。</p>
<p>软性素质达标：软性素质这个词有些泛，我个人觉得最核心的两点，<strong>沟通协调能力和做事靠不靠谱</strong>。软性都是可以锻炼的，但是一定要有意识去提升。著名管理学家陈春花老师说，“一个人被组织提拔，其实不是因为能力，而是因为信任”，聪明的人很多，但是靠谱的人很少，比能力更重要的是工作的投入感和靠谱的态度。</p>
<p>如果你觉得上述3个方面都达到要求了，我觉得只是差一个机会，否则好好提升自己吧。</p>
<h3 id="2-你选择做管理的初衷是什么？"><a href="#2-你选择做管理的初衷是什么？" class="headerlink" title="2. 你选择做管理的初衷是什么？"></a>2. 你选择做管理的初衷是什么？</h3><p>之前有人问过我一个问题，“你觉得我适合做管理吗？能给我些建议吗？”，我当时没有正面回答他，而是反过来问他，“你能先告诉我，做管理对你意味着什么？它能给你带来什么呢？”。当然我不是在质疑他，而是想让他反思他做管理的初衷。我觉得『最原始的动机』会决定你在管理路上能扛多大的压力以及能走多远。关于初衷，我见过最普遍的说法有这么几种：</p>
<ul>
<li>技术不能做一辈子，很多前辈在能力达到一定水平后都转管理了，自己也这么想</li>
<li>在技术路线上遇到了晋升瓶颈，想尝试下管理方向，看自己是否合适</li>
<li>公司发展太快了，老板让我带团队，自己也没办法</li>
<li>管理者工资高，在别人眼中是优秀的代表</li>
<li>指挥做事即可，可以脱离执行层面，越往上走越轻松</li>
</ul>
<p>上面这几类都属于『外部因素』驱动，说实话，都很难在管理路上走得很远。因为技术管理是极其复杂和琐碎的工作，它远没有你想象中的轻松和风光，而在这些外力下，你做出决策后的结果很多时候跟你的预期是不一致的，这个时候你的怨气和转型痛苦就会出现，你开始质疑你选择的这条路是不是错了？</p>
<p>再来看另外一个问题，作为技术管理者，对于公司、团队以及你个人，你觉得它的价值分别是什么？我个人的解读是这样的：</p>
<ul>
<li><strong>对于公司：能带领技术团队支撑好业务，帮助业务实现公司定的战略目标。</strong></li>
<li><strong>对于团队：规划好方向，别让组员瞎忙，同时能帮助他们成长。</strong></li>
<li><strong>对于个人：提升自身的技术和管理能力。</strong></li>
</ul>
<p>这是对于技术管理岗位的基本认知，你的初衷必须建立在这个认知基础之上。然后试问你自己：是否认可这个岗位的价值？如果你觉得全是牺牲自己来成就公司和团队，那你不可能做得开心，也不可能做好。</p>
<p>第2个问题，你是否对管理者的工作充满热情？并且享受这个过程呢？比如项目协调，比如制定流程并推动落地执行，比如招聘。如果你说我只喜欢做技术相关的工作（比如架构设计、技术评审等），那么你还是走技术路线吧。</p>
<p>认可技术管理岗位的价值所在，并且能激发你的投入意愿。这些就是底层最好的动力，你的成长和回报都是付出后水到渠成的东西。所以这个初衷很重要，<strong>三观一定要正</strong>。</p>
<h3 id="3-转型期你会遇到哪些困惑或者挑战？"><a href="#3-转型期你会遇到哪些困惑或者挑战？" class="headerlink" title="3. 转型期你会遇到哪些困惑或者挑战？"></a>3. 转型期你会遇到哪些困惑或者挑战？</h3><p>转型期会经历心态、工作方式的转变，很多事情会刷新你的认知。下面几点，我认为是绝大部分人在转型过程中会遇到的困惑或者挑战：</p>
<ul>
<li><strong>时间不够用</strong>：成为团队leader后有很多日常事务要处理，要参加各种会议，有时候还需要分出一部分精力在一线coding上，时间完全被碎片化，根本不够用。</li>
<li><strong>嫌组员效率低</strong>：一个你认为简单的需求或者技术问题，交给团队成员后，他们的处理时间远超出你的预期，当外界施压时，你忍不住抱怨和责怪，并开始自己动手处理，久而久之，习惯自己冲在一线，觉得这样效率最高。</li>
<li><strong>恨人际关系复杂</strong>：对内对外、对上对下，每天需要和不同职位、不同level的人打交道，有靠谱的，有不靠谱的，某些你认为很简单的事情推动起来却很难，感觉情商不够用。</li>
<li><strong>成就感不强</strong>：偶尔会收到上级、平级、甚至下级的负面反馈，你开始质疑自己的管理能力，不像做工程师那样经常被认可，落差感强。</li>
<li><strong>不敢放弃一线</strong>：担心自己不合适做管理，如果脱离一线执行，感觉技术能力会停滞不前。不放弃一线，精力又跟不上，这个度把握不好。</li>
</ul>
<p>上述疑惑是我个人转型过程中体会最深的几点，我在后文中会分别给出自己的看法和建议。</p>
<h3 id="4-转型期应该具备哪些心智？"><a href="#4-转型期应该具备哪些心智？" class="headerlink" title="4. 转型期应该具备哪些心智？"></a>4. 转型期应该具备哪些心智？</h3><p>从技术转型做管理，更多的不是能力的变化，而是思维方式和行为的改变。很多刚转型的leader管理做不好，绝大部分不是因为能力不行，而是出现在了认知上。以下几点，我认为是转型期leader一定要具备的心智：</p>
<ul>
<li>学会从团队的角度考虑问题</li>
<li>注重执行细节</li>
<li>学会用人所长，具备包容心</li>
<li>重视情商，做好自我情绪控制</li>
<li>做好时间管理</li>
</ul>
<h4 id="学会从团队角度考虑问题"><a href="#学会从团队角度考虑问题" class="headerlink" title="学会从团队角度考虑问题"></a>学会从团队角度考虑问题</h4><p>以前作为工程师，更多是从事情本身或者从个人角度出发，成为leader后，转变成团队思维是最最重要的，因为你的KPI取决于你整个团队的完成情况，你要权衡的是团队整体的利益和效能。</p>
<table>
<thead>
<tr>
<th>员工思维</th>
<th>leader思维</th>
</tr>
</thead>
<tbody>
<tr>
<td>别人不会自己上</td>
<td>教会其他人</td>
</tr>
<tr>
<td>出问题责怪、惩罚、抱怨</td>
<td>事前做好提醒、跟进、支持；事后做好复盘和case分享</td>
</tr>
<tr>
<td>技术决策主要考虑自己不会、有没有技术提升</td>
<td>技术决策通盘考虑对团队整体的技术提升、复用性、业务影响等</td>
</tr>
<tr>
<td>归功于自己</td>
<td>弱化自己，归功于团队中表现优秀的人</td>
</tr>
</tbody>
</table>
<p><strong>上面4项对比，是我个人认为比较典型的case，比如上一节提到的一种情况：leader觉得某个问题很简单，嫌员工处理效率低，然后自己跳出来三下五除二给解决了，这种就属于很典型的员工思维。单从搞定这件事情来看，这也许是很好的处理方式，业务方也会很满意，但是带团队是长远的事情，上述做法紧急情况可行，但是变成常态就是非常大的问题。</strong></p>
<p><strong>团队能力不提高，leader永远不会解放，这是作为leader应该具备的意识。如果通过这个问题能够提升组员某方面的能力，leader应该扮演好教练的角色，放手让组员自己去做，你要做的仅仅是观察、给一些指点、适当给予时间上的支持。这次处理也许效率不高，但是下次碰到类似的问题，团队是不需要依靠你来解决的，另外组员也有自己的发挥空间，觉得团队在帮助他成长。</strong></p>
<h4 id="注重执行细节"><a href="#注重执行细节" class="headerlink" title="注重执行细节"></a>注重执行细节</h4><p>对于刚转型做管理的一线leader，切忌被放权式的管理方式洗脑。放权式管理对于对管理者的经验要求很高，它比较适用于工作流程清晰，团队骨干目标认知以及自驱力很强的团队。</p>
<p>当你个人的管理水平还处于菜鸟期时，一定要从细节抓起，通过手把手带员工，教会他们如何正确的做事，怎么才能达到你的要求，以及如何培养出团队骨干，搭建出团队的核心组织架构，所有这些都经历过了，你在管理上才会有自己的心得体会，才会走得更扎实。</p>
<p>通过观察执行细节，你能非常清楚团队每个人的优劣势，深入感受自己的管理方式是否存在问题，然后再辅以leader思维去思考和解决问题，管理上才能真正获得成长。这个过程，你可能会收到上级、平级、下级的很多反馈，清楚细节后其实你就有了自己的判断，知道是否是自身的问题，是否要调整，而不是沮丧抓瞎。</p>
<h4 id="学会用人所长，具备包容心"><a href="#学会用人所长，具备包容心" class="headerlink" title="学会用人所长，具备包容心"></a>学会用人所长，具备包容心</h4><p>知人善任、人尽其才，是每个管理者都懂的道理，但是能做到的不多。尤其在技术管理岗上，我见过有些leader在技术上非常强势，技术权威不容有任何挑战，当组员提出更合理的技术方案时，他会用职级强制要求按自己说的执行，根本不做任何解释。</p>
<p>对于新晋leader，团队对你的信任感还在磨合期，上述做法很容易打击组员的积极性，消灭他们的创造力，这对你带团队来说是非常致命的。如果组员的方案更合理，leader应该倍感欣慰，包容并鼓励这种行为，因为组员某方面的专业能力超过你了，你不再是团队各方面最强的人，你需要做的是调整自己的心智，学会用人所长。另外，还有一种情况是：组员和leader的技术方案都可行，我个人倾向将选择权交给组员，毕竟他们是真正的执行者，应该给他们自由发挥的空间，最后就算出问题对他们来说也是很好的经验积累。</p>
<h4 id="重视情商，做好自我情绪控制"><a href="#重视情商，做好自我情绪控制" class="headerlink" title="重视情商，做好自我情绪控制"></a>重视情商，做好自我情绪控制</h4><p>管理上能做多大事情，真的和情商有非常大的关系。IT界的技术人员由于工作性质的原因，普遍注重技术上的提升，而忽略情商的培养和维护，作为新晋leader必须从一开始就意识到情商的重要性。管理是一个复合型的岗位，当你的专业技能和处理问题的方法论已经形成后，越往上发展，为人处事的软技能占比会越来越重。</p>
<p>每天和不同的人打交道，这个是管理者的日常工作，因为你需要调动所有可能的资源去解决团队的困难。面对不同职位、不同level、不同性格的人，你要反复琢磨采取何种沟通方式和沟通技巧。上一节提到一种情况：一件你认为很简单的事情，推动起来却很困难。可能是因为你对外的沟通方式太生硬，别人不想配合你，或者别人确实有其他更重要的事情，但是如果私下关系建立好，你再当面软磨硬泡，多半也是可以解决的。人际关系上，难免会有碰壁的时候，不要气馁，这跟技术同学写出1个bug一样，是家常便饭的事情，但是一定要注意积累经验。线下和关键的配合方维护好私人关系，多吃饭喝酒，别人有困难能及时伸出援手等等，套路有很多。</p>
<p>情绪控制，是一个比较难的事情。情绪很容易传递，如果leader碰到不爽的事情，把组员当做出气筒，这是非常伤士气的，之前建立的信任感很容易消失，受不了的组员也可能就离职了。另外，对外沟通上，如果leader控制不好情绪，不将重点放在解决问题上，只是抱怨或者发火，也非常容易引起配合方的不满，认为你不专业，久而久之，你的团队也会被打上这种标签。</p>
<p>个人在情商方面目前做得也很差，踩过很多坑。提供3点建议：</p>
<ul>
<li>保持积极乐观的心态，同时提高自己面对问题时的承受能力，想清楚情绪化是解决不了问题的，只会加大解决问题的难度。</li>
<li>能够自我反省并吸收别人的反馈，做得不好的地方要勇于正视并且持续改进。</li>
<li>培养亲和力，不要觉得自己是leader就带着架子，要有一种鞠着的姿态，能够尊重人并且真诚待人。</li>
</ul>
<h4 id="做好时间管理"><a href="#做好时间管理" class="headerlink" title="做好时间管理"></a>做好时间管理</h4><p>时间管理的4象限理论可以百度一下。重点说下我个人遇到时间管理问题是怎么解决的，以及技术和管理两个维度如何分配时间。</p>
<p>第1步，可以拿过去一周或者一个月的时间跨度为例，详细列一下你的时间花在哪些具体事情上了，以及每类事情大概的时间占比。对于技术leader可能的事情包括：需求评审，资源规划和项目排期，技术评审，团队周例会，研发规范制定和落地，项目管理，技术调研，架构设计，coding，紧急任务协调和处理，业务以及新技术充电等等。</p>
<p>第2步，针对第一步列举的每类事情，考虑下哪些是非必须的，哪些是可以授权给团队骨干去做的，哪些是可以优化提高效率的。比如一些简单的需求评审或者技术方案评审让骨干把关即可，项目管理制定好流程规范同时培养一些scrum master或者项目经理下放给他们来做。不用凡事都事必躬亲，leader应该把时间聚焦在对团队最关键的事情上，学会授权和放权。</p>
<p>对于一线leader，技术和管理两个维度如何分配时间，个人的建议是：大部分时间leader是不需要亲自写代码的，但是如果有需要，leader要能够随时顶上，所以不能长期远离一线，纸上谈兵。长此以往，技术判断可能容易出现失误，而且如果管理不合适再转型回去代价太高。技术维度：可以将重点放在架构设计、代码审查、技术调研、以及一些框架性的代码开发上，这些事情对于维持技术优势是足够的。如果管理维度的时间占比超过60%，个人觉得比例是有些失衡的，要么团队太大了（比如超过了10人），要么自身的管理存在问题或者时间管理存在问题，需要关注并考虑做出调整。</p>
<p>上面这些内容，就是关于工程师转型管理的个人心得。关于管理，后续我会将更多实用的技巧以及方法论结合具体case进行总结和分。2020年，又一个十年的开端，认清基本盘，不忘初心，再接再厉！</p>
]]></content>
      <categories>
        <category>个人日志</category>
      </categories>
      <tags>
        <tag>技术管理</tag>
      </tags>
  </entry>
  <entry>
    <title>Drill性能识别和调优</title>
    <url>/blog/8c6d7818.html</url>
    <content><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>本文的内容来自<a href="https://drill.apache.org/docs/" target="_blank" rel="noopener">DRILL</a>的官方提供的性能识别和调优指南。</p>
<h2 id="查询计划和调整"><a href="#查询计划和调整" class="headerlink" title="查询计划和调整"></a>查询计划和调整</h2><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>DRILL可以修改很多配置来影响如何计划一个查询。这里将介绍一些配置的修改来提升性能。</p>
<h4 id="Join计划"><a href="#Join计划" class="headerlink" title="Join计划"></a>Join计划</h4><p>DRILL使用分布式和广播join来关联表，可以修改如下配置项来控制drill来生成join计划。</p>
<h5 id="分布式join"><a href="#分布式join" class="headerlink" title="分布式join"></a>分布式join</h5><p>针对分布式join，在join的两侧都是hash分布的，使用基于hash分布操作集的其中一个在join的key上。如果每个表中有多个join key，则drill将考虑以下两种类型的计划：</p>
<ul>
<li>在所有键上分布数据的计划；</li>
<li>在每个单独的键上分发数据的计划。</li>
</ul>
<p>对于merge join，drill在执行hash分布之后对join的两侧进行排序。drill可以分布hash join和merge join的两侧，但是nested loop join不可以。</p>
<h5 id="广播join"><a href="#广播join" class="headerlink" title="广播join"></a>广播join</h5><p>在广播join中，join之前所有选中的数据记录被广播到其他节点。join的内部被广播而外部保持不动没有分发。内部预估的基数必须小于<code>planner.broadcast_threshold</code>参数设定的值才有资格进行广播。drill针对hash join、merge join和nested loop join都可以使用broadcast join。</p>
<p>当一个大表关联一个小表时广播join是比较有用的。如果大表在分布式文件系统中保存了很多文件，不用基于网络重新分布大表数据，直接广播小表数据也许廉价得多。然而，广播会发送同样的数据到集群所有其他节点。取决于集群的大小和数据大小，在某些场景下也许并不是最有效的策略。</p>
<p>广播join的配置项</p>
<p>你可以采用ALTER SYSTEM或者ALTER SESSION的方式修改drill使用广播join的参数大小和系数。一般，在会话级别设置配置项，除非你想在所有session中生效。以下配置项可以控制广播join的行为：</p>
<ul>
<li><code>planner.broadcast_factor</code>： 当执行join时控制广播的成本。这个值越小，广播join相比于其他分布式join（如hash分布）成本更低。默认值为1，范围为0-1.7976931348623157e+308。</li>
<li><code>planner.enable_broadcast_join</code>：改变aggregation和join操作的状态。广播join可以用于hash join、merge join以及nested loop join。广播join用于大表关联小表。</li>
<li><code>planner.broadcast_threshold</code>：一个阀值，数据行数，决定了一个查询是否要使用广播join。不管广播join是否开启，只有join右侧预估的数据行数小于阀值才会选择广播join。这个配置项的目的是为了避免广播太多的数据，因为广播涉及跨节点发送数据和网络密集型操作。join的右侧可能是一个join或者一个简单的table，取决于物理计划期间的基于成本的优化和启发式方法。默认值为10000000，范围为0-2147483647。</li>
</ul>
<h4 id="Aggregation优化"><a href="#Aggregation优化" class="headerlink" title="Aggregation优化"></a>Aggregation优化</h4><p>针对包含GROUP BY的查询，drill执行聚合在1阶段或者2阶段。在这两种计划中，drill可以使用hash join或streaming join的物理操作。drill中默认操作是执行2阶段聚合。</p>
<p>在2阶段聚合方式中，每个minor fragment在1阶段执行本地或部分聚合，它将部分聚合的结果发送到其他fragments，通过基于hash的分布操作。hash分布式是按照GROUP BY的key完成的。在第2阶段所有fragments使用来自阶段1的数据进行总体聚合。</p>
<p>当GROUP BY的keys数据有合理数量的重复值时，2阶段聚合的方式是比较有效的，因为分组可以减少发送到下游操作集的数据行数。然而，如果没有太多的减少，则最好使用1阶段聚合。</p>
<p>例如，假设GROUP BY x,y的查询，在输入的数据中{x, y}的组合值时唯一的（或近似唯一），这种情况执行GROUP数据行没有减少，使用1阶段聚合可以提升性能。</p>
<p>你可以使用ALTER SYSTEM或ALTER SESSION命令设置配置来控制drill的聚合：</p>
<ul>
<li><code>planner.enable_multiphase_agg</code>：默认值为true。</li>
</ul>
<h4 id="修改Query计划"><a href="#修改Query计划" class="headerlink" title="修改Query计划"></a>修改Query计划</h4><p>Planner配置项会影响drill如何计划一个查询。通过ALTER SYSTEM或ALTER SESSION命令设置，配置如下：</p>
<ul>
<li><p><code>planner.width.max_per_node</code></p>
<p>配置此选项，获取并行度的细粒度绝对控制。在这个上下文中，width指扇出(fan out)或分布（distribution）潜力，即在节点的core上和集群的节点上并行运行查询的能力。一个物理计划由中间操作（称为查询片段）组成，这些操作能够并发运行，在计划每个exchange操作之上和之下产生并行的机会。一个exchange操作代表了执行流中的一个断点，这个断点是可以分发处理的。例如，文件单线程扫描流向exchange操作，然后是多线程聚合片段。</p>
<p>每个节点的最大width定义了一个查询中任何片段的最大并行度，但是这个是作用在一个集群的单个节点上。默认每个节点的最大并行度计算方式如下：理论最大值自动缩回（和舍入），使得只有实际可用容量的70%被考虑：<code>活跃drillbit数（一般一个节点一个）* 每个节点核数 * 0.7</code>。</p>
<p>例如，一个单节点2核测试系统并开启超线程：<code>1 * 4 * 0.7 = 3</code>。当你修改这个默认值时，可以提供任何有意义的数值，此时系统将不会自动缩小你的设置。</p>
</li>
<li><p><code>planner.width_max_per_query</code></p>
<p>默认值是1000。一个查询跨所有节点并行运行的最大线程数。仅当drill在非常大的集群上并行时更改此设置。</p>
</li>
<li><p><code>planner.slice_target</code> </p>
<p>默认值是100000。在使用额外并行化之前，在major fragment中工作的最小预估记录数。</p>
</li>
<li><p><code>planner.broadcast_threshold</code></p>
<p>默认值是10000000。作为join的一部分，可以被广播的最大记录数。当达到这个阈值时，drill将采用reshuffles而不是广播。你可以增加这个值来提升性能（<strong>尤其在10GB以太网集群上</strong>）。</p>
</li>
</ul>
<h4 id="基于排序和基于hash的内存限制操作集"><a href="#基于排序和基于hash的内存限制操作集" class="headerlink" title="基于排序和基于hash的内存限制操作集"></a>基于排序和基于hash的内存限制操作集</h4><p>Drill支持以下内存密集型操作集，如果这些操作集耗尽内存，会将数据临时溢写到磁盘：</p>
<ul>
<li>External Sort</li>
<li>Hash Join (Semi Join，出现在IN或EXISTS中的子查询，用于outer_table过滤)</li>
<li>Hash Aggregate</li>
</ul>
<p>Drill仅仅使用External Sort算子来排序数据，使用Hash Aggregate算子来聚合数据。<strong>可替代方法</strong>，Drill可以对数据排序，然后使用（轻量）的Streaming Aggregate算子来聚合数据。</p>
<p>Drill使用Hash Join算子来关联数据，在1.15版本将Semi Join引入Hash Join算子来提升查询性能。Semi Join移除了Hash Join下的去重处理，并消除使用Hash Aggregate产生的开销。在1.15版本之前或者关闭Semi Join功能，Drill使用去重Hash Aggregate来实现Semi Join的功能。<strong>可替代方法</strong>，Drill使用Nested-Loop-Join或者对数据进行排序再使用（轻量）Merge-Join。Drill一般使用hash算子来进行关联和聚合，它们比排序算子有更好的性能（Hash和Sort的时间复杂度分别为O(N)和O(N * log(N))）。然而，如果你关闭Hash算子或者数据已经排好序，Drill将使用前面描述的替代方法。</p>
<p>Drill中的内存配置可以为每个查询、每个节点的内存进行限制。分配的内存在可溢出算子的所有实例中平均分配（每个节点上的每个查询）。实例的数量 = 查询计划中的可溢出算子数量 * 最大并行度。最大并行度 = 为可溢出算子的每个实例执行工作的minor fragment的数量。当一个可溢出算子的实例必须处理更多的数据，超过它内存所能存放的，这个算子会临时溢写一些数据到磁盘目录来完成整个工作。 </p>
<h5 id="溢写到磁盘"><a href="#溢写到磁盘" class="headerlink" title="溢写到磁盘"></a>溢写到磁盘</h5><p>溢写到磁盘可以避免内存密集型操作因为内存耗尽而失败。当算子的内存要求超过设置时，溢写磁盘的特性可以使可溢写算子自动将多余的数据写到磁盘的临时目录。当算子在后台执行溢写操作时，查询不会间断。</p>
<p>当可溢写算子完成内存数据的处理后，会将磁盘中数据读回完成数据的处理，之后清除溢写位置的数据。</p>
<p>理想情况下，可以分配足够的内存，为Drill在内存中执行所有的操作。当数据溢写到磁盘，并不会看到查询运行的差异，然而溢写到磁盘会影响性能，因为写到磁盘再从磁盘读取，会产生额外的IO。</p>
<p>(1) 溢写位置</p>
<p>溢写的默认位置是<code>/tmp/drill/spill</code> 。这个目录适合小的负载和示例来用。因此，需要重新设置溢写的位置，有足够的磁盘空间来支撑大的工作负载。</p>
<p><strong>备注：溢出的数据可能需要更大的空间，相比查询中涉及的表引用的数据。例如，当底层表数据是压缩的（ORC、Parquet）或者算子接收的数据要join多张表。</strong></p>
<p>当你配置溢写位置，可以设置单个目录或目录列表给可溢出算子使用。</p>
<p>(2) 溢写磁盘配置</p>
<p>在配置文件<code>drill-override.conf</code>中可以设置溢写位置，管理可以更改文件系统以及目录列表。配置项如下：</p>
<ul>
<li><code>drill.exec.spill.fs</code>：默认的文件系统是本地机器，<code>file:///</code>。也可以配置分布式文件系统，比如<code>hdfs:///</code> ；</li>
<li><code>drill.exec.spill.directories</code>：默认是<code>[&quot;/tmp/drill/spill&quot;]</code> 。也可以配置为<code>[&quot;/fs1/drill/spill&quot;,&quot;/fs2/drill/spill&quot;]</code>。</li>
</ul>
<h5 id="内存分配"><a href="#内存分配" class="headerlink" title="内存分配"></a>内存分配</h5><p>Drill在可溢出算子的所有实例之间均匀地分割可用内存。当查询被并行化时，算子的数量将会成倍增加，这会减少查询期间为算子的所有实例提供的内存数量。要查看算子之间内存消耗差异，可以运行查询，在Drill Web UI中查看query profiles。或者，关闭hash算子，强制Drill使用Merge Join和Streaming Aggregate。</p>
<p>(1) 内存分配配置</p>
<ul>
<li><p><code>planner.memory.max_query_memory_per_node</code> ：drill在一个节点上每个查询可使用的最小内存，默认是2GB，在JVM的Direct Memory默认值为8GB的情况下，可以到2-3个并发。当Drill的内存要求增加，默认的2GB被约束，必须要添加这个值大小才能完成查询。除非<code>planner.memory.percent_per_query</code>设置允许Drill使用更多的内存。</p>
</li>
<li><p><code>planner.memory.percent_per_query</code></p>
<p>另外一种方式，这个配置设置为总Direct Memory的百分比，默认值是5%。当throttling关闭时这个值才会使用，设置为0时关闭该选项。可以增加或减少该值，但是要将百分比设置在远低于JVM Direct Memory的位置，因为要考虑Drill不管理内存的情况，例如内存密集度较低的算子。</p>
<ul>
<li>计算公式如下：(1 - non-managed allowance) / concurrency</li>
<li>non-managed allowance是non-managed算子使用的假设系统内存。non-managed算子不会溢写到磁盘。non-managed allowance的保守假设是系统内存的50%。concurrency是并发查询的数量。默认假设是10个并发查询。</li>
</ul>
</li>
</ul>
<p>(2) 增加可获得的内存</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">SYSTEM</span> | <span class="keyword">SESSION</span> <span class="keyword">SET</span> <span class="string">`planner.memory.max_query_memory_per_node`</span>= new_value</span><br><span class="line">// the <span class="keyword">default</span> <span class="keyword">value</span> <span class="keyword">is</span> <span class="keyword">to</span> <span class="number">2</span>GB</span><br><span class="line">ALSTER <span class="keyword">SYSTEM</span> | <span class="keyword">SESSION</span> <span class="keyword">SET</span> <span class="string">`planner.memory.percent_per_query`</span> = new_value</span><br><span class="line">// the <span class="keyword">default</span> <span class="keyword">value</span> <span class="keyword">is</span> <span class="number">0.05</span></span><br></pre></td></tr></table></figure>
<h5 id="关闭Hash算子"><a href="#关闭Hash算子" class="headerlink" title="关闭Hash算子"></a>关闭Hash算子</h5><p>可以关闭hash聚合和hash join的算子。当关闭这些算子之后，Drill会创建一个替代查询计划，采用Sort算子和Streaming聚合/Merge Join算子。配置项如下：</p>
<ul>
<li><p><code>planner.enable_hashagg</code> : 开启或关闭hash聚合，关闭后Drill采用基于Sort的聚合。默认是开启的，推荐开启。在Drill 1.11版本之前，hash聚合算子内存不受控制（达到10GB），然后就耗尽内存。从这个版本之后，支持溢写到磁盘；</p>
</li>
<li><p><code>planner.enable_hashjoin</code> : 开启或关闭hash关联，默认是开启的。Drill假设一个查询有足够的内存来完成，则会尝试使用最可能快的操作。Drill 1.14版本之前，Hash Join算子使用不受控制的内存（达到10G），然后就耗尽内存。从这个版本之后，这个算子支持溢写到磁盘；</p>
</li>
<li><p><code>planner.enable_semijoin</code> : 开启或关闭Hash Join的Semi Join功能，默认是开启的。开启后，Hash Join中的semi-join标记设置为true。Drill使用semi-join来去除Hash Join下的不同处理。关闭后，Drill还是可以执行semi-join，但是semi-join将会Hash Join的外部执行，如下示例所示：</p>
<ul>
<li>Semi Join关闭</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">explain</span> plan <span class="keyword">for</span> </span><br><span class="line"><span class="keyword">select</span> employee_id, full_name </span><br><span class="line"><span class="keyword">from</span> employee <span class="keyword">where</span> employee_id <span class="keyword">IN</span> (<span class="keyword">select</span> employee_id <span class="keyword">from</span> employee);</span><br><span class="line"></span><br><span class="line">screen</span><br><span class="line">project(employee_id=[$0], full_name=[$1])</span><br><span class="line"> project(employee_id=[$0], full_name=[$1])</span><br><span class="line">  HashJoin(condition=[=($0, $2)], joinType=[inner], semi-join: =[false]) <span class="comment">--哈希连接</span></span><br><span class="line">	 scan(...)</span><br><span class="line">     project(employee_id0=[$0])</span><br><span class="line">       HashAgg(group=[$0]) <span class="comment">-- 去重使用</span></span><br><span class="line">      scan(...)</span><br></pre></td></tr></table></figure>
<ul>
<li>Semi Join开启</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">explain</span> plan <span class="keyword">for</span> </span><br><span class="line"><span class="keyword">select</span> employee_id, full_name </span><br><span class="line"><span class="keyword">from</span> employee <span class="keyword">where</span> employee_id <span class="keyword">IN</span> (<span class="keyword">select</span> employee_id <span class="keyword">from</span> employee);</span><br><span class="line"></span><br><span class="line">screen</span><br><span class="line">project(employee_id=[$0], full_name=[$1])</span><br><span class="line"> project(employee_id=[$0], full_name=[$1])</span><br><span class="line">  HashJoin(condition=[=($0, $2)], joinType=[inner], semi-join: =[true]) <span class="comment">--半连接</span></span><br><span class="line">    scan(...)</span><br><span class="line">    scan(...)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="开启查询队列"><a href="#开启查询队列" class="headerlink" title="开启查询队列"></a>开启查询队列</h4><p>Drill默认允许并发查询，但是当少量并发查询时，Drill性能会有所提高。开启查询队列来限制并发运行查询的最大数量。将大查询拆分为多个小查询并开启查询队列可以提高查询性能。</p>
<p>当你开启查询队列，你可以配置大队列和小队列。Drill根据查询的大小，将查询路由到哪个队列。Drill能够快速完成查询，继续执行下一组查询。</p>
<p>请看如下示例：</p>
<p><img src="/blog/8c6d7818/query_queuing.png" alt></p>
<ul>
<li><p>A查询（蓝色部分）：10亿条记录，预估1000万行数据处理;</p>
</li>
<li><p>B查询（红色部分）：20亿条记录，预估2000万行数据处理；</p>
</li>
<li><p>C查询：10亿条记录；</p>
</li>
<li><p>D查询：100条记录；</p>
</li>
</ul>
<p><code>exec.queue.threshold</code>是3000万作为查询预估处理行数。因此，A和B属于large query，在large queue中排队。当预估处理的行数达到3000万的阀值时，A和B的查询已经填满了队列。之后C查询到来，只能等待。D查询到来理解进入small queue中。</p>
<p>相关配置如下：</p>
<ul>
<li><code>exec.queue.enable</code> : 默认关闭。开启后，控制同时运行的查询数量。</li>
<li><code>exec.queue.large</code> : 默认为10，范围为0-1000。设置集群中大查询并发数量。</li>
<li><code>exec.queue.small</code> : 默认为100，范围为0-10001。</li>
<li><code>exec.queue.threshold</code> : 决定一个查询是否为large还是small的。复杂查询有更高的值。默认为30000000，范围为0-9223372036854775807。</li>
<li><code>exec.queue.timeout_millis</code> : 表示一个查询在队列等待的时间在失败之前。默认为300000，范围为0-9223372036854775807。</li>
<li><code>exec.queue.memory_ratio</code> : 默认情况下，大查询使用的内存是小查询的10倍。如果实际过程中，发现其他值效果更好，则可以调整这个比例来满足实际的查询。</li>
<li><code>exec.queue.memory_reserve_ratio</code> : 还有Sort和Hash聚合算子要观察内存限制，溢写到磁盘。其他算子没有被管理，所需的内存量因您的特定查询而异。考虑到这些算子，需要预留一些内存。默认值是20%。但是重Join工作负载可能需要更大的值，比如50%甚至更多。</li>
</ul>
<h4 id="限流"><a href="#限流" class="headerlink" title="限流"></a>限流</h4><p>Drill 1.12版本引入限流。限流限制了并发查询的数量，防止内存耗尽导致查询失败。当开启限流后，可以控制并发查询的数量和每个查询的资源要求。Drill会为每个节点和每个查询计算要分配的内存量。</p>
<p>如果限流关闭，可能需要增大<code>planner.memory.max_query_memory_per_node</code>的可分配内存。Drill必须决定为每个算子分配多大的内存，但不知道可能运行多少并发查询。如果Drill不能给Sort和Hash聚合算子足够的内存，查询将会失败。此时，就需要开启限流防止这种情况的发生。</p>
<h5 id="限流配置"><a href="#限流配置" class="headerlink" title="限流配置"></a>限流配置</h5><p>小队列和大队列的计算方式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">memory unit = small_queue + (large_queue * memory_ratio)</span><br><span class="line">total memory available = total_direct_mem * (1 - memory_reserve_ratio)</span><br><span class="line">small queue memory allocation = total_mem_available / memory_unit</span><br><span class="line">large queue memory allocation = small queue_memory_allocation * memory_ratio</span><br></pre></td></tr></table></figure>
<h5 id="调校"><a href="#调校" class="headerlink" title="调校"></a>调校</h5><p>通过查看query profiles来决定正确的参数：</p>
<ul>
<li>设置队列大小保守一些确保查询成功；</li>
<li>做实验，通过观察典型的查询的实际开销来调整队列阀值；</li>
<li>如果由于Join等操作导致OOM发生，请调整内存设置。</li>
</ul>
<h2 id="识别性能问题"><a href="#识别性能问题" class="headerlink" title="识别性能问题"></a>识别性能问题</h2><h4 id="查询计划"><a href="#查询计划" class="headerlink" title="查询计划"></a>查询计划</h4><p>如果在Drill中遇到性能问题，你通常能够在query plan或query profiles中识别问题来源。本小节介绍逻辑计划和物理计划。</p>
<p><strong>Drill有一个优化器和并行器一起工作来计划一个查询。Drill基于相关文件或数据源的统计信息来创建逻辑计划、物理计划以及执行计划。Drill的运行节点数量以及运行时配置有助于Drill如何计划执行一个查询</strong>。</p>
<p>我们可以通过执行<code>explain</code>命令来查看逻辑计划和物理计划，当时执行计划看不到，可以通过8047界面查看query profile。</p>
<h5 id="逻辑计划"><a href="#逻辑计划" class="headerlink" title="逻辑计划"></a>逻辑计划</h5><p>一个逻辑计划是一系列逻辑算子的集合，描述了要生成的查询结果、定义的数据源和应用的算子。Drill的解析器将SQL算子转为逻辑算子语法，Drill理解后创建逻辑计划。你可以通过逻辑计划看到这些计划算子集。通过<code>submit_plan</code>命令，将修改后的逻辑计划重新提交给Drill，但这个作用并不是很大，因为计划阶段Drill还不能决定并行度。</p>
<h5 id="物理计划"><a href="#物理计划" class="headerlink" title="物理计划"></a>物理计划</h5><p>一个物理计划描述了针对查询语句被选中的物理执行计划。优化器会应用多种规则重新安排算子和函数，形成一个优化的计划，然后将这个逻辑计划转为物理计划，告知Drill如何执行一个查询。</p>
<p>你可以重新评审一个物理执行计划中的问题，修改这个计划，再次提交给Drill。比如，你遇到转换错误或者你想改变表的join的顺序看是否查询会更快。你可以修改物理计划来解决问题，提交给Drill来执行查询。</p>
<p>Drill将物理计划转为minor fragments的执行树，在集群上并发运行来执行任务（参见query execution）。你可以在query profile中查看fragments执行查询的活动（参见query profiles）。</p>
<h5 id="查看物理计划"><a href="#查看物理计划" class="headerlink" title="查看物理计划"></a>查看物理计划</h5><p>物理计划显示major fragments和与major fragment ids、operator ids相关的指定算子。Major fragments是一个抽象概念表示查询执行的一个阶段，不执行任何任务。</p>
<p>物理计划中的展示的ID格式：<code>&lt;MajorFragmentID&gt; - &lt;OperatorID&gt;</code></p>
<h4 id="Query-Profiles"><a href="#Query-Profiles" class="headerlink" title="Query Profiles"></a>Query Profiles</h4><p>一个profile是Drill每个查询收集的metrics信息的摘要。Query Profiles提供的信息，我们可以用来监控和分析查询性能。当Drill执行查询，会将每个查询的profile写入到磁盘，本地文件系统或分布式文件系统。<strong>在Drill 1.16版本中，如果有问题会影响性能，Web UI展示了告警信息</strong>。</p>
<h5 id="query-profile"><a href="#query-profile" class="headerlink" title="query profile"></a>query profile</h5><ul>
<li>STATE(查询状态)：running、completed、failed;</li>
<li>FOREMAN(协调器): drillbit接收到来自客户端或应用的查询后，节点作为foreman来运行，驱动整个查询；</li>
<li>Total Fragments: 要求在执行的minor fragment的总数量。</li>
</ul>
<h5 id="标示符构成"><a href="#标示符构成" class="headerlink" title="标示符构成"></a>标示符构成</h5><p>query profile文件中的metrics和标示符的坐标系相关联。Drill使用由query、fragment和operator标示符构成的坐标系来跟踪查询执行的活动和资源。Drill分配一个唯一标示符Query ID，给每个接收到的查询，然后给每个fragment和operator分配ID。示例如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">MajorFragmentID-MinorFragmentID-OperatorID</span><br><span class="line">	01-00-02</span><br></pre></td></tr></table></figure>
<p>当Drill执行一个查询时，工作负载应该被统一分布到fragment和operator中来处理数据。当你评估query profile时，看到fragment的处理时间不成比例的分布或者内存的过度使用，一般表明性能存在问题，要求性能调优。比如：</p>
<ul>
<li>一定时间内查询没有进度；</li>
<li>算子有数据溢写到磁盘（没有足够内存完成整个操作）；</li>
<li>算子花在等待数据的时间远远大于处理数据的时间。</li>
</ul>
<p>下面列出，其他的一些告警信息：</p>
<h5 id="告警阈值设置"><a href="#告警阈值设置" class="headerlink" title="告警阈值设置"></a>告警阈值设置</h5><p>通过<code>drill-override.conf</code>设置，如下所示：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">http: &#123;</span><br><span class="line">	profile.warning: &#123;</span><br><span class="line"> 		progress.threshold: 300,</span><br><span class="line">  		time.skew: &#123;</span><br><span class="line">			min: 2,</span><br><span class="line">  			ratio: &#123;</span><br><span class="line">				process: 2</span><br><span class="line">  				wait: 2</span><br><span class="line">			&#125;	</span><br><span class="line">		&#125;,</span><br><span class="line">		scan.wait.min: 60</span><br><span class="line">	&#125;,</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="物理计划视图"><a href="#物理计划视图" class="headerlink" title="物理计划视图"></a>物理计划视图</h4><p>物理计划视图提供了统计信息，一个查询操作的实际成本，包括memory、IO以及CPU。可以通过这个信息识别查询期间操作消耗的主要资源。</p>
<h2 id="性能调优参考"><a href="#性能调优参考" class="headerlink" title="性能调优参考"></a>性能调优参考</h2><h4 id="Query-Profile描述"><a href="#Query-Profile描述" class="headerlink" title="Query Profile描述"></a>Query Profile描述</h4><h5 id="Fragment-概览表格"><a href="#Fragment-概览表格" class="headerlink" title="Fragment 概览表格"></a>Fragment 概览表格</h5><table>
<thead>
<tr>
<th>列名</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Major Fragment ID</td>
<td>major片段坐标ID。例如，03-xx-xx。其中03是主片段ID，后面两位分别代表minor片段ID和算子ID。</td>
</tr>
<tr>
<td>Minor Fragments Reporting</td>
<td>为major片段并行化的minor片段数量</td>
</tr>
<tr>
<td>First Start</td>
<td>第一个minor片段开始任务前的总时间</td>
</tr>
<tr>
<td>Last Start</td>
<td>最后一个minor片段开始任务前的总时间</td>
</tr>
<tr>
<td>First End</td>
<td>第一个minor片段完成任务的总时间</td>
</tr>
<tr>
<td>Last End</td>
<td>最后一个minor片段完成任务的总时间</td>
</tr>
<tr>
<td>Min Runtime</td>
<td>minor片段完成任务花费总时间的最小值</td>
</tr>
<tr>
<td>Avg Runtime</td>
<td>minor片段完成任务花费总时间的平均值</td>
</tr>
<tr>
<td>Max Runtime</td>
<td>minor片段完成任务花费总时间的最大值</td>
</tr>
<tr>
<td>Last Update</td>
<td>minor片段发送更新状态给foreman最后一次时间。时间24小时制表示。</td>
</tr>
<tr>
<td>Last Progress</td>
<td>minor片段进度变化的最后一次时间，如fragment状态或从磁盘读数据。时间24小时制表示。</td>
</tr>
<tr>
<td>Max Peak Memory</td>
<td>所有minor片段中申请direct memory的最大峰值</td>
</tr>
</tbody>
</table>
<h5 id="Major-Fragment块"><a href="#Major-Fragment块" class="headerlink" title="Major Fragment块"></a>Major Fragment块</h5><p>展示每个major片段中minor片段被并行化的度量信息。</p>
<table>
<thead>
<tr>
<th>列名</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Minor Fragment ID</td>
<td>major片段中被并行化的minor片段坐标ID。例如，02-03-xx，02是major片段ID，03是minor片段ID，xx是算子ID。</td>
</tr>
<tr>
<td>Host</td>
<td>minor片段执行任务所在的节点</td>
</tr>
<tr>
<td>Start</td>
<td>minor片段开始任务之前经过的时间</td>
</tr>
<tr>
<td>End</td>
<td>minor片段完成任务之前经过的时间</td>
</tr>
<tr>
<td>Runtime</td>
<td>fragment完成任务的持续时间。这个值是End-Start</td>
</tr>
<tr>
<td>Max Records</td>
<td>算子从单个输入流中消耗的最大记录数</td>
</tr>
<tr>
<td>Max Batches</td>
<td>跨输入流、算子以及minor片段的最大输入批次数</td>
</tr>
<tr>
<td>Last Update</td>
<td>fragment发送更新状态给foreman最后一次时间</td>
</tr>
<tr>
<td>Last Progress</td>
<td>fragment产生进度，比如状态变化、从磁盘读数据的最后一次时间</td>
</tr>
<tr>
<td>Peak Memory</td>
<td>minor fragment执行期间分配德尔direct memory的峰值</td>
</tr>
<tr>
<td>State</td>
<td>minor fragment的状态，完成、运行、取消或失败</td>
</tr>
</tbody>
</table>
<h5 id="Operator-概览表格"><a href="#Operator-概览表格" class="headerlink" title="Operator 概览表格"></a>Operator 概览表格</h5><p>显示的是在执行查询期间一个major片段执行关系操作中每个算子的聚合度量信息。</p>
<table>
<thead>
<tr>
<th>列名</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Operator ID</td>
<td>在查询的一个特定阶段，一个执行操作的算子坐标ID。比如02-xx-03，02是major片段，xx是对应minor片段，03是算子ID。</td>
</tr>
<tr>
<td>Type</td>
<td>算子类型，如project、filter、hash join、single sender或者unordered receiver。</td>
</tr>
<tr>
<td>Min/Avg/Max Setup Time</td>
<td>在执行操作之前，算子setup所花费的最小、平均和最大时间</td>
</tr>
<tr>
<td>Min/Avg/Max Process Time</td>
<td>算子执行操作所花费的最小、平均和最大时间</td>
</tr>
<tr>
<td>Wait(Min/Avg/Max)</td>
<td>算子等待外部数据源所花费最小、平均和最大时间</td>
</tr>
<tr>
<td>Avg Peak Memory</td>
<td>在minor fragment中分配direct memory的平均峰值。跟算子执行操作所需的内存有关，比如hash join或sort。</td>
</tr>
<tr>
<td>Max Peak Memory</td>
<td>在minor fragment中分配direct memory的最大峰值。跟算子执行操作所需的内存有关，比如hash join或sort。</td>
</tr>
</tbody>
</table>
<h5 id="Operator-块"><a href="#Operator-块" class="headerlink" title="Operator 块"></a>Operator 块</h5><p>显示每个major片段中每个操作类型时间和内存度量。</p>
<table>
<thead>
<tr>
<th>列名</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>minor fragment</td>
<td>算子所在的minor fragment坐标ID。例如，04-03-01，04是major fragment ID，03是minor fragment ID，01是算子ID。</td>
</tr>
<tr>
<td>Setup Time</td>
<td>算子执行操作之前的启动时间，包含runtime code的生成和打开文件</td>
</tr>
<tr>
<td>Process Time</td>
<td>算子执行操作的时间</td>
</tr>
<tr>
<td>Wait Time</td>
<td>算子等待外部数据源的时间，比如等待发送记录，等待接收记录，等待写入磁盘，等待从磁盘读取。</td>
</tr>
<tr>
<td>Max Batches</td>
<td>从单个输入流消费的最大记录批次</td>
</tr>
<tr>
<td>Max Records</td>
<td>从单个输入流消费的最大记录数量</td>
</tr>
<tr>
<td>Peak Memory</td>
<td>代表分配的direct memory峰值。跟算子执行操作所需要的内存有关，比如hash join或sort。</td>
</tr>
</tbody>
</table>
<h4 id="物理算子"><a href="#物理算子" class="headerlink" title="物理算子"></a>物理算子</h4><h5 id="分发算子"><a href="#分发算子" class="headerlink" title="分发算子"></a>分发算子</h5><p>以下算子在网络中执行数据分布：</p>
<table>
<thead>
<tr>
<th>算子</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>HashToRandomExchange</td>
<td>获取一个输入行，基于分布的key计算hash值，然后基于hash值决定终端接收器，在一个batch的操作中发送该行。分布的key可以是join key或者group by聚合的key。目标接收器是目标节点上的一个minor fragment。</td>
</tr>
<tr>
<td>HashToMergeExchange</td>
<td>和<code>HashToRandomExchange</code>类似，只是每个目标接收器合并来自发送者的排序后的数据。</td>
</tr>
<tr>
<td>UnionExchange</td>
<td>是一个序列化算子，每个发送器向同一个目标节点发送数据，接收器union各个发送者的数据。</td>
</tr>
<tr>
<td>SingleMergeExchange</td>
<td>是一个分发算子，每个发送者向一个单接收器发送排序数据，接收器合并所有数据。可用于order by操作，要求最终全局有序。</td>
</tr>
<tr>
<td>BroadcastExchange</td>
<td>是一个分发算子，每个发送器发送数据给N个接收器，通过广播的形式。</td>
</tr>
<tr>
<td>UnorderedMuxExchange</td>
<td>在一个节点上所有的minor fragment的数据进行复用，使得数据通过一个单一通道就可以发送到目标接收器。一个发送节点上只要为每个接收节点维护一个缓冲区，而不是每个接收节点的每个minor fragment。</td>
</tr>
</tbody>
</table>
<h5 id="Join算子"><a href="#Join算子" class="headerlink" title="Join算子"></a>Join算子</h5><table>
<thead>
<tr>
<th>算子</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hash Join</td>
<td>用于内连接、左连接、右连接以及全外连接。一个hash表构建于Hash Join的inner child产生的数据，outer child的数据用于探测这个hash表并寻找匹配的。这个算子持有整个join右侧的数据集在内存中，每个minor fragment能达到20亿。</td>
</tr>
<tr>
<td>Merge Join</td>
<td>用于内连接、左连接、右连接以及全外连接。要求输入的数据必须排好序的，从两侧读取排序记录，寻找匹配的行。这个算子持有来自join两侧一个输入记录批次的内存。</td>
</tr>
<tr>
<td>Nested Loop Join</td>
<td>内嵌循环连接用于特定类型的笛卡尔连接和不等式连接。</td>
</tr>
</tbody>
</table>
<h5 id="聚合算子"><a href="#聚合算子" class="headerlink" title="聚合算子"></a>聚合算子</h5><table>
<thead>
<tr>
<th>算子</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hash Aggregate</td>
<td>基于group by的key构建的hash表，hash聚合对输入数据执行分组聚合。这个算子持有每个聚合分组的内存，每个minor fragment聚合的值达到20亿个值。</td>
</tr>
<tr>
<td>Streaming Aggregate</td>
<td>流聚合执行分组聚合和非分组聚合。对于分组聚合，数据必须是按照分组key进行排序的。聚合的值在每个组中被计算。对于非分组聚合，数据不一定必须被排序。这个算子维护一个单一聚合分组（keys和聚合中间值），以及接入的一个记录批次大小。</td>
</tr>
</tbody>
</table>
<h5 id="排序和limit算子"><a href="#排序和limit算子" class="headerlink" title="排序和limit算子"></a>排序和limit算子</h5><table>
<thead>
<tr>
<th>算子</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sort</td>
<td>用来执行order by操作，以及要求数据有序的上行算子操作（例如，merge join、streaming aggregate）</td>
</tr>
<tr>
<td>ExternalSort</td>
<td>外排算子可能在内存中持有整个数据集。当内存有压力时，算子也会托管到磁盘，这种情况下，算子也会尽量使用更多的内存。在所有场景下，外部排序为每个记录溢出在内存中至少保留一个记录批次。溢出大小目前取决于外部排序算子的可用内存量。</td>
</tr>
<tr>
<td>TopN</td>
<td>用于执行order by + limit</td>
</tr>
<tr>
<td>Limit</td>
<td>限制返回行数</td>
</tr>
</tbody>
</table>
<h5 id="投影算子"><a href="#投影算子" class="headerlink" title="投影算子"></a>投影算子</h5><table>
<thead>
<tr>
<th>算子</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Project</td>
<td>投影字段或包含列和常量表达式</td>
</tr>
</tbody>
</table>
<h5 id="过滤和关系算子"><a href="#过滤和关系算子" class="headerlink" title="过滤和关系算子"></a>过滤和关系算子</h5><table>
<thead>
<tr>
<th>算子</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Filter</td>
<td>计算WHERE或者HAVING谓词</td>
</tr>
<tr>
<td>SelectionVectorRemover</td>
<td>和Sort、Filter算子一起使用，此算子维护的内存量大约是单个传入批次内存量的两倍</td>
</tr>
</tbody>
</table>
<h5 id="集合算子"><a href="#集合算子" class="headerlink" title="集合算子"></a>集合算子</h5><table>
<thead>
<tr>
<th>算子</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Union All</td>
<td>接收两个输入流，和生成一个输出流。right输入行紧跟left输入行。列名继承自left输入，left和right的字段类型必须兼容。</td>
</tr>
</tbody>
</table>
<h5 id="scan算子"><a href="#scan算子" class="headerlink" title="scan算子"></a>scan算子</h5><table>
<thead>
<tr>
<th>算子</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Scan</td>
<td>执行底层表的扫描，格式有<code>parquet</code>、<code>text</code>、<code>json</code>等等。</td>
</tr>
</tbody>
</table>
<h5 id="接收算子"><a href="#接收算子" class="headerlink" title="接收算子"></a>接收算子</h5><table>
<thead>
<tr>
<th>算子</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>UnorderedReceiver</td>
<td>可容纳5个输入记录批次</td>
</tr>
<tr>
<td>MergingReceiver</td>
<td>这个算子为每个输入流保存5个记录批次（一般，节点数量或发送fragment的数量，取决于muxxing的使用）</td>
</tr>
</tbody>
</table>
<h5 id="发送算子"><a href="#发送算子" class="headerlink" title="发送算子"></a>发送算子</h5><table>
<thead>
<tr>
<th>算子</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>PartitionSender</td>
<td>为每个outbound目标维护一个队列。outbound的minor fragment数量或者节点数量，取决于muxxing操作的使用。每个队列为每个目标最多可村3个记录批次。</td>
</tr>
</tbody>
</table>
<h5 id="文件写入"><a href="#文件写入" class="headerlink" title="文件写入"></a>文件写入</h5><table>
<thead>
<tr>
<th>算子</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>ParquetFileWriter</td>
<td>写缓冲大小大约是默认Parquet在minor fragment内存中行组大小的两倍</td>
</tr>
</tbody>
</table>
]]></content>
      <categories>
        <category>计算引擎</category>
      </categories>
      <tags>
        <tag>Drill</tag>
        <tag>查询计划</tag>
      </tags>
  </entry>
  <entry>
    <title>我的论文</title>
    <url>/blog/1d1f5c7b.html</url>
    <content><![CDATA[<h2 id="分布式系统"><a href="#分布式系统" class="headerlink" title="分布式系统"></a>分布式系统</h2><p><a href="https://ai.google/research/pubs/pub37200" target="_blank" rel="noopener">Tenzing A SQL Implementation On The MapReduce Framework</a></p>
<p><a href="https://raft.github.io/raft.pdf" target="_blank" rel="noopener">Raft Consensus Algorithm</a></p>
<p><a href="https://cs.stanford.edu/~matei/papers/2013/sosp_sparrow.pdf" target="_blank" rel="noopener">Sparrow: Distributed, Low Latency Scheduling</a></p>
<p><a href="https://github.com/theanalyst/awesome-distributed-systems" target="_blank" rel="noopener">https://github.com/theanalyst/awesome-distributed-systems</a></p>
<h2 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h2><p><a href="http://csce.uark.edu/~xintaowu/5203/pinot.pdf" target="_blank" rel="noopener">Pinot: Realtime OLAP for 530 Million Users</a></p>
<p><a href="http://www.vldb.org/pvldb/vol11/p1835-samwel.pdf" target="_blank" rel="noopener">F1 Query: Declarative Querying at Scale</a></p>
<p><a href="http://db.csail.mit.edu/projects/cstore/abadi-sigmod08.pdf" target="_blank" rel="noopener">Column-Stores vs. Row-Stores: How Different Are They Really?</a></p>
<p><a href="http://info.snowflake.net/rs/252-RFO-227/images/Snowflake_SIGMOD.pdf" target="_blank" rel="noopener">The Snowflake Elastic Data Warehouse</a></p>
<p><a href="http://static.druid.io/docs/druid.pdf" target="_blank" rel="noopener">A Real-time Analytical Data Store - Druid</a></p>
<p><a href="https://research.google.com/pubs/archive/41376.pdf" target="_blank" rel="noopener">Online, Asynchronous Schema Change in F1</a></p>
<p><a href="https://15721.courses.cs.cmu.edu/spring2016/papers/p337-soliman.pdf" target="_blank" rel="noopener">Orca: A Modular Query Optimizer Architecture for Big Data</a></p>
<p><a href="http://adrianmarriott.net/logosroot/papers/LifeBeyondTxns.pdf" target="_blank" rel="noopener">Life beyond Distributed Transactions: an Apostate’s Opinion</a></p>
<p><a href="https://cs.stanford.edu/people/chrismre/cs345/rl/aries.pdf" target="_blank" rel="noopener">ARIES: A Transaction Recovery Method Supporting Fine-Granularity Locking and Partial Rollbacks Using Write-Ahead Logging</a></p>
<p><a href="http://www.vldb.org/conf/1999/P1.pdf" target="_blank" rel="noopener">Repeating History Beyond ARIES</a></p>
<p><a href="https://www.computer.org/csdl/proceedings/cloud/2018/7235/00/723501a508-abs.html" target="_blank" rel="noopener">Automatic Tuning of SQL-On-Hadoop Engines on Cloud Platforms</a></p>
<p><a href="https://pdfs.semanticscholar.org/a817/a3e74d1663d9eb35b4baf3161ab16f57df85.pdf" target="_blank" rel="noopener">The Volcano Optimizer Generator: Extensibility and Efficient Search</a></p>
<h2 id="大数据"><a href="#大数据" class="headerlink" title="大数据"></a>大数据</h2><p><a href="https://www.52cs.com/archives/story/大数据必读文献" target="_blank" rel="noopener">https://www.52cs.com/archives/story/大数据必读文献</a></p>
]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>论文</tag>
      </tags>
  </entry>
  <entry>
    <title>Drill执行引擎介绍</title>
    <url>/blog/66333501.html</url>
    <content><![CDATA[<h4 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h4><p>Apache Drill是一个低延迟的分布式查询引擎，用于大规模数据集，包括结构化和半结构化/嵌套数据。受Google的Dremel启发，Drill被设计成可扩展到数千个节点，并以BI/分析环境所需的交互速度查询数PB的数据。Drill对于大型数据集上的简短、交互式即席查询也很有用。Drill能够以JSON和Parquet等格式查询嵌套数据，并执行动态模式发现。</p>
<p>Drill具有高吞吐和低延迟的特性，它不使用像MapReduce、Tez或Spark这样的通用执行引擎。因此，Drill是灵活和高效的。Drill的优化器利用了基于规则和成本的优化技术，以及数据局部性和操作下推，具备将查询片段下推到后端数据源的能力。Drill还提供了一个列式和矢量化的执行引擎，从而提高了内存和CPU的效率。</p>
<p>如果为了简单使用，可以下载并且直接运行在笔记本电脑。当用于分析更大的数据集时，可以在Hadoop集群（最多可达1000个节点）上直接部署Drill。Drill利用集群中的聚合内存，使用乐观的流水线模型执行查询，并在工作集不适合内存时自动溢出到磁盘。</p>
<p>下面，本文将从Drill执行引擎的整体架构以及优化器技术、物理计划生成和执行方面，结合源代码进行分析介绍。</p>
<h4 id="2-整体架构"><a href="#2-整体架构" class="headerlink" title="2. 整体架构"></a>2. 整体架构</h4><p>Drill包括一个分布式执行环境，专门为大规模数据处理而构建。它的核心是Drillbit服务，负责接收来自客户端的请求、处理查询并将结果返回给客户端（如下图一所示）。可以在Hadoop集群中的所有必须节点上安装并运行Drillbit服务，以形成分布式集群环境。当Drillbit在集群中的每个数据节点上运行时，Drill可以在查询执行过程中最大化数据局部性，而无需再网络上或节点之间移动数据。Drill使用Zookeeper维护集群成员和健康检测信息。当然，Drill不与Hadoop绑定 ，可以在任何分布式集群环境中运行。唯一的前提条件是依赖Zookeeper。</p>
<p><img src="/blog/66333501/query-flow-client.png" alt></p>
<p><center>图一  客户端与Drillbit的关系</center><br>当Drillbit接收到客户端查询时，就成为了Foreman来驱动整个查询，如下图二所示。首先，Foreman中的解析器负责解析SQL，应用自定义规则将特定的SQL运算符转换为Drill可以理解的特定逻辑预算符，以此来形成一个逻辑计划。逻辑计划描述了生成查询结果所需的工作，并定义了要应用的数据源和操作。</p>
<p>其次，Foreman将逻辑计划发送到基于成本的优化器中，优化器应用各种类型的规则，将运算符和函数重新排列到最优计划中。优化器将逻辑计划转换为描述如何执行查询的物理计划。</p>
<p><img src="/blog/66333501/client-phys-plan.png" alt></p>
<p><center>图二  SQL到物理计划流程</center><br>Foreman中的并行化器将物理计划转换为多个阶段，称为major fragment和minor fragment。这些fragment创建了一个多级执行树，该树重写查询并对配置的数据源并行执行查询，将结果发送回客户端或应用程序。如图三所示。</p>
<p><img src="/blog/66333501/execution-tree.png" alt></p>
<p><center>图三 执行树</center></p>
<h4 id="3-查询接入"><a href="#3-查询接入" class="headerlink" title="3. 查询接入"></a>3. 查询接入</h4><p>第一步，Drillbit服务端接收请求的处理器。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.drill.exec.rpc.user;</span><br><span class="line"><span class="comment">//包引入省略</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UserServerRequestHandler</span> <span class="keyword">implements</span> <span class="title">RequestHandler</span>&lt;<span class="title">BitToUserConnection</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">//处理各种用户的请求</span></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> UserWorker worker;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(BitToUserConnection connection, <span class="keyword">int</span> rpcType, ByteBuf pBody, ByteBuf dBody, ResponseSender responseSender)</span> </span>&#123;</span><br><span class="line">  	<span class="keyword">case</span> RpcType.RUN_QUERY_VALUE:<span class="comment">//执行查询</span></span><br><span class="line">      logger.debug(<span class="string">"Received query to run.  Returning query handle."</span>);</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">//解析请求为RunQuery protobuf信息</span></span><br><span class="line">        <span class="keyword">final</span> RunQuery query = RunQuery.PARSER.parseFrom(<span class="keyword">new</span> ByteBufInputStream(pBody));</span><br><span class="line">        <span class="comment">//提交查询，并返回一个查询ID</span></span><br><span class="line">        <span class="keyword">final</span> QueryId queryId = worker.submitWork(connection, query);</span><br><span class="line">        <span class="comment">//设置返回的响应信息</span></span><br><span class="line">        responseSender.send(<span class="keyword">new</span> Response(RpcType.QUERY_HANDLE, queryId));</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      &#125; <span class="keyword">catch</span> (InvalidProtocolBufferException e) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> RpcException(<span class="string">"Failure while decoding RunQuery body."</span>, e);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">case</span> RpcType.CANCEL_QUERY_VALUE:<span class="comment">//取消查询</span></span><br><span class="line">      	<span class="comment">//省略 worker.cancelQuery(queryId);</span></span><br><span class="line">      <span class="keyword">case</span> RpcType.RESUME_PAUSED_QUERY_VALUE:<span class="comment">//恢复暂停的查询</span></span><br><span class="line">      	<span class="comment">//省略</span></span><br><span class="line">      <span class="keyword">case</span> RpcType.GET_QUERY_PLAN_FRAGMENTS_VALUE:<span class="comment">//获取查询计划fragment</span></span><br><span class="line">      	<span class="comment">//省略 worker.getQueryPlan(connection, req)</span></span><br><span class="line">      <span class="keyword">case</span> RpcType.GET_CATALOGS_VALUE:<span class="comment">//获取所有数据库分类，默认为DRILL</span></span><br><span class="line">      	<span class="comment">//省略</span></span><br><span class="line">      <span class="keyword">case</span> RpcType.GET_SCHEMAS_VALUE:<span class="comment">//获取所有数据库</span></span><br><span class="line">      	<span class="comment">//省略</span></span><br><span class="line">      <span class="keyword">case</span> RpcType.GET_TABLES_VALUE:<span class="comment">//获取指定库下的所有表</span></span><br><span class="line">      	<span class="comment">//省略</span></span><br><span class="line">      <span class="keyword">case</span> RpcType.GET_COLUMNS_VALUE:<span class="comment">//获取指定表下的所有列</span></span><br><span class="line">      	<span class="comment">//省略</span></span><br><span class="line">      <span class="keyword">case</span> RpcType.CREATE_PREPARED_STATEMENT_VALUE:</span><br><span class="line">      	<span class="comment">//省略</span></span><br><span class="line">      <span class="keyword">case</span> RpcType.GET_SERVER_META_VALUE:<span class="comment">//获取服务端相关配置信息</span></span><br><span class="line">      	<span class="comment">//省略</span></span><br><span class="line">      <span class="keyword">default</span>:</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(</span><br><span class="line">          String.format(<span class="string">"UserServerRequestHandler received rpc of unknown type. Type was %d."</span>, rpcType));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>第二步，解析出请求类型后，进行相应处理。比如是查询请求，则初始化Foreman开始处理。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.drill.exec.work.user;</span><br><span class="line"><span class="comment">//包引入省略</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserWorker</span></span>&#123;</span><br><span class="line">    </span><br><span class="line">  <span class="comment">//管理foreman的执行</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> WorkerBee bee;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//提交查询</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> QueryId <span class="title">submitWork</span><span class="params">(UserClientConnection connection, RunQuery query)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//生成查询ID</span></span><br><span class="line">    <span class="keyword">final</span> QueryId id = queryIdGenerator();</span><br><span class="line">    <span class="comment">//本次会话查询数量加1</span></span><br><span class="line">    incrementer.increment(connection.getSession());</span><br><span class="line">    <span class="comment">//初始化包工头，是一个线程</span></span><br><span class="line">    Foreman foreman = <span class="keyword">new</span> Foreman(bee, bee.getContext(), connection, id, query);</span><br><span class="line">    <span class="comment">//加入执行队列</span></span><br><span class="line">    bee.addNewForeman(foreman);</span><br><span class="line">    <span class="keyword">return</span> id;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>第三步，Foreman在驱动节点或根节点上，为单个查询管理所有的fragments(本地或远程)。主要流程为：当Foreman初始化时，查询处于准备状态。作为Runnable被提交进入队列排队，用于执行查询计划。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.drill.exec.work.foreman;</span><br><span class="line"><span class="comment">//包引入省略</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Foreman</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//处理查询执行的一些细节，每个Foreman都持有一个</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> QueryManager queryManager;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> QueryStateProcessor queryStateProcessor;</span><br><span class="line">     </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">Foreman</span><span class="params">(...)</span> </span>&#123;</span><br><span class="line">      </span><br><span class="line">      <span class="keyword">this</span>.queryManager = <span class="keyword">new</span> QueryManager(queryId, queryRequest, drillbitContext.getStoreProvider(), drillbitContext.getClusterCoordinator(), <span class="keyword">this</span>);</span><br><span class="line">      <span class="comment">//初始状态为：QueryState.PREPARING;</span></span><br><span class="line">      <span class="keyword">this</span>.queryStateProcessor = <span class="keyword">new</span> QueryStateProcessor(queryIdString, queryManager, drillbitContext, <span class="keyword">new</span> ForemanResult());</span><br><span class="line">     </span><br><span class="line">  &#125;</span><br><span class="line">    </span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  	<span class="comment">//部分省略...</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">//检测foreman是否在线，否则不能接收新的查询</span></span><br><span class="line">      <span class="keyword">if</span> (!drillbitContext.isForemanOnline()) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> ForemanException(<span class="string">"Query submission failed since Foreman is shutting down."</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (ForemanException e) &#123;</span><br><span class="line">      logger.debug(<span class="string">"Failure while submitting query"</span>, e);</span><br><span class="line">      <span class="comment">//新增QueryState.FAILED的事件</span></span><br><span class="line">      queryStateProcessor.addToEventQueue(QueryState.FAILED, e);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//从QueryState.PREPARING状态切换到QueryState.PLANNING状态</span></span><br><span class="line">    queryStateProcessor.moveToState(QueryState.PLANNING, <span class="keyword">null</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 将查询请求转为具体的行为</span></span><br><span class="line">      <span class="keyword">switch</span> (queryRequest.getType()) &#123;</span><br><span class="line">          <span class="keyword">case</span> LOGICAL:<span class="comment">//请求是逻辑计划</span></span><br><span class="line">            parseAndRunLogicalPlan(queryRequest.getPlan());</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">          <span class="keyword">case</span> PHYSICAL:<span class="comment">//请求是物理计划</span></span><br><span class="line">            parseAndRunPhysicalPlan(queryRequest.getPlan());</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">          <span class="keyword">case</span> SQL:<span class="comment">//请求是SQL</span></span><br><span class="line">            <span class="keyword">final</span> String sql = queryRequest.getPlan();</span><br><span class="line">            logger.info(<span class="string">"Query text for query with id &#123;&#125; issued by &#123;&#125;: &#123;&#125;"</span>, queryIdString,queryContext.getQueryUserName(), sql);</span><br><span class="line">            runSQL(sql);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">          <span class="keyword">case</span> EXECUTION:<span class="comment">//请求是fragments</span></span><br><span class="line">            runFragment(queryRequest.getFragmentsList());</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">          <span class="keyword">case</span> PREPARED_STATEMENT:<span class="comment">//请求是预编译</span></span><br><span class="line">            runPreparedStatement(queryRequest.getPreparedStatementHandle());</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">          <span class="keyword">default</span>:</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException();</span><br><span class="line">      &#125;</span><br><span class="line">      </span><br><span class="line">    &#125; <span class="keyword">catch</span> (...) &#123;</span><br><span class="line">     	<span class="comment">//...</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//当run方法执行完成后，foreman实例还一直存在，并且通过QueryManager的stateListener来间接地接收关于fragment完成的事件，直到一切都完成、失败或者被取消。</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="4-查询执行"><a href="#4-查询执行" class="headerlink" title="4. 查询执行"></a>4. 查询执行</h4><p>查询请求进入Foreman后，针对SQL类查询，进入foreman#runSQL(sql)方法处理。如下代码所示：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.drill.exec.work.foreman;</span><br><span class="line"><span class="comment">//包引入省略</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Foreman</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">	<span class="comment">//省略部分</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">runSQL</span><span class="params">(<span class="keyword">final</span> String sql)</span> <span class="keyword">throws</span> ExecutionSetupException </span>&#123;</span><br><span class="line">      <span class="keyword">final</span> Pointer&lt;String&gt; textPlan = <span class="keyword">new</span> Pointer&lt;&gt;();</span><br><span class="line">      <span class="comment">//将sql转为物理计划</span></span><br><span class="line">      <span class="keyword">final</span> PhysicalPlan plan = DrillSqlWorker.getPlan(queryContext, sql, textPlan);</span><br><span class="line">      <span class="comment">//执行物理计划</span></span><br><span class="line">      runPhysicalPlan(plan, textPlan);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="4-1-SQL转物理计划"><a href="#4-1-SQL转物理计划" class="headerlink" title="4.1 SQL转物理计划"></a>4.1 SQL转物理计划</h5><p>第一步，根据SQL类型进行相应的处理。比如，SQL查询进入DefaultSqlHandler#.getPlan(sqlNode)。代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.drill.exec.planner.sql;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DrillSqlWorker</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 将查询sql语句转为查询物理计划</span></span><br><span class="line"><span class="comment">   * 捕获各种异常并尽可能转为用户异常</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> PhysicalPlan <span class="title">getPlan</span><span class="params">(QueryContext context, String sql, Pointer&lt;String&gt; textPlan)</span> <span class="keyword">throws</span> ForemanSetupException </span>&#123;</span><br><span class="line">    <span class="comment">//省略  convertPlan(context, sql, textPlan);</span></span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * sql中使用的函数，本地函数库和远程函数库同步问题</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> PhysicalPlan <span class="title">convertPlan</span><span class="params">(QueryContext context, String sql, Pointer&lt;String&gt; textPlan)</span> </span>&#123;</span><br><span class="line">   <span class="comment">// 省略 getQueryPlan(context, sql, textPlan);</span></span><br><span class="line">  &#125;</span><br><span class="line">    </span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> PhysicalPlan <span class="title">getQueryPlan</span><span class="params">(QueryContext context, String sql, Pointer&lt;String&gt; textPlan)</span> <span class="keyword">throws</span> ForemanSetupException, RelConversionException, IOException, ValidationException </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//sql解析器，基于calcite parser.jj和自定义扩展parser.jj形成drill的sql解析</span></span><br><span class="line">    <span class="keyword">final</span> SqlConverter parser = <span class="keyword">new</span> SqlConverter(context);</span><br><span class="line">    <span class="comment">//生成calcite表示的抽象语法树（AST）</span></span><br><span class="line">    <span class="keyword">final</span> SqlNode sqlNode = checkAndApplyAutoLimit(parser, context, sql);</span><br><span class="line">    <span class="comment">//sql操作处理</span></span><br><span class="line">    <span class="keyword">final</span> AbstractSqlHandler handler;</span><br><span class="line">    <span class="keyword">final</span> SqlHandlerConfig config = <span class="keyword">new</span> SqlHandlerConfig(context, parser);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">switch</span>(sqlNode.getKind()) &#123;</span><br><span class="line">      <span class="keyword">case</span> EXPLAIN: <span class="comment">//查看sql执行计划</span></span><br><span class="line">       <span class="comment">//省略</span></span><br><span class="line">      <span class="keyword">case</span> SET_OPTION:<span class="comment">//设置配置项</span></span><br><span class="line">       <span class="comment">//省略</span></span><br><span class="line">      <span class="keyword">case</span> DESCRIBE_TABLE:<span class="comment">//查看表元数据</span></span><br><span class="line">       <span class="comment">//省略</span></span><br><span class="line">      <span class="keyword">case</span> DESCRIBE_SCHEMA:<span class="comment">//查看数据库元数据</span></span><br><span class="line">       <span class="comment">//省略</span></span><br><span class="line">      <span class="keyword">case</span> CREATE_TABLE:<span class="comment">//建表</span></span><br><span class="line">       <span class="comment">//省略</span></span><br><span class="line">      <span class="keyword">case</span> DROP_TABLE:</span><br><span class="line">      <span class="keyword">case</span> CREATE_VIEW:</span><br><span class="line">      <span class="keyword">case</span> DROP_VIEW:</span><br><span class="line">      <span class="keyword">case</span> OTHER_DDL:</span><br><span class="line">      <span class="keyword">case</span> OTHER:</span><br><span class="line">		<span class="comment">//省略</span></span><br><span class="line">        <span class="comment">// fallthrough</span></span><br><span class="line">      <span class="keyword">default</span>:</span><br><span class="line">        <span class="comment">//sql处理，主要用于AST向执行计划的转换（含基于规则和成本的优化）</span></span><br><span class="line">        handler = <span class="keyword">new</span> DefaultSqlHandler(config, textPlan);</span><br><span class="line">        context.setSQLStatementType(SqlStatementType.OTHER);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//省略...</span></span><br><span class="line">    <span class="keyword">return</span> handler.getPlan(sqlNode);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>第二步，DefaultSqlHandler#getPlan(sqlNode)实现了AST转物理计划的功能。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.drill.exec.planner.sql.handlers;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DefaultSqlHandler</span> <span class="keyword">extends</span> <span class="title">AbstractSqlHandler</span> </span>&#123;</span><br><span class="line">  </span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> PhysicalPlan <span class="title">getPlan</span><span class="params">(SqlNode sqlNode)</span> <span class="keyword">throws</span> ValidationException, RelConversionException, IOException, ForemanSetupException </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 1. 将AST转为关系表达式</span></span><br><span class="line"><span class="comment">     * 其中采用HepPlanner将subquery和window function阶段设置的静态规则进行SQL改写</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">final</span> ConvertedRelNode convertedRelNode = validateAndConvert(sqlNode);</span><br><span class="line">    <span class="keyword">final</span> RelDataType validatedRowType = convertedRelNode.getValidatedRowType();</span><br><span class="line">    <span class="keyword">final</span> RelNode queryRelNode = convertedRelNode.getConvertedNode();</span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 2. 将原始关系表达式RelNode转为Drill逻辑关系表达式DrillRelNode</span></span><br><span class="line"><span class="comment">	 * 2.1. 调用convertToRawDrel</span></span><br><span class="line"><span class="comment">	 * 2.2. 在最上层添加一个Screen算子，用于显示查询结果</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">final</span> DrillRel drel = convertToDrel(queryRelNode);</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 3. 将DrillRelNode转为Drill物理表达式PhysicalRelNode</span></span><br><span class="line"><span class="comment">     * 3.1 通过VolcanoPlanner通过内置的以及存储自己设置的物理规则进行优化调整 （考虑成本）</span></span><br><span class="line"><span class="comment">     * 3.2 针对join sql中多表字段冲突重命名</span></span><br><span class="line"><span class="comment">     * 3.3 inner join时左右表交换，大表在左，小表在右（构建hash表）</span></span><br><span class="line"><span class="comment">     * 3.4 将所有复杂输出拆解为对应的EXP$名称</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">final</span> Prel prel = convertToPrel(drel, validatedRowType);</span><br><span class="line">    logAndSetTextPlan(<span class="string">"Drill Physical"</span>, prel, logger);</span><br><span class="line">    <span class="comment">// 4. 将Drill物理关系表达式转为Drill物理算子</span></span><br><span class="line">    <span class="keyword">final</span> PhysicalOperator pop = convertToPop(prel);</span><br><span class="line">    <span class="comment">// 5. 提取算子，构建root、leaf的graph</span></span><br><span class="line">    <span class="keyword">final</span> PhysicalPlan plan = convertToPlan(pop);</span><br><span class="line">    log(<span class="string">"Drill Plan"</span>, plan, logger);</span><br><span class="line">    <span class="keyword">return</span> plan;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">protected</span> DrillRel <span class="title">convertToRawDrel</span><span class="params">(<span class="keyword">final</span> RelNode relNode)</span> <span class="keyword">throws</span> SqlUnsupportedException </span>&#123;</span><br><span class="line">    <span class="comment">//开启针对含有limit 0的查询进行优化</span></span><br><span class="line">    <span class="keyword">if</span> (context.getOptions().getOption(ExecConstants.EARLY_LIMIT0_OPT) &amp;&amp;</span><br><span class="line">        context.getPlannerSettings().isTypeInferenceEnabled() &amp;&amp;</span><br><span class="line">        FindLimit0Visitor.containsLimit0(relNode)) &#123;</span><br><span class="line">      <span class="keyword">final</span> DrillRel shorterPlan;</span><br><span class="line">      <span class="comment">//返回列类型都是可识别的，则直接生成特定执行计划</span></span><br><span class="line">      <span class="keyword">if</span> ((shorterPlan = FindLimit0Visitor.getDirectScanRelIfFullySchemaed(relNode)) != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> shorterPlan;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">//针对limit 0的查询，关闭分布式模式</span></span><br><span class="line">      <span class="keyword">if</span> (FindHardDistributionScans.canForceSingleMode(relNode)) &#123;</span><br><span class="line">        context.getPlannerSettings().forceSingleMode();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 适用于HepPlanner优化器的规则，在VolcanoPlanner优化器中会失败的</span></span><br><span class="line">      <span class="comment">// 将集合操作算子（union)和其他算子进行转换</span></span><br><span class="line">      <span class="keyword">final</span> RelNode setOpTransposeNode = transform(PlannerType.HEP, PlannerPhase.PRE_LOGICAL_PLANNING, relNode);</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 通过HepPlanner优化器从下往上的匹配顺序，进行目录剪枝优化。主要规则有：</span></span><br><span class="line">      <span class="comment">// 1. *转为字段引用，便于分区剪枝和下推规则来检测可以修改或下推的字段</span></span><br><span class="line">      <span class="comment">// 2. 没有group by或distinct的聚合，将scan优化为可以直接从统计信息获取</span></span><br><span class="line">      <span class="keyword">final</span> RelNode pruned = transform(PlannerType.HEP_BOTTOM_UP, PlannerPhase.DIRECTORY_PRUNING, setOpTransposeNode);</span><br><span class="line">      <span class="comment">//为剪枝后的关系表达式加上Drill逻辑特性，关联Drill对关系表达式的相关实现</span></span><br><span class="line">      <span class="keyword">final</span> RelTraitSet logicalTraits = pruned.getTraitSet().plus(DrillRel.DRILL_LOGICAL);</span><br><span class="line"></span><br><span class="line">      <span class="keyword">final</span> RelNode convertedRelNode;</span><br><span class="line">      <span class="keyword">if</span> (!context.getPlannerSettings().isHepOptEnabled()) &#123;</span><br><span class="line">        <span class="comment">// HepPlanner关闭（默认开启），采用VolcanoPlanner</span></span><br><span class="line">        <span class="comment">// LOGICAL_PRUNE_AND_JOIN 限定的规则集合，如分区剪枝和Join置换</span></span><br><span class="line">        convertedRelNode = transform(PlannerType.VOLCANO, PlannerPhase.LOGICAL_PRUNE_AND_JOIN, pruned, logicalTraits);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">final</span> RelNode intermediateNode2;</span><br><span class="line">        <span class="keyword">final</span> RelNode intermediateNode3;</span><br><span class="line">        <span class="comment">//默认开启通过HepPlanner进行分区剪枝</span></span><br><span class="line">        <span class="keyword">if</span> (context.getPlannerSettings().isHepPartitionPruningEnabled()) &#123;</span><br><span class="line">		  <span class="comment">//采用基于成本代价的VolcanoPlanner进行逻辑计划优化</span></span><br><span class="line">          <span class="comment">// 1. PlannerPhase.LOGICAL 限定的规则集合</span></span><br><span class="line">          <span class="keyword">final</span> RelNode intermediateNode = transform(PlannerType.VOLCANO, PlannerPhase.LOGICAL, pruned, logicalTraits);</span><br><span class="line"></span><br><span class="line">          <span class="comment">// Hep对Join相关的优化，比如将join中的谓词传递到具体表中</span></span><br><span class="line">          <span class="comment">// 2. PlannerPhase.TRANSITIVE_CLOSURE 限定的规则集合</span></span><br><span class="line">          <span class="keyword">final</span> RelNode transitiveClosureNode =</span><br><span class="line">              transform(PlannerType.HEP, PlannerPhase.TRANSITIVE_CLOSURE, intermediateNode);</span><br><span class="line"></span><br><span class="line">          <span class="comment">// Hep 分区剪枝，包含自定义的逻辑优化规则、limit推到scan等</span></span><br><span class="line">          <span class="comment">// 3. PARTITION_PRUNING 限定的规则集合，主要侧重文件系统上的目录文件存储</span></span><br><span class="line">          intermediateNode2 = transform(PlannerType.HEP_BOTTOM_UP, PlannerPhase.PARTITION_PRUNING, transitiveClosureNode);</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// 1. VolcanoPlanner 执行 PlannerPhase.LOGICAL_PRUNE 限定的规则</span></span><br><span class="line">          <span class="keyword">final</span> RelNode intermediateNode =</span><br><span class="line">              transform(PlannerType.VOLCANO, PlannerPhase.LOGICAL_PRUNE, pruned, logicalTraits);</span><br><span class="line">          <span class="comment">// 2. HepPlanner 执行 PlannerPhase.TRANSITIVE_CLOSURE 限定的规则</span></span><br><span class="line">          intermediateNode2 = transform(PlannerType.HEP, PlannerPhase.TRANSITIVE_CLOSURE, intermediateNode);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// HepPlanner 执行 PlannerPhase.JOIN_PLANNING 限定的规则，主要是转为DrillJoinRel</span></span><br><span class="line">        intermediateNode3 = transform(PlannerType.HEP_BOTTOM_UP, PlannerPhase.JOIN_PLANNING, intermediateNode2);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//默认开启</span></span><br><span class="line">        <span class="keyword">if</span> (context.getPlannerSettings().isRowKeyJoinConversionEnabled()) &#123;</span><br><span class="line">          <span class="comment">// 将Join转为主键join，方便join条件下推</span></span><br><span class="line">          convertedRelNode = transform(PlannerType.HEP_BOTTOM_UP, PlannerPhase.ROWKEYJOIN_CONVERSION, intermediateNode3);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          convertedRelNode = intermediateNode3;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//省略...</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> drillRel;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (RelOptPlanner.CannotPlanException ex) &#123;</span><br><span class="line">      <span class="comment">//省略...</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="4-2-物理计划转执行树"><a href="#4-2-物理计划转执行树" class="headerlink" title="4.2 物理计划转执行树"></a>4.2 物理计划转执行树</h5><p>首先，将优化后的物理计划转为多个Fragment，其次，将多个Fragment通过并行器parallelizer形成一个可跨节点执行的fragment树。主要代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.drill.exec.work.foreman;</span><br><span class="line"><span class="comment">//包引入省略</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Foreman</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">	</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> QueryManager queryManager;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 用于控制算子中buffer的大小</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> QueryResourceManager queryRM;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 负责执行local或remote的fragment</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> FragmentsRunner fragmentsRunner;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Foreman</span><span class="params">(...)</span> </span>&#123;</span><br><span class="line">       <span class="comment">// 集群模式下是DynamicResourceManager，</span></span><br><span class="line">       <span class="comment">// 通过配置exec.queue.enable 可以任意开启或关闭ThrottledResourceManager</span></span><br><span class="line">       <span class="keyword">this</span>.queryRM = drillbitContext.getResourceManager().newQueryRM(<span class="keyword">this</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">runPhysicalPlan</span><span class="params">(<span class="keyword">final</span> PhysicalPlan plan, Pointer&lt;String&gt; textPlan)</span> <span class="keyword">throws</span> ExecutionSetupException </span>&#123;</span><br><span class="line">    <span class="comment">// 验证该计划是可执行的</span></span><br><span class="line">    validatePlan(plan);</span><br><span class="line">    <span class="comment">// 为sort、join等算子分配内存</span></span><br><span class="line">    queryRM.visitAbstractPlan(plan);</span><br><span class="line">    <span class="comment">// 封装了fragment的root、child的fragment信息</span></span><br><span class="line">    <span class="keyword">final</span> QueryWorkUnit work = getQueryWorkUnit(plan, queryRM);</span><br><span class="line">    <span class="keyword">if</span> (enableRuntimeFilter) &#123;</span><br><span class="line">      runtimeFilterRouter = <span class="keyword">new</span> RuntimeFilterRouter(work, drillbitContext);</span><br><span class="line">      runtimeFilterRouter.collectRuntimeFilterParallelAndControlInfo();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (textPlan != <span class="keyword">null</span>) &#123;</span><br><span class="line">      queryManager.setPlanText(textPlan.value);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 计算算子需要的内存是否可以获取</span></span><br><span class="line">    queryRM.visitPhysicalPlan(work);</span><br><span class="line">    <span class="comment">// 根据cost判断是large query还是small query</span></span><br><span class="line">    queryRM.setCost(plan.totalCost());</span><br><span class="line">    queryManager.setTotalCost(plan.totalCost());</span><br><span class="line">    <span class="comment">// work将fragment转为json信息</span></span><br><span class="line">    work.applyPlan(drillbitContext.getPlanReader());</span><br><span class="line">    logWorkUnit(work);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将work中的fragment相关信息传给fragmentrunner</span></span><br><span class="line">    fragmentsRunner.setFragmentsInfo(work.getFragments(), work.getRootFragment(), work.getRootOperator());</span><br><span class="line">	</span><br><span class="line">    <span class="comment">// 开始查询</span></span><br><span class="line">    startQueryProcessing();</span><br><span class="line">  &#125;</span><br><span class="line">    </span><br><span class="line">  <span class="function"><span class="keyword">private</span> QueryWorkUnit <span class="title">getQueryWorkUnit</span><span class="params">(<span class="keyword">final</span> PhysicalPlan plan, <span class="keyword">final</span> QueryResourceManager rm)</span> <span class="keyword">throws</span> ExecutionSetupException </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> PhysicalOperator rootOperator = plan.getSortedOperators(<span class="keyword">false</span>).iterator().next();</span><br><span class="line">	<span class="comment">// 通过MakeFragmentsVisitor将物理算子分解为构成fragment的各个组成部分</span></span><br><span class="line">    <span class="keyword">final</span> Fragment rootFragment = rootOperator.accept(MakeFragmentsVisitor.INSTANCE, <span class="keyword">null</span>);</span><br><span class="line">	<span class="comment">// 并行器创建工作单元</span></span><br><span class="line">    <span class="comment">// 主要工作是初始化fragment树，基于每个fragment代价进行并行化；</span></span><br><span class="line">    <span class="comment">// 计算每个minor fragment的算子需要的memory</span></span><br><span class="line">    <span class="keyword">return</span> rm.getParallelizer(plan.getProperties().hasResourcePlan).generateWorkUnit(queryContext.getOptions().getOptionList(), queryContext.getCurrentEndpoint(), queryId, queryContext.getOnlineEndpoints(), rootFragment, initiatingClient.getSession(), queryContext.getQueryContextInfo());</span><br><span class="line">  &#125;</span><br><span class="line">    </span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">startQueryProcessing</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 压入队列，进入QueryState.ENQUEUED状态</span></span><br><span class="line">    <span class="comment">// 如果开启查询队列，先阻塞直到可以查询</span></span><br><span class="line">    enqueue();</span><br><span class="line">    <span class="comment">// fragmentsRunner.submit();</span></span><br><span class="line">    <span class="comment">// 提交root and non-root fragments执行</span></span><br><span class="line">    runFragments();</span><br><span class="line">    queryStateProcessor.moveToState(QueryState.RUNNING, <span class="keyword">null</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="4-3-执行树运行"><a href="#4-3-执行树运行" class="headerlink" title="4.3 执行树运行"></a>4.3 执行树运行</h5><p>(1) 物理执行计划转为执行树后，主要获得三个主要信息<code>FragmentRoot</code>、<code>PlanFragment</code>以及<code>List&lt;PlanFragment&gt;</code>。fragments分发的执行逻辑在FragmentRunner中，代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.drill.exec.work.foreman;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FragmentsRunner</span> </span>&#123;</span><br><span class="line"> </span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> WorkerBee bee;</span><br><span class="line">    </span><br><span class="line">  <span class="keyword">private</span> List&lt;PlanFragment&gt; planFragments;</span><br><span class="line">  <span class="keyword">private</span> PlanFragment rootPlanFragment;</span><br><span class="line">  <span class="keyword">private</span> FragmentRoot rootOperator;</span><br><span class="line">    </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">submit</span><span class="params">()</span> <span class="keyword">throws</span> ExecutionSetupException </span>&#123;</span><br><span class="line">    <span class="comment">// 省略...</span></span><br><span class="line">    logger.debug(<span class="string">"Submitting fragments to run."</span>);</span><br><span class="line">    <span class="comment">// 首先启动root fragment，获取incoming buffers</span></span><br><span class="line">    setupRootFragment(rootPlanFragment, rootOperator);</span><br><span class="line">    <span class="comment">// 启动non root fragment（local or remote）,没有完成前可能会返回数据</span></span><br><span class="line">    setupNonRootFragments(planFragments);</span><br><span class="line">    logger.debug(<span class="string">"Fragments running."</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 启动非根fragment</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">setupNonRootFragments</span><span class="params">(<span class="keyword">final</span> Collection&lt;PlanFragment&gt; fragments)</span> <span class="keyword">throws</span> ExecutionSetupException </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (fragments.isEmpty()) &#123;</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 向每个节点发送一条消息，不管该节点运行多少fragment</span></span><br><span class="line"><span class="comment">     * 我们需要先运行intermediate fragments处于准备状态，因为leaf fragments一开始运行就会产生数据 </span></span><br><span class="line"><span class="comment">     * 下面将intermediate和leaf fragments分出来</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">final</span> Multimap&lt;DrillbitEndpoint, PlanFragment&gt; leafFragmentMap = ArrayListMultimap.create();</span><br><span class="line">    <span class="keyword">final</span> Multimap&lt;DrillbitEndpoint, PlanFragment&gt; intFragmentMap = ArrayListMultimap.create();</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 这里省略分离的逻辑</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将intermediate fragments调度到相应的节点准备就绪</span></span><br><span class="line">    scheduleIntermediateFragments(intFragmentMap);</span><br><span class="line"></span><br><span class="line">    injector.injectChecked(foreman.getQueryContext().getExecutionControls(), <span class="string">"send-fragments"</span>, ForemanException<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 将属于一个节点的所有leaf fragments一个请求发过去</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">final</span> DrillbitEndpoint ep : leafFragmentMap.keySet()) &#123;</span><br><span class="line">      sendRemoteFragments(ep, leafFragmentMap.get(ep), <span class="keyword">null</span>, <span class="keyword">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>(2) fragment的接收通过ControlMessageHandler来处理，代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.drill.exec.work.batch;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ControlMessageHandler</span> <span class="keyword">implements</span> <span class="title">RequestHandler</span>&lt;<span class="title">ControlConnection</span>&gt; </span>&#123;</span><br><span class="line">	</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(ControlConnection connection, <span class="keyword">int</span> rpcType, ByteBuf pBody, ByteBuf dBody,ResponseSender sender)</span> <span class="keyword">throws</span> RpcException </span>&#123;</span><br><span class="line">  	<span class="keyword">switch</span> (rpcType) &#123;</span><br><span class="line">       <span class="comment">// 部分省略</span></span><br><span class="line">            </span><br><span class="line">       <span class="comment">// 接受fragment并初始化</span></span><br><span class="line">       <span class="keyword">case</span> RpcType.REQ_INITIALIZE_FRAGMENTS_VALUE: &#123;</span><br><span class="line">         <span class="keyword">final</span> InitializeFragments fragments = get(pBody, InitializeFragments.PARSER);</span><br><span class="line">         initializeFragment(fragments);</span><br><span class="line">         sender.send(ControlRpcConfig.OK);</span><br><span class="line">         <span class="keyword">break</span>;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">default</span>:</span><br><span class="line">         <span class="keyword">throw</span> <span class="keyword">new</span> RpcException(<span class="string">"Not yet supported."</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">    </span><br><span class="line">  <span class="function"><span class="keyword">public</span> Ack <span class="title">initializeFragment</span><span class="params">(InitializeFragments fragments)</span> <span class="keyword">throws</span> RpcException </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> DrillbitContext drillbitContext = bee.getContext();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; fragments.getFragmentCount(); i++) &#123;</span><br><span class="line">      <span class="comment">// 在当前节点启动一个fragment(leaf or intermediate fragment, 本地或远程节点分发过来的)</span></span><br><span class="line">      <span class="comment">// leaf fragment通过FragmentExecutor执行</span></span><br><span class="line">      <span class="comment">// intermediate fragment通过NonRootFragmentManager执行</span></span><br><span class="line">      startNewFragment(fragments.getFragment(i), drillbitContext);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> Acks.OK;</span><br><span class="line">  &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>(3) leaf fragment通过FragmentExecutor执行，代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.drill.exec.work.fragment;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FragmentExecutor</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">volatile</span> RootExec root; </span><br><span class="line">   </span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">   	  <span class="comment">//省略...</span></span><br><span class="line">      </span><br><span class="line">      <span class="comment">// 查询计划中最后一个处理的节点，其他节点包括exchange和storage节点，驱动整个查询</span></span><br><span class="line">      <span class="keyword">final</span> FragmentRoot rootOperator = <span class="keyword">this</span>.rootOperator != <span class="keyword">null</span> ? <span class="keyword">this</span>.rootOperator :</span><br><span class="line">          fragmentContext.getPlanReader().readFragmentRoot(fragment.getFragmentJson());</span><br><span class="line">      <span class="comment">// 根据物理算子树创建RecordBatch树（物理算子实现树）</span></span><br><span class="line">      root = ImplCreator.getExec(fragmentContext, rootOperator);</span><br><span class="line">      <span class="keyword">if</span> (root == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// fragment切换到运行状态</span></span><br><span class="line">      updateState(FragmentState.RUNNING);</span><br><span class="line">      <span class="comment">// 获取用户信息</span></span><br><span class="line">      <span class="keyword">final</span> UserGroupInformation queryUserUgi = fragmentContext.isImpersonationEnabled() ? ImpersonationUtil.createProxyUgi(fragmentContext.getQueryUserName()) :ImpersonationUtil.getProcessUserUGI();</span><br><span class="line"></span><br><span class="line">      queryUserUgi.doAs(<span class="keyword">new</span> PrivilegedExceptionAction&lt;Void&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Void <span class="title">run</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">          injector.injectChecked(fragmentContext.getExecutionControls(), <span class="string">"fragment-execution"</span>, IOException<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">          <span class="keyword">while</span> (shouldContinue()) &#123;</span><br><span class="line">              </span><br><span class="line">            <span class="keyword">for</span> (FragmentHandle fragmentHandle; (fragmentHandle = receiverFinishedQueue.poll()) != <span class="keyword">null</span>;) &#123;</span><br><span class="line">              <span class="comment">// 针对完成的请求进行处理</span></span><br><span class="line">              root.receivingFragmentFinished(fragmentHandle);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 算子的next，驱动下游算子</span></span><br><span class="line">            <span class="comment">// 比如join、sort、fliter等算子会通过codegen技术提升cpu计算性能</span></span><br><span class="line">            <span class="keyword">if</span> (!root.next()) &#123;</span><br><span class="line">              <span class="comment">// Fragment已经处理完所有数据</span></span><br><span class="line">              <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;);</span><br><span class="line">  &#125;    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="4-4-CodeGen"><a href="#4-4-CodeGen" class="headerlink" title="4.4 CodeGen"></a>4.4 CodeGen</h5><p>之所以需要代码生成，是为了提升CPU的执行效率。比如，我们生成的物理执行计划中会存在一些类似if或switch这样的判断逻辑，而在运行时条件的内容已经可知，我们可以直接去掉不必要的分支。另外，运行时可以了解到循环代码的具体循环次数，从而可以将循环展开，同样去除了分支判断逻辑。通过类似这样的优化可以消除分支预测，从而极大的提升性能。可以参考这个<a href="https://yq.aliyun.com/articles/644856" target="_blank" rel="noopener">帖子</a>。</p>
<p>DRILL的算子支持批量的处理，通过代码模板和列式数据，去除无效的分支处理，平铺代码，重新生成充分利用cache和利于CPU计算的代码，并编译交给JVM运行。</p>
<p>比如，拿FilterRecordBatch中的filter算子举例，代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.drill.exec.physical.impl.filter;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FilterRecordBatch</span> <span class="keyword">extends</span> <span class="title">AbstractSingleRecordBatch</span>&lt;<span class="title">Filter</span>&gt; </span>&#123;</span><br><span class="line">  <span class="comment">//一个选择向量，最多可前置64K个值。用于两种情况：</span></span><br><span class="line">  <span class="comment">// 1. 创建由筛选器保留的值列表</span></span><br><span class="line">  <span class="comment">// 2. 为已排序的批次提供重定向级别</span></span><br><span class="line">  <span class="keyword">private</span> SelectionVector2 sv2;</span><br><span class="line">  <span class="keyword">private</span> Filterer filter;</span><br><span class="line">    </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">FilterRecordBatch</span><span class="params">(Filter pop, RecordBatch incoming, FragmentContext context)</span> <span class="keyword">throws</span> OutOfMemoryException </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>(pop, context, incoming);</span><br><span class="line">  &#125;</span><br><span class="line">   </span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">setupNewSchema</span><span class="params">()</span> <span class="keyword">throws</span> SchemaChangeException </span>&#123;</span><br><span class="line">    <span class="comment">//清空选择器向量</span></span><br><span class="line">  	<span class="keyword">if</span> (sv2 != <span class="keyword">null</span>) &#123;</span><br><span class="line">      sv2.clear();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">switch</span> (incoming.getSchema().getSelectionVectorMode()) &#123;</span><br><span class="line">      <span class="keyword">case</span> NONE:</span><br><span class="line">        <span class="keyword">if</span> (sv2 == <span class="keyword">null</span>) &#123;</span><br><span class="line">          sv2 = <span class="keyword">new</span> SelectionVector2(oContext.getAllocator());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">this</span>.filter = generateSV2Filterer();</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      <span class="keyword">case</span> TWO_BYTE:</span><br><span class="line">        sv2 = <span class="keyword">new</span> SelectionVector2(oContext.getAllocator());</span><br><span class="line">        <span class="keyword">this</span>.filter = generateSV2Filterer();</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      <span class="keyword">case</span> FOUR_BYTE:</span><br><span class="line">      <span class="keyword">default</span>:</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (container.isSchemaChanged()) &#123;</span><br><span class="line">      container.buildSchema(SelectionVectorMode.TWO_BYTE);</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> IterOutcome <span class="title">doWork</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    container.zeroVectors();</span><br><span class="line">    <span class="keyword">int</span> recordCount = incoming.getRecordCount();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">//执行过滤操作</span></span><br><span class="line">      filter.filterBatch(recordCount);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (SchemaChangeException e) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(e);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> getFinalOutcome(<span class="keyword">false</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 创建selection vector v2 filterer</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> Filterer <span class="title">generateSV2Filterer</span><span class="params">()</span> <span class="keyword">throws</span> SchemaChangeException </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> ErrorCollector collector = <span class="keyword">new</span> ErrorCollectorImpl();</span><br><span class="line">    <span class="keyword">final</span> List&lt;TransferPair&gt; transfers = Lists.newArrayList();</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// CodeGenerator用于创建源代码来实现abstract template</span></span><br><span class="line">    <span class="comment">// 它包含多个ClassGenerator，用于实现外部类和内部类，与运行时生成的实例相关联</span></span><br><span class="line">    <span class="comment">// DRILL支持两种方式生成和编译代码。1. 控制字节码 2. java源文件</span></span><br><span class="line">    <span class="keyword">final</span> ClassGenerator&lt;Filterer&gt; cg = CodeGenerator.getRoot(Filterer.TEMPLATE_DEFINITION2, context.getOptions());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> LogicalExpression expr = ExpressionTreeMaterializer.materialize(popConfig.getExpr(), incoming, collector,</span><br><span class="line">            context.getFunctionRegistry(), <span class="keyword">false</span>, unionTypeEnabled);</span><br><span class="line">    <span class="keyword">if</span> (collector.hasErrors()) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> SchemaChangeException(String.format(<span class="string">"Failure while trying to materialize incoming schema.  Errors:\n %s."</span>, collector.toErrorString()));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    cg.addExpr(<span class="keyword">new</span> ReturnValueExpression(expr), ClassGenerator.BlkCreateMode.FALSE);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">final</span> VectorWrapper&lt;?&gt; v : incoming) &#123;</span><br><span class="line">      <span class="keyword">final</span> TransferPair pair = v.getValueVector().makeTransferPair(container.addOrGet(v.getField(), callBack));</span><br><span class="line">      transfers.add(pair);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">final</span> TransferPair[] tx = transfers.toArray(<span class="keyword">new</span> TransferPair[transfers.size()]);</span><br><span class="line">      CodeGenerator&lt;Filterer&gt; codeGen = cg.getCodeGenerator();</span><br><span class="line">      codeGen.plainJavaCapable(<span class="keyword">true</span>);</span><br><span class="line">      <span class="comment">//代码模板和incoming数据相集合，生成高效执行的filter实例</span></span><br><span class="line">      <span class="keyword">final</span> Filterer filter = context.getImplementationClass(codeGen);</span><br><span class="line">      filter.setup(context, incoming, <span class="keyword">this</span>, tx);</span><br><span class="line">      <span class="keyword">return</span> filter;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (ClassTransformationException | IOException e) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> SchemaChangeException(<span class="string">"Failure while attempting to load generated class"</span>, e);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="5-参考资料"><a href="#5-参考资料" class="headerlink" title="5. 参考资料"></a>5. 参考资料</h4><ul>
<li><a href="http://drill.apache.org/docs" target="_blank" rel="noopener">http://drill.apache.org/docs</a></li>
<li><a href="https://github.com/apache/drill" target="_blank" rel="noopener">https://github.com/apache/drill</a></li>
</ul>
]]></content>
      <categories>
        <category>计算引擎</category>
      </categories>
      <tags>
        <tag>SQL引擎</tag>
        <tag>Drill</tag>
        <tag>大数据</tag>
        <tag>MPP</tag>
      </tags>
  </entry>
  <entry>
    <title>数据库之行列存储简介</title>
    <url>/blog/9ff3ee6b.html</url>
    <content><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>数据库之所以有行存和列存之分，主要是为了满足不同的使用场景。我们常见的Oracle、MySQL等主流关系型数据库都是以行存为主，适合OLTP的应用，涉及事务处理、增删改查等操作。随着大数据的发展，新兴的Vertica、Greenplum、MonetDB、C-Store等数据库支持列式存储，适合OLAP的应用，涉及海量数据的分析操作。甚至业界一些数据库为了同时支持OLTP和OLAP的能力，采用行列混合存储的模式，来兼容这两种应用场景。因此，数据库采用不同的数据存储布局，决定了它本身对外支持的特性，用户据此并结合业务场景来选择合适的数据库产品。下面，本文将主要针对行列存储的概念、组织形式、优缺点进行简要介绍。</p>
<h4 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h4><p>在基于行式存储的（row-based storage）数据库中，数据是按照行的逻辑存储单元进行存储的，一个整行的数据在存储介质中以连续形式存在。如下图所示：</p>
<p><img src="/blog/9ff3ee6b/row-store.png" alt></p>
<p>在基于列式存储的（column-based storage）数据库中，数据是按照列的逻辑存储单元进行存储的，一个整列的数据在存储介质中以连续形式存在。如下图所示：</p>
<p><img src="/blog/9ff3ee6b/column-store.png" alt></p>
<p>以上描述的是行列存储的逻辑结构，具体物理层面的数据布局，要看各数据库支持行或列物理存储的具体实现。</p>
<h4 id="优缺点分析"><a href="#优缺点分析" class="headerlink" title="优缺点分析"></a>优缺点分析</h4><p>行式存储可以将一行数据一次性写入，保证数据的完整性。在读取过程中，根据条件进行精确查询时，可以一次性读取整行数据返回。但针对按列统计分析时，如果涉及的数据量大，读取整行时会存在大量冗余列，占用系统资源高，影响读取性能。</p>
<p>列式存储由于是按照列来存储，每列都有各自的数据类型，同一个类型的数据放在一起存储，方便做对应的编码压缩（比如行程编码、字典编码），极大地节省存储空间和传输带宽，同时也降低了按列分析的IO操作。但是，按列拆开存储，数据的完整性和写入效率也会不如行式存储，同时针对精确查询并且返回大部分列时也会产生大量IO。</p>
<h4 id="适用场景"><a href="#适用场景" class="headerlink" title="适用场景"></a>适用场景</h4><p>根据行存的特性，比较适用于OLTP的应用场景，比如小数据量的事务型增删改查操作。然而，为了应对海量数据的存储和计算，传统的OLTP数据库并不能满足。因此，列存的特性适用于海量日志型数据的分析查询，可以用一张成百上千个列的宽表来存储分析这些数据，各列独自存储，也提高了并发读取的性能。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>以上介绍了行式存储和列式存储的基本概念、组织方式、各自的优缺点以及应用场景。可以看出，列存相对于行存，在存储压缩、按列分析、降低IO等方面存在优势，但在精确查询返回大部分列时存在不足。因此，在一些分析型数据库存储引擎中引入了行列混合存储的概念，来兼顾OLAP式查询和精确查询的两种场景，这个后续再做介绍。</p>
<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><ul>
<li>Abadi D J ,  Madden S R ,  Hachem N . <a href="http://db.csail.mit.edu/projects/cstore/abadi-sigmod08.pdf" target="_blank" rel="noopener">Column-stores vs. row-stores: How different are they really?</a>[C]// Proceedings of the ACM SIGMOD International Conference on Management of Data, SIGMOD 2008, Vancouver, BC, Canada, June 10-12, 2008. ACM, 2008.</li>
<li><a href="https://www.the-paper-trail.org/post/2013-01-30-columnar-storage/" target="_blank" rel="noopener">https://www.the-paper-trail.org/post/2013-01-30-columnar-storage/</a></li>
<li>C. Zhan, M. Su, C. Wei, X. Peng, L. Lin, S. Wang, Z. Chen, F. Li, Y. Pan, F. Zheng, C. Chai, AnalyticDB: Real-time OLAP Database System at Alibaba Cloud, VLDB 2019.</li>
</ul>
]]></content>
      <categories>
        <category>数据库与大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>数据库</tag>
        <tag>行列存储</tag>
      </tags>
  </entry>
  <entry>
    <title>Paper翻译 AnalyticDB Real-time OLAP Database System at Alibaba Cloud</title>
    <url>/blog/e3ee66c7.html</url>
    <content><![CDATA[<h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>随着数据在规模和多样性方面的爆炸式增长，OLAP数据库在提供低延时（比如几百毫秒）的实时分析服务方面发挥着越来越重要的作用，尤其是当接入的查询天生就是复杂的和即席的。而且，这些系统被期望能够提供高查询并发和写入吞吐量，以及支持结构化和复杂数据类型（比如，JSON、Vector和文本）上的查询。</p>
<p>本文我们将介绍由阿里巴巴开发的实时OLAP数据库系统AnalyticDB（以下简称ADB）。ADB在可接受的负载下通过异步的方式维护所有的列索引，来提供低延时的复杂即席查询。它的存储引擎针对快速检索结构化和复杂类型的数据扩展了<strong>混合行列布局</strong>。为了以高并发查询和写入吞吐来处理大规模数据，ADB分离了读写路径。为了进一步降低查询延迟，<strong>为了充分利用底层存储和索引的优势，开发了一种新的存储感知SQL优化器和执行引擎</strong>。ADB已经成功部署在阿里云上，服务更多的消费者。它能够容纳100万亿行记录，即10PB+大小，同时每秒能够提供10m+的写和100k+的查询，在数百毫秒内完成复杂查询。</p>
<h4 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h4><p>ADB是一个提供PB级数据规模高并发、低延时、实时分析的OLAP数据库，运行在阿里云2000+物理节点之上。服务于广泛的阿里云业务场景，包括电子商务、金融、物流、公共交通、气象分析、娱乐等，以及阿里集团内部商业运营。</p>
<p>近期的工作(见文献 [35, 28, 29, 36, 25] )总结了开发一个具备低查询延时、数据实时性、灵活性、低成本、高扩展和高可用性OLAP数据库的主要挑战。相对这些工作，ADB实现的主要挑战在于PB级别的分析负载、万级表数量以及万亿级数据量。</p>
<p>第一个挑战，当今的用户面对比之前更加复杂的分析场景，但是对低查询延时还是有高的期望，用户无能容忍较长的分析时间。然而，ADB的用户来自各个领域，他们的分析需求大不相同而且经常变化，这使得他们多样和复杂的查询难以优化。这些包括了<strong>全表扫描、点查、多表关联、多条件组合</strong>，<strong>虽然构建索引是提高查询最直接的方式，但为每个列构建索引通常不再是有效的</strong>。</p>
<p>第二个挑战，新兴的复杂分析趋向于不同类型的查询，同时数据在存储层具备友好的、统一的数据布局。传统的OLAP查询和点查要求不同的数据布局，即分别为列存和行存[34, 12]。此外，我们的用户超过一半的数据是复杂类型，如文本、json串、向量和其他多媒体资源。<strong>一个实用的存储结构需要能够提供多个数据类型的快速检索，来提供高效的结构化和复杂类型的数据查询</strong>。</p>
<p>第三个挑战，系统在处理低延时实时查询时，还需要处理每秒数百万行在线写入请求。传统的设计( [6, 8, 10, 29, 5])读写在一个进程中，使得数据一旦提交就可以直接读到。然而，这样的设计并不适合我们的场景，为了保证读取性能需要消耗大量的资源从而影响写入性能，反之亦然。所以，<strong>一个谨慎的设计需要考虑查询性能、写入性能和数据可见性的权衡</strong>。</p>
<p>为了解决以上的挑战，我们在ADB中提出了很多新颖的设计与实现，并作出了如下的贡献：</p>
<ul>
<li>高效的索引管理</li>
</ul>
<p>ADB内嵌了一个高效的索引引擎，利用两个关键点在可接受的开销范围内提供低延时的方法。第一，在每一个表上所有列建立索引来获得即席复杂查询关键性能。我们进一步提出了<strong>一种基于运行时过滤比的索引路径选择机制</strong>来避免索引滥用导致性能下降。第二，因为在关键按路径上更新大量索引是被禁止的，索引是在非高峰期异步构建的。我们也维护了<strong>一个轻量级排序索引来降低增量数据（索引开始构建后新写入的数据）异步索引构建过程中带来的影响</strong>。</p>
<ul>
<li>结构化数据和复杂类型数据的存储结构</li>
</ul>
<p>我们设计<strong>一个底层存储来支持混合行列布局</strong>。尤其，我们使用了磁盘<strong>快速的顺序读写IO特性</strong>，实现在可接受的开销下运行OALP式和点查式工作负载。在存储层面，我们进一步将复杂类型数据和结构化数据整合在一起，来提供复杂类型数据的检索能力。</p>
<ul>
<li>读写分离</li>
</ul>
<p>为了支持高吞吐写入和低延时查询，我们的系统采用了读写分离的架构，分别通过读节点和写节点提供。这<strong>两种类型的节点是相互独立的，可以独自扩容</strong>。尤其，在写节点中，将写请求持久化到可靠的分布式存储Pangu中([3])。为了保持数据的实时性，<strong>版本验证机制</strong>引入到读节点中，使得读节点对写节点上之前写入的数据是可见的。</p>
<ul>
<li>增强的优化器和执行引擎</li>
</ul>
<p>为了进一步改善查询延时和并发度，我们增强了ADB的优化器和执行引擎，来充分发挥出存储和索引的优势。具体来说，我们提出了<strong>一种存储感知的SQL优化机制</strong>，<strong>它根据存储的特性，并为成本优化器的基数估计进行有效的实时采样，来生成最佳执行计划</strong>。此外，<strong>我们还为混合存储设计了高性能向量执行引擎来提升计算密集型的查询分析</strong>。</p>
<p>本论文其他章节安排如下标题所示。</p>
<h4 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h4><p>ADB是从零构建的基于云平台的大规模、实时分析系统。本章节，将ADB和其他系统做一个对比。</p>
<ul>
<li>OLTP数据库</li>
</ul>
<p>针对OLTP数据库，例如<strong>MySQL</strong>[6]、<strong>PostgreSQL</strong>[8]被设计用来<strong>支持事务查询，同时也考虑一行或多行的点查</strong>。因此，在OLTP数据库中的存储引擎是面向行的，并且通过构建B+树索引来提高查询性能。然而，行存并不适合分析查询，当查询只要求返回部分列时，行存会造成读写放大。而且，OLTP数据库通常在写入路径中更新索引比较活跃，这个操作代价很高，会影响写入吞吐和查询延时。</p>
<ul>
<li>OLAP数据库</li>
</ul>
<p>为了提高分析查询的效率，开发了许多OLAP数据库像<strong>Vertica</strong>[29]，Teradata DB[10]和<strong>Greenplum</strong>[5]。Vertica使用projection提高查询性能，取代了在列上构建常规索引，<strong>仅仅保存min/max信息，由于修剪效率较低而导致高延迟</strong>。Teradata DB和Greenplum采用列式存储，用户可以设置索引列。然而，<strong>它们有两个主要的局限：一是写路径中修改列索引，针对所有列索引来说是禁止的。二是列存针对点查需要大量随机IO</strong>。</p>
<ul>
<li>大数据系统</li>
</ul>
<p>随着MR模型[18]的出现，像<strong>Hive</strong>[35]、<strong>SparkSQL</strong>[37, 13]等批处理引擎，在多个机器上处理大数据变得很流行。但是，这些查询的执行是离线的，整个执行持续分钟或小时级别，并不适合实时查询。<strong>Impala</strong>[28]采用<strong>pipeline模型和列存</strong>将离线查询转为交互式查询，将一般查询延时降低到秒级。但是，<strong>Impala没有列索引，只有min/max统计信息，也不能处理复杂查询</strong>。</p>
<ul>
<li>实时OLAP系统</li>
</ul>
<p>最近，实时OLAP系统包括<strong>Druid</strong> [36]和<strong>Pinot</strong> [25]都采用了列存。<strong>Druid在纬度列上，Pinot在所有列上都构建了基于位图的倒排索引</strong>。如果Druid上的查询不在纬度列上，会产生更高的延时。它们在写流程中都需要更新索引，影响写入性能。同时，缺乏对UPDATE、JOIN和DELETE的支持。由于是列存的，点查的效率也不高。</p>
<ul>
<li>云分析服务</li>
</ul>
<p>近期又出现许多云服务，比如Amazon Redshift和Google BigQuery。其中，Amazon Redshift是完全托管的云数据库服务，采用列式存储和MPP结构将查询分布到多个节点，具有两个或多个计算节点，通过leader节点来协调。ADB与此相比，ADB引入读写分离的架构，具有多个读写节点且是独立的，并且有一系列协调器节点与它们通信。<strong>Google BigQuery</strong>是Google核心技术（<strong>Dremel</strong> [31]）的外部实现，采用高存储率的列式存储、树形拓扑结构分发查询、秒级内跨数千个节点聚合结果。<strong>和它不同的是，ADB采用了索引引擎和DAG执行框架</strong>。</p>
<h4 id="3-系统设计"><a href="#3-系统设计" class="headerlink" title="3. 系统设计"></a>3. 系统设计</h4><p>作为一个云数据库，ADB运行在Apsara（<strong>飞天</strong>）上，它是阿里云自2009年开始开发的大型通用高可靠性计算基础设施。Apsara管理数万物理机器的所有资源，维护多个阿里云服务，包括检索、计算和存储。ADB采用了Apsara两个核心组件，分别为Pangu(盘古，可靠的分布式存储系统)和Fuxi（伏羲，资源管理和作业调度），如下图一所示。本章节，我们将给出ADB的关键技术选型，包括数据模型和系统架构。</p>
<p><img src="/blog/e3ee66c7/adb_archi.png" alt></p>
<center> 图1 ADB架构</center>


<h5 id="3-1-数据模型和查询语言"><a href="#3-1-数据模型和查询语言" class="headerlink" title="3.1 数据模型和查询语言"></a>3.1 数据模型和查询语言</h5><p>ADB遵循标准的关系数据模型，即数据记录有固定的模式。主流的复杂类型，像JSON、Vector和Text等，需要支持来满足实际应用日益增长的分析需求。ADB支持ANSI SQL 2003，以及增强了一些额外功能，比如分区规范、复杂类型的数据操作。</p>
<h5 id="3-2-表分区"><a href="#3-2-表分区" class="headerlink" title="3.2 表分区"></a>3.2 表分区</h5><p>在ADB中，每张表都有两级分区，即一级分区和二级分区。如下一个DDL SQL样例所示，创建一个有两级分区的表。一级分区在字段<code>id</code>上有50个分区，二级分区在字段<code>dob</code>上有12个分区。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> db_name.table_name (</span><br><span class="line">	<span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line">	city <span class="built_in">varchar</span>,</span><br><span class="line">	dob <span class="built_in">date</span>,</span><br><span class="line">	primary <span class="keyword">key</span> (<span class="keyword">id</span>)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">HASH</span> <span class="keyword">KEY</span>(<span class="keyword">id</span>) <span class="comment">-- 散列到不同节点cluster by</span></span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">NUM</span> <span class="number">50</span></span><br><span class="line"><span class="keyword">SUBPARTITION</span> <span class="keyword">BY</span> <span class="keyword">LIST</span> (dob) <span class="comment">-- 节点内部的划分 partition by</span></span><br><span class="line"><span class="keyword">SUBPARTITION</span> OPTIONS (available_partition_num = <span class="number">12</span>);</span><br></pre></td></tr></table></figure>
<p>一级分区基于用户指定的列进行hash，因此所有行被分布到所有一级分区中来最大化并发度。实际上，任何高基数的列（NDV大）都可以作为分区列，这样可以使每个分区均衡。同时，用户还可以设置二级分区（可选的），二级分区是一个列表分区，设置了最大分区数为12，用于自动数据保存和回收。通常，表示时间间隔的字段（如，天、周、月）作为二级分区字段，可以将同一个时间间隔的数据分到同一个分区。一旦分区的数量超过指定阈值，就会自动将最旧的分区剔除。</p>
<h5 id="3-3-总体架构"><a href="#3-3-总体架构" class="headerlink" title="3.3 总体架构"></a>3.3 总体架构</h5><p>如图1所示的系统架构，ADB中节点共有三种类型，协调器、写节点和读节点。客户端通过JDBC/ODBC连接发送请求（读写），协调器负责接收并分发到相应的读写节点。写节点负责处理写请求（如INSERT、DELETE、UPDATE）和将SQL描述持久化到Pangu。读节点负责处理查询请求（如SELECT）。在这种方式下，读写节点是相互分离的。Fuxi将利用所有节点中可利用的资源为异步任务执行提供计算worker，此外ADB的pipeline执行引擎（如下图2所示）就运行在计算worker上。数据以列block为单位（称为page）从存储流向客户端。所有数据的处理都在内存中，通过网络在不同阶段之间进行pipeline连接。这个pipeline工作流以高吞吐和低延时提供用户的复杂查询。</p>
<p><img src="/blog/e3ee66c7/pipeline_engine.png" alt></p>
<center>图2 pipeline执行引擎</center>


<h5 id="3-4-读写分离"><a href="#3-4-读写分离" class="headerlink" title="3.4 读写分离"></a>3.4 读写分离</h5><p>传统的OLAP将读写合在一起，即一个数据库实例在同一个执行流程中处理所有请求，不区分读还是写。因此，<strong>所有并发的请求共享一个资源池会相互影响。当读写并发都很高的情况下，由于资源竞争会导致较低的性能</strong>。为了解决这个问题，我们提出了一个读写分离的架构。写节点负责写，读节点负责读，读写节点相互隔离，使得读写完全在不同流程中执行。</p>
<h6 id="3-4-1-高吞吐写"><a href="#3-4-1-高吞吐写" class="headerlink" title="3.4.1 高吞吐写"></a>3.4.1 高吞吐写</h6><p>write节点中选择一个作为master节点，其他作为worker节点，它们之间通过Zookeeper[24]的锁服务相互协调。当写节点初次启动，master会配置表分区到各个worker上。基于配置，coordinators会将写请求分发到相应的workers。当一个写请求到达，coordinator会解析SQL，识别为写入操作，然后派发到相应的write节点。针对接收到的SQL，每个write节点作为memory buffer，然后周期性地写入日志到Pangu（跟传统数据库日志写入线程类似）。一旦buffer完成flush，这个node就会返回一个版本号（log seq num）给coordinators，然后针对每个提交的写入返回用户一个成功的消息。</p>
<h6 id="3-4-2-实时读"><a href="#3-4-2-实时读" class="headerlink" title="3.4.2 实时读"></a>3.4.2 实时读</h6><p>每个读取节点都由协调器分配若干个分区，其中具有相同哈希值的分区是放置在一个节点中。如下图3所示：</p>
<p><img src="/blog/e3ee66c7/data_place_in_read_node.png" alt></p>
<center>图3 读节点中的数据放置</center>


<p>分区在读节点中的位置，利用存储感知优化器，这种布局有助于节省数据重新分布的成本超过80%，这是从我们生产服务中测量的。而且，为了并发和可靠性，读节点是可以被复制的。每个读节点从Pangu加载初始分区，然后周期性地从相应的写节点拉取后续的更新。然后，将更新应用在本地数据副本，这些副本不会写回Pangu中。我们选择持续从写节点拉取数据而不是Pangu，是为了减少同步的延迟。因此，写节点作为缓存提供不同读节点副本并发拉取更新数据。</p>
<p>由于近期写入的数据读节点需要远程拉取，因此读节点给用户提供两种可见性级别：一是<strong>实时读</strong>，数据写入后可以立即读到；二是<strong>有界过时读</strong>，在一定的延时内数据是可见的。<strong>为了保证查询的低延时，默认采用第二种方式，在大部分OLAP场景下是可以接受的</strong>。对于实时性要求高的用户，可以开启实时读，不过可能引发读写节点数据同步的问题。</p>
<p>为了解决这个问题，我们采用了<strong>版本验证机制</strong>。具体来说，在写节点中每个一级分区都有它自己的版本。在分区上多个写入请求被flush后，写节点将增加分区的版本并附加到响应消息中。如下图4所示，拿读写请求流程作为例子。</p>
<p><img src="/blog/e3ee66c7/read_write_seq.png" alt></p>
<center>图4 实时读流程</center>


<p>一个用户写入一条记录到表里（步骤1和2），立刻下发查询检索数据。当协调器收到这个请求，将查询和上一次flush响应（有界延时读或从写节点实时拉取，步骤3）的版本（V1）缓存都发送到相应的读节点（步骤4）。针对每个分区，读节点将本地版本（标记为V2）与V1版本比较。如果版本V1没有V2大，则读取节点直接执行查询操作。否则，读节点必须从写节点拉取最新的数据（步骤5），优先更新本地副本。</p>
<p>遵循上述的操作，针对实时查询，我们可以确保读写节点之间数据的可见性。然而，如果读节点向写节点发送拉取请求，需要等待所需的数据，这个延迟将会比较高。我们这里进行了优化，将读节点拉取改为了写节点推送。当写节点监测到有新写入的数据时，将主动附上版本号推送给相应的读节点。</p>
<h6 id="3-4-3-可靠性和可扩展性"><a href="#3-4-3-可靠性和可扩展性" class="headerlink" title="3.4.3 可靠性和可扩展性"></a>3.4.3 可靠性和可扩展性</h6><p>ADB为读写节点提供了高可靠性。针对写节点，当worker失败时，master会平滑地将该worker上的分区分发给其他可用的写节点。当master失败时，会从活跃的workers中选举出一个新的master。</p>
<p>针对读节点，用户可以指定副本因子（默认为2），同一个节点的不同副本可以部署在不同的物理机器上。当一个读节点在执行查询时失败，协调器会自动地重新发送查询给其他副本，这对用户来说是透明的。注意当读节点从写节点拉取数据时出现失败，读节点是不会被阻塞的。如果读节点不能访问写节点，它们将直接从Pangu（更高的延迟）中读取数据，继续执行查询（步骤6）。</p>
<p>ADB也可以保证读写节点的高可扩展性。当加入一个新的写节点时，<strong>master</strong>将会调整表分区的位置来保证负载均衡。新的位置被更新到zookeeper，然后协调器会根据新的信息来发送后续的写请求。读节点的扩展是类似的，除了表分区位置是通过<strong>coordinators</strong>调整的。</p>
<h5 id="3-5-集群管理"><a href="#3-5-集群管理" class="headerlink" title="3.5 集群管理"></a>3.5 集群管理</h5><p>ADB的集群管理支持多租户，也就是说在一个集群中有多个ADB实力。我们设计并实现了一个集群管理组件Gallardo，利用CGroup技术隔离不同ADB的实例的资源（CPU、内存、网络带宽），来保证它们的稳定性。当一个新的ADB被创建，Gallardo会分配它所需要的资源。在分配期间，Gallardo会谨慎地将不同的角色（协调器、写节点和读节点）和读节点副本放置到不同的物理机器上，来遵循可靠性的要求。<strong>注意这里Gallardo和Fuxi是不冲突的，Gallardo负责为不同ADB实例分配和隔离资源，而Fuxi是为计算任务使用所有ADB实例的可用资源</strong>。</p>
<h4 id="4-存储"><a href="#4-存储" class="headerlink" title="4. 存储"></a>4. 存储</h4><p>ADB的存储模型支持结构化数据和其他复杂数据类型，比如JSON和向量。我们首先讨论混合行列存储结构，其次是它快速和强大的索引引擎。</p>
<h5 id="4-1-物理数据结构"><a href="#4-1-物理数据结构" class="headerlink" title="4.1 物理数据结构"></a>4.1 物理数据结构</h5><p>本章节首先描述ADB数据结构和元数据结构，然后说明数据是如何管理的。</p>
<h6 id="4-1-1-混合行列存储"><a href="#4-1-1-混合行列存储" class="headerlink" title="4.1.1 混合行列存储"></a>4.1.1 混合行列存储</h6><p>ADB设计的一个主要目标是支持OLAP和精确查询。OLAP的查询一般会涉及一个宽表中的部分列，列存比较适合这样的查询，由于它高效的数据压缩和IO减少。但对于精确查询是比较困难的，因为这类查询需要返回一个或多个整行。行存在精确查询中比较适合，但是针对OLAP查询访问成本增大了很多。为了解决这个问题，我们提出了行列混合存储布局，如下图5所示。</p>
<p><img src="/blog/e3ee66c7/hybrid_row_column_storage_layout.png" alt></p>
<center>图5 包含元数据和索引的混合行列存储数据格式</center>

<p>在这个设计中，每个表分区的数据都维护在一个单一的文件中（称为detail file），内部分为多个行组，每个行组包含固定行数（生产环境默认为30000，是可配置的）。在一个行组中，同一列的所有值是连续的且分组在一个数据块（data block）中，所有的数据块按序存储。数据块是ADB中基本的操作单元（拉取和缓存），有助于获得较高的压缩比来节省存储空间。像这样的混合设计能够在可接受的工作负载下，平衡OLAP和精确查询[12, 20, 34]。和列存类似，混合存储也会根据列来划分数据，有助于ADB的OLAP查询。虽然一整个列属于不同行组的多个数据块中，仅仅有一小部分顺序检索要求获取所有数据。通过我们对真实ADB服务的观察，这个负载占比小于整个查询延时的5%。针对精确查询，为了保留好的性能，将一行的所有列存储在同一个行组当中。行集合只涉及短距离顺序查找[23]，而不是列存中的跨段查找。</p>
<p><strong>复杂类型数据</strong>。混合行列存储适合较小的列，例如数值型和短字符类型，但不适合复杂类型数据（比如JSON和向量），因为这些数据大小可变和通常都比较大。如果把这些行分为固定数量的行组会导致不可预期的大数据块。为了解决这个问题，针对复杂类型数据设计了一个固定大小的存储模型。利用另外一个级别的块，名为FBlock，固定大小为32KB。特别地，一个含有30000行的数据块，会进一步拆分为多个FBlocks，并存储指向这些FBlocks的指针。在这个方式下，数据块还是固定行数，所有的FBlocks都存在一个单独的文件中，如下图6所示：</p>
<p><img src="/blog/e3ee66c7/fblocks.png" alt></p>
<center>图6 复杂类型数据格式</center>

<p>然而，一个FBlock中包含的行数各有不同，少于一行（即部分行）到多行。为了支持快速检索，我们在datablock中为每个FBlock维护了一个block entry，每个entry包含两个标识符，即对应FBlock的起始行和结束行。一行被切分为多个连续的FBlocks。图中，FBlock1和FBlock2分别存储0-99行和99-200行，同时第99行被分为两个FBlock。为了访问到该行，需要首先从数据块中扫描block entrie定位到FBlock1和FBlock2，然后提取和合并其中的部分行。</p>
<h6 id="4-1-2-元数据"><a href="#4-1-2-元数据" class="headerlink" title="4.1.2 元数据"></a>4.1.2 元数据</h6><p>在detail文件中每个列都有自己的元数据信息，用于加速在这个列上进行海量数据的检索。这些为每个列单独存储元数据的文件，称为detail meta文件（如图六所示），它的大小非常小，一般小于1MB，由于频繁访问一般缓存在内存中。每列的元数据由四个部分组成：</p>
<ul>
<li>header: 包含一个版本号和detail meta文件总大小；</li>
<li>summary：包含查询优化需要的统计信息，如行数、NULL数量、NDV、sum、max和min；</li>
<li>dictionary：对于ndv数较低的列，将会自动开启字典功能，来节省空间。还包含在文件中的偏移量和长度用于快速访问；</li>
<li>block map：持有每个data block的entry，包含在文件中的偏移量和长度用于快速访问。</li>
</ul>
<h6 id="4-1-3-数据操作"><a href="#4-1-3-数据操作" class="headerlink" title="4.1.3 数据操作"></a>4.1.3 数据操作</h6><p>ADB底层存储采用Lamda架构，如图7所示，包含基线数据和增量数据。基线数据存储历史数据，包括索引和行列数据。增量数据保持新写入的数据，不包含全部索引只是一个简单的排序索引。增量数据仅仅在读节点上出现，当它们从写节点拉取并重放日志的时候。基线和增量数据遵循相同的数据格式和和元数据格式。</p>
<p><img src="/blog/e3ee66c7/query_exec.png" alt></p>
<center>图7 在存储之上的操作和查询执行</center>

<p>查询执行**。为了支持update我们采用bit-set来记录要删除数据的row-ids。通过Copy-on-Write技术来实现MVCC（多版本并发控制）[15]。当一行数据被更新或删除时，一个带有版本的bit-set快照保存在内存map中用于后续的查询。这个用于delete的bit-set被划分为多个小的经过压缩的segment，使得快照可以共享那些没有变化的segment，提高空间利用率。此外，新版本的快照被创建，一旦没有查询，旧版本的快照将被删除。算法1、2、3描述了在基线数据和增量数据上执行INSERT、DELETE和过滤查询，流程见图七。为了查询，首先根据给定的版本号，来获取相应的基线数据和增量数据的删除标记位bit-set快照。基线数据可以从全量索引中获取限定的row-ids，增量数据可以从排序索引中获取限定的row-ids。最后，我们会从bit-set中过滤出要删除的行获得最后的结果。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">算法一 INSERT(SQL, version)</span><br><span class="line">--------------------------------------------------</span><br><span class="line">输入：SQL语句和版本号</span><br><span class="line"><span class="comment">//从sql中解析多个列的值values</span></span><br><span class="line">values = parse(SQL);</span><br><span class="line"><span class="comment">//将values追加到增量数据的尾部</span></span><br><span class="line">row_id = incremental_data.append(values);</span><br><span class="line"><span class="comment">//在delete_bits新增一个新的bit</span></span><br><span class="line">delete_bitset[row_id] = <span class="number">0</span>;</span><br><span class="line"><span class="comment">//为delete_bitset创建快照</span></span><br><span class="line">delete_bit_snap = create_snap(delete_bitset);</span><br><span class="line"><span class="comment">//以快照版本作为key，存入snap_map中</span></span><br><span class="line">snap_map.put(version, delete_bitset_snap);</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">算法二 DELETE(SQL, version)</span><br><span class="line">--------------------------------------------------</span><br><span class="line">输入：SQL语句和版本号</span><br><span class="line"><span class="comment">//根据where条件检索row_ids</span></span><br><span class="line">row_ids = search(baseline_data, incremental_data, sql.where)</span><br><span class="line"><span class="comment">//遍历delete_bitset,将命中的row_ids剔除</span></span><br><span class="line"><span class="keyword">for</span> row_id in row_ids <span class="keyword">do</span></span><br><span class="line">    delete_bitset[row_id] = <span class="number">1</span>;</span><br><span class="line"><span class="comment">//为delete_bitset创建快照</span></span><br><span class="line">delete_bit_snap = create_snap(delete_bitset);</span><br><span class="line"><span class="comment">//以快照版本作为key，存入snap_map中</span></span><br><span class="line">snap_map.put(version, delete_bitset_snap);</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">算法三 FILTER(conditions, version)</span><br><span class="line">--------------------------------------------------</span><br><span class="line">输入：过滤条件和版本号</span><br><span class="line">输出：命中的row_ids</span><br><span class="line">delete_bitset_snap = snap_map.get(version);</span><br><span class="line">row_ids = search(baseline_data, incremental_data, conditions);</span><br><span class="line"><span class="keyword">return</span> minus(row_ids, delete_bitset_snap);</span><br></pre></td></tr></table></figure>
<p><strong>基线数据和增量数据合并</strong>。随着新数据持续写入，在increment_data上的检索会变得很慢。因此，build进程异步启动来讲增量数据合并到基线数据。在build过程中，忽略删除的数据和创建新的索引。如图8所示，合并的过程如下：</p>
<p><img src="/blog/e3ee66c7/merge_procedure.png" alt></p>
<center>图8 基线和增量数据合并处理</center>

<p>当合并开始，增量数据置为不可变，创建另外一个新的增量数据实例，来接收新数据。在数据合并完成之前，所有查询都基于基线数据、老增量数据和新增量数据。一旦新版本的基线数据合并完成，老的基线数据和老的增量数据将被安全地移除。此时，后续的查询都是基于新的基线数据和新的增量数据。</p>
<h5 id="4-2-索引管理"><a href="#4-2-索引管理" class="headerlink" title="4.2 索引管理"></a>4.2 索引管理</h5><p>在所有数据库中，索引是很关键的组件，可以用来提高查询性能。然而，已有的一些索引方案，并不能很好地满足OLAP的查询需求。例如，由于节点切分，B+树的更新代价高昂，因此只能在精心选择的列上使用。像Druid这样的系统，采用基于位图的倒排索引，在更多的列上构建，但是只适用于简单类型（如string）。越来越多的查询需求需要支持复杂数据类型（如json、vector、text等），这些数据类型的索引也需要支持创建。而且，大部分系统都是在写入过程中[8,6,5,10]构建索引，会极大地限制写入性能。</p>
<p>因此，我们设计并实现了一个索引引擎，支持结构化和复杂数据类型的数据创建索引，并且不影响写入性能。该引擎可以在所有列上建立索引，全部支持ad-hoc查询，并且将索引构建从写入流程中移出来。许多复杂的设计，就是为了最小化存储负载和最大化性能。</p>
<h6 id="4-2-1-索引过滤"><a href="#4-2-1-索引过滤" class="headerlink" title="4.2.1 索引过滤"></a>4.2.1 索引过滤</h6><p>在一个分区中每个列创建一个倒排索引，存储在一个单独的文件中。倒排索引中，key是字段原始值，value为相应行号列表。根据4.1.1章节，我们可以很容易通过行号定位到一行，因为每个行组的数量是固定的。</p>
<p>基于每个列上的索引，ADB可以支持高性能的ad-hoc查询。图9给了一个SQL过滤的例子，过滤条件包含结构化数据和复杂类型数据。对于每个条件，索引引擎基于相应的索引进行过滤获取结果集（也就是row ids）。最后，所有的row ids通过交、并、差的操作，合并成最终的结果。大部分数据库都是采用二路归并来合并结果，该方式需要耗费大量内存且并发度低。为了减轻影响，我们采用了K路归并来合并数据，来达到在大数据集下的亚秒级查询延迟。</p>
<p><img src="/blog/e3ee66c7/query_over_index.png" alt></p>
<center>图9 全字段索引查询</center>

<p><strong>索引路径选择。</strong>然而，在所有列上的索引过度使用反而会降低查询的性能。例如，有A and B这个条件，A的过滤的结果远小于B过滤的结果，则应该先过滤A再过滤B，而不是把A和B结果都过滤出来再合并。为了解决这个问题，我们提出了一个基于运行时过滤选择率索引路径选择机制，通过评估每个条件的选择率来决定是否采用该索引，这个选择率=根据索引限定的行数/总行数。ADB选择索引来过滤条件是根据索引选择率降序来排的。如果所有的处理条件（比率相乘）的联合过滤比足够小（比如总行数1%），这个过程停止和之前获取的结果进行K路合并。后续的条件直接基于row ids过滤而不是索引。</p>
<h6 id="4-2-2-复杂数据类型索引"><a href="#4-2-2-复杂数据类型索引" class="headerlink" title="4.2.2 复杂数据类型索引"></a>4.2.2 复杂数据类型索引</h6><p><strong>JSON</strong>。当插入一个json对象，会将层级结构的json属性扁平化为多个字段，为每个字段构建倒排索引。例如，给定一个json对象<code>{id, product_name, properties{color, size}}</code>，扁平化后的字段为<code>id, product_name, properties.color, properties.size</code>，为每一个列构建一个索引。我们将采用<code>PForDelta</code>算法[39]来压缩每个索引下的row ids。而且，一个json对象可能包含上千个属性（也就是上千个索引）。我们将一个json对象的所有索引打包在一个单独的文件中，来限制文件的数量。<strong>利用这个索引，ADB能够直接以json格式的谓词条件来获取json对象</strong>，比起从磁盘上直接读取和解析json数据块要更加高效。</p>
<p><strong>Full-Text</strong>。针对全文数据，ADB通过存储更多的信息来扩展倒排索引，包括词频、doc和term的映射。我们使用<code>TF/IDF</code>公式打分来计算查询和文本之间的相似度，只有那些排名在设置阈值前面的记录才会返回给用户。</p>
<p><strong>Vector Data</strong>。特征向量是计算视觉任务中常见的组件，如物体和场景识别、机器学习中用到的高维向量，是通过训练的AI模型从图片中提出来的。<strong>两个物体之间的相似度是通过计算特征向量距离来度量的</strong>。在向量数据查询中，用户要求最近邻搜索（NNS），为了能够找到数据库中和该查询点最相近的对象。（部分内容跳过…）</p>
<h6 id="4-2-3-索引空间节省"><a href="#4-2-3-索引空间节省" class="headerlink" title="4.2.3 索引空间节省"></a>4.2.3 索引空间节省</h6><p>我们采用自适应方式来减少索引的大小。针对索引中的每个key_value, 根据空间消耗来选择使用bitmap还是integer数组来存储value。例如，索引的值为[1, 2, 8, 12],bitmap（2个字节）比起integer数组（4个字节）要好。但如果是[1, 12, 35, 67]，integer数组（4个字节）要比bitmap（9个字节）要好。通过采用这种方式，整个索引大小可以减少50%。我么也可以允许关闭指定列的索引，通过延时来换取空间。</p>
<h6 id="4-2-4-异步索引构建"><a href="#4-2-4-异步索引构建" class="headerlink" title="4.2.4 异步索引构建"></a>4.2.4 异步索引构建</h6><p>ADB需要支持每秒上千万写请求，因此不可以在写请求的过程中创建索引。换种方式，就是要索引引擎异步构建索引。回忆一下3.4.1章节，在写请求结束后，写节点会将写日志flush到pangu。索引引擎会周期性为新写入的数据（increment data）构建倒排索引，之后在后台把它们合并到存在的全量索引中。这种异步的方式完全屏蔽了从用户侧构建的工作负载，保证了查询性能和写入吞吐。构建和合并索引会被翻译为许多的Map-Reduce任务，在借助于Fuxi[38]，在非高峰期并发地、自动地执行这些任务，达到可接受的工作负载。</p>
<p>表1给出了ADB和Greenplum（一个列存的OLAP系统）在1 TB数据上构建全列索引的对比。我们可以看到ADB只使用了0.66 TB的额外空间存储索引，而Greenplum使用了2.17 TB的大小。虽然ADB构建索引花了双倍的时间，但异步的处理方式并不会影响在线读写性能。如表1所示，在Greenplumn中实时INSERT 1 TB数据时间大约是ADB的4倍。因此，ADB为了ad-hoc查询的较大性能提升（第6章节具体说明），在交换数据过程中产生了一些开销。</p>
<table>
<thead>
<tr>
<th></th>
<th>ADB</th>
<th>GP</th>
</tr>
</thead>
<tbody>
<tr>
<td>索引空间</td>
<td>0.66 TB</td>
<td>2.71 TB</td>
</tr>
<tr>
<td>索引构建时间</td>
<td>1 h</td>
<td>0.5 h</td>
</tr>
<tr>
<td>是否异步</td>
<td>是</td>
<td>否</td>
</tr>
<tr>
<td>数据实时INSERT时间</td>
<td>4015 s</td>
<td>20910 s</td>
</tr>
</tbody>
</table>
<center>表1 ADB和GP在1TB数据上构建全列索引的对比</center>

<h6 id="4-2-5-增量数据索引"><a href="#4-2-5-增量数据索引" class="headerlink" title="4.2.5 增量数据索引"></a>4.2.5 增量数据索引</h6><p>采用异步索引的方式会带来一定的性能差距，在新索引上线之前，增量数据缺乏索引的支持，因此需要scan数据提供高延迟的查询。<strong>为了取消这个差距，索引引擎在读节点上为增量数据独立构建sorted index</strong>。这个sorted index在数据块中其实就是一个row id的数组。如图10所示，一个升序排列的索引，第i个元素Ti表示在数据块中第i小的值在行Ti。</p>
<p><img src="/blog/e3ee66c7/sorted_idx.png" alt></p>
<center>图10 增量数据的排序索引</center>

<p>因此，在增量数据上的查找转为了二分查找，复杂度从O(n)将O(logn)。为了保存sorted index，我们在每个数据块中开辟了附加的header信息。由于一个数据块的行数为30000行，因此行号为短整型。header（排序索引）的大小，大约是60 KB。在flush数据之前，索引引擎构建sorted index，并dump到文件的header中。这个构建过程在读节点本地执行，是非常轻量的。</p>
<h6 id="4-2-6-条件索引缓存"><a href="#4-2-6-条件索引缓存" class="headerlink" title="4.2.6 条件索引缓存"></a>4.2.6 条件索引缓存</h6><p>传统数据库在内存中缓存索引（粒度为index page）是为了减少磁盘IO。查询条件缓存将查询条件（例如id&lt;23)作为key，查询结果作为值（即row ids)。因此，可以完全避免在index page上重复过滤。当查询条件缓存失效了，我们可以在index page缓存中访问索引来计算查询结果。</p>
<p>在采用两层缓存策略中存在一个挑战就是用户条件持续变化且相差很大，导致缓存频繁失效。然而，根据我们的观察，对整个缓存的有效性影响不是很大。一个是大查询结果的条件比较少且变化不频繁（比如where city=’Beijing’），所以它们的缓存能够持续较长时间；第二个小查询结果的条件比较多且变化较大（比如where userid=’xxx’)，但是它们的查询结果重新构建，代价较小。总之，计算成本较高的结果可以很好地被缓存来节省资源，轻量级的查询重新计算不会带来太多额外开销。可以保证索引缓存的有效性。</p>
<h4 id="5-优化器和执行引擎"><a href="#5-优化器和执行引擎" class="headerlink" title="5. 优化器和执行引擎"></a>5. 优化器和执行引擎</h4><p>本章节，我们将讨论优化器和执行引擎采用各种新式的优化方式，来进一步优化查询延时和并发。</p>
<h5 id="5-1-优化器"><a href="#5-1-优化器" class="headerlink" title="5.1 优化器"></a>5.1 优化器</h5><p>ADB优化器提供了CBO（基于成本的优化）和RBO（基于规则的优化），目标是实时在线分析，具备低延时和高并发。它包含了大量的关系代数转换规则，保证总是能够选择到较优的计划。这些规则包括：</p>
<ul>
<li>基础优化规则（如裁剪、下推/合并、重复消除、常量折叠/谓词派生）；</li>
<li>针对不同JOIN的probe优化规则（广播HASH JOIN、重分布HASH JOIN、嵌套循环Index JOIN），Aggregate，Join-Reorder，GroupBy下推，Exchage下推，SortBy下推等；</li>
<li>高级优化规则（如Common Table Expresssion，即with clause的使用）。</li>
</ul>
<p><strong>除了上述提到的通用RBO和CBO规则，还开发了两个关键特性：存储感知优化和高效实时采样</strong>。</p>
<h6 id="5-1-1-存储感知计划优化"><a href="#5-1-1-存储感知计划优化" class="headerlink" title="5.1.1 存储感知计划优化"></a>5.1.1 存储感知计划优化</h6><p><strong>Predicate下推</strong>。谓词（即条件）为了将SQL中的关系代数计算抽取出来，充分利用底层存储的特性，将查询计划转为两个等价的部分（分别针对计算层和存储层）。因为在原始的查询计划中，没有清晰的边界来支持这种操作，完全依赖于优化器。<font color="red">在许多分布式数据库中已经实现谓词下推，但主要集中在单列的AND操作上</font>。他们并没有考虑其他通用操作，比如function和join，这些一般在计算层实现。这是因为许多已有的数据库并没有为存储层提供接口来注册高级的能力。因此，存储仅仅能够做到单个列或列组合的过滤。</p>
<p>ADB引入STARS（策略替换规则）框架[30, 14]，来扩展优化器支持谓词下推，如图11所示。STARS针对查询执行提供了高级的、声明式的、独立于实现的合法性策略。每个STAR定义了一系列高级的构建，来自低级的数据库操作或其他STARS。基于STARs框架，ADB从关系代数的维度，抽象出异构数据源的能力，将存储能力描述为可用的关系代数。另外，ADB也会提供成本计算。执行谓词下推，不仅仅依赖于存储的能力，也要考虑关系代数的能力成本。<strong>在动态规划的过程中，成本和执行能力都会作为参考因素，避免盲目的谓词下推带来性能衰减</strong>。这在低延时和高并发的环境中，是非常重要的。在优化器完成初始的分布式执行计划之后，作用在目标数据源上的关系代数算子会通过动态编程的方式进行封装，转为相应的存储API的调用。</p>
<p><img src="/blog/e3ee66c7/adb_stars.png" alt></p>
<center>图11 ADB的STARS框架</center>

<p><strong>Join下推</strong>。<strong>数据重新分布</strong>是分布式数据库执行计划的另一个重要方面。这个不同于传统数据库，主要是因为物理数据分布特征和关系代数的逻辑语义之间的不匹配。例如，SQL语句<code>SELECT T.tid, count(*) FROM T JOIN S ON T.sid = S.sid GROUP BY T.tid</code>，T和S基于同一个字段来hash，分区放置在同一个读节点上（如3.4.2章节）。ADB能够选择到最好的JOIN下推策略。避免数据的重新分布式是非常重要的，因为重分布的代价非常高，比如涉及序列化、反序列化以及网络负载等。如果T和S不是基于同一个字段hash，则ADB会清楚的知道shuffle哪个表是最高效的，通过获取T和S底层存储的大小。正如上面提到的，优化器扩展和计算了所有可能执行计划的成本，ADB通过这种方式来选择适合不同数据量下的，适合于数据特点的最优执行计划。</p>
<p><strong>基于Index的Join和Aggregation</strong>。在所有列上构建索引，可以减少构建hash索引的开销，通过查找已存在的索引来取代。当调整Join的顺序后，优化器可以避免构建Bushy Tree，倾向于Left Deep Tree，这样ADB可以更好地使用索引，如图12所示。而且，我们也会下推谓词和聚合。例如，count的聚合操作可以直接从索引返回，filter也可以在索引上直接计算。所有的这些优化，都可以降低查询的延时，来提升集群利用率，使得ADB更容易支持高并发。</p>
<p><img src="/blog/e3ee66c7/join_order.png" alt></p>
<center>图12 Join Order优化</center>

<h6 id="5-1-2-高效实时采样"><a href="#5-1-2-高效实时采样" class="headerlink" title="5.1.2 高效实时采样"></a>5.1.2 高效实时采样</h6><p>成本估算是CBO的基石，同时又取决于基数估算，基数估算重度依赖可获得的统计信息。在现代数据库中，统计信息的收集和使用非常有限，使得数据倾斜和相关性得不到很好的处理，从而导致获取次优的查询计划。另外，我们的系统设计目标之一是查询（简单查询或复杂查询）短时间响应，传统的做法（<strong>实时统计、谓词选择率、执行结果反馈</strong>）由于负载和复杂度显得不太适合。取而代之，<strong>我们实现了一个基于高效采样的cardinality估算框架</strong>。我们的框架，充分利用了ADB高性能存储引擎提供的高效数据访问，以及通过丰富的索引类型、缓存和优化计算进行估算。在优化的时候，优化器通过框架API发送采样谓词请求（单个或多个，取决于优化器）给存储引擎。存储引擎通过适当的索引或缓存来访问采样数据，通过优化后的计算路径来估算谓词，返回基数结果。优化器利用采样的基数结果来估算候选的执行计划，选择其中最优的一个。</p>
<p>虽然我们的框架可以很高效地估算基数，但是进一步的优化可以减少负载，尤其是那些关键的业务场景，需要亚秒级查询。<strong>这些优化包含了缓存预先采样（基数估算）、优化的采样算法、改进的derived基数等等</strong>。通过采样这些方式，我们的cardinality估算框架在估算的时候，可以降低负载以及提供更高的估算准确度。</p>
<h5 id="5-2-执行引擎"><a href="#5-2-执行引擎" class="headerlink" title="5.2 执行引擎"></a>5.2 执行引擎</h5><p><strong>ADB提供了一个通用的、管道模型的执行引擎，以及在这个引擎之上的DAG[27]（有向无环图）执行框架。因此，它适合于小规模（低延时）和大规模（高吞吐）的workloads</strong>。ADB的执行引擎是面向列的，充分利用了底层存储引擎将数据按列进行聚集的特点。与基于行存的执行引擎相比，向量化引擎是cache友好的，不会load一些没有必要的数据到内存。</p>
<p>像许多OLAP系统，CodeGen[32]被用于提升CPU密集型操作的执行性能。ADB的CodeGen是基于ANTLR ASM，为Expression Tree动态生成代码。这个CodeGen引擎也会考虑运行时因素，允许我们在任务级别的粒度上，充分利用异构硬件的能力。例如，在向量化引擎中，大部分数据类型为int或double。在异构集群中，<strong>有支持AVX-512指令集的CPU，我们能够使用SIMD指令生成字节码来提升性能</strong>。而且，<strong>通过固化存储引擎和执行引擎之间内部数据表现形式</strong>，这样ADB能够在序列化后的二进制数据上直接操作而不是JAVA对象，这样有助于减少序列化和反序列化的开销，当shuffle大量数据时，可以节省20%的时间。</p>
<h4 id="6-实验评估"><a href="#6-实验评估" class="headerlink" title="6. 实验评估"></a>6. 实验评估</h4><p>本章节，我们将从实际workload和TPC-H基准测试[11]来评估ADB，并给出不同查询类型和写入能力下的ADB性能。</p>
<h5 id="6-1-实验设置"><a href="#6-1-实验设置" class="headerlink" title="6.1 实验设置"></a>6.1 实验设置</h5><p>实验集群由8台物理机构成，每台机器配置如下表2所示。集群上，启动4个coordinators，4个write节点，32个read节点。</p>
<table>
<thead>
<tr>
<th>配置</th>
<th>参数说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>CPU</td>
<td>Intel Xeon Platinum 8163 CPU(@2.50GHz)</td>
</tr>
<tr>
<td>RAM</td>
<td>300GB MEM</td>
</tr>
<tr>
<td>DISK</td>
<td>3TB SSD</td>
</tr>
<tr>
<td>NIC</td>
<td>10Gbps Ethernet network</td>
</tr>
</tbody>
</table>
<center>表2 物理机配置</center>

<table>
<thead>
<tr>
<th>类型</th>
<th>查询语句</th>
</tr>
</thead>
<tbody>
<tr>
<td>Full Scan (Q1)</td>
<td>SELECT * FROM orders ORDER BY o_trade_time LIMIT 10</td>
</tr>
<tr>
<td>Point Lookup (Q2)</td>
<td>SELECT * FROM orders WHERE o_trade_time BETWEEN ‘2018-11-13 15:15:21’ AND ‘2018-11-13 16:15:21’ AND o_trade_prize BETWEEN 50 AND 60 AND o_seller_id=9999 LIMIT 1000</td>
</tr>
<tr>
<td>Multi-table Join(Q3)</td>
<td>SELECT o_seller_id, SUM(o_trade_prize) AS c FROM orders JOIN users ON orders.o_user_id = users.u_id WHERE u_age=10 AND o_trade_time BETWEEN ‘2018-11-13 15:15:2’ AND ‘2018-11-13 16:15:21’ GROUP BY o_seller_id ORDER BY c DESC LIMIT 10</td>
</tr>
</tbody>
</table>
<center>表3 三种查询评估</center>

<p><strong>实际workload</strong>。在我们的生产环境中，使用了两张实际的表。第一个表示<code>users</code>表，使用<code>user_id</code>作为主键，有64个一级分区，没有分二级分区；第二个表为<code>orders</code>表，使用<code>order_id</code>作为主键，有64个一级分区，10个二级分区。这两张表通过<code>user_id</code>进行关联。通过表3给出三种查询进行测试，三种查询都包含<code>o_trade_time</code>，是一个<code>timestamp</code>的类型。由于Druid必须使用<code>timestamp</code>的字段作为分区键。没有指定这个分区键，查询会更慢[36]。</p>
<p><strong>与其他系统对比</strong>。我们将ADB和4个OLAP系统进行对比。PrestoDB[9]，SparkSQL[13]，Druid[36]和GreenPlum[5]。GreenPlum在所有列上建立索引；Druid不支持在数值类型列上建立索引；PrestoDB和SparkSQL采用ORC文件存储数据，任意列上都没有索引。所有系统都运行在默认配置上，其中Druid不支持复杂查询如Join，是TPC-H的大部分查询和表3的Q3无法执行，因此我们跳过这些实验。在整个实验中，concurrency number表示并发查询数量。</p>
<h5 id="6-2-真实workload"><a href="#6-2-真实workload" class="headerlink" title="6.2 真实workload"></a>6.2 真实workload</h5><p>本章节，首先表述在1TB和10TB数据量的查询性能，其次是写入吞吐。</p>
<h6 id="6-2-1-1TB数据查询"><a href="#6-2-1-1TB数据查询" class="headerlink" title="6.2.1 1TB数据查询"></a>6.2.1 1TB数据查询</h6><p>首先，我们在1TB数据上运行表3的3个查询。图13和图14分别给出了AnalyticDB、PrestoDB、Druid、SparkSQL以及GreenPlum的50%和95%的查询延时。如图中可以看到，ADB具有较低的延时，比其他系统至少快一个数据量级。</p>
<p><img src="/blog/e3ee66c7/50_olap_sys_comparison.png" alt></p>
<center>图13 1TB 50%的查询时延</center>

<p><img src="/blog/e3ee66c7/95_olap_sys_comparison.png" alt></p>
<center>图14 1TB 95%的查询时延</center>

<p><strong>Q1</strong>。借助于索引引擎，ADB避免了在全表的scan和sort，这不同于PrestoDB和SparkSQL。特别地，ADB会将Order By和Limit算子分发到含有<code>o_trade_time</code>字段索引的二级分区上。因为索引是有序的，所以每个分区只需要遍历整个索引就可以获取到对应的row ids，只涉及几十个index entries。虽然，GP也在所有列上建立了索引，担不是不能执行Order By算子只能full scan，所以会比ADB慢一些。Druid使用<code>o_trade_time</code>作为range分区[36]，可以在大量的range分区中进行过滤，所以会比GP的性能好一些。但是还是比ADB要慢一些，因为Druid要在每个分区上进行scan所有行。</p>
<p><strong>Q2</strong>。在我们的数据集中，满足条件<code>o_trade_time</code>, <code>o_trade_prize</code>和<code>o_seller_id</code>的行数分别为<code>306,340,963</code>，<code>209,994,127</code>和<code>210,408</code>。在没有索引的情况下，PresotDB和SparkSQL必须scan所有行来进行过滤。Druid和GreenPlum由于在索引列上的快速检索获得了更好的性能。 但是，Druid只能在string类型列上构建索引，GreenPlum虽然可以在所有列上获得索引，但必须按顺序筛选多个条件，并且对于未更改的条件没有缓存。<strong>与它们相比，ADB直接在三个列上并行scan索引，并分别缓存命中的row ids（4.2.6章节）。因此，后续的同样条件的查询，就可以使用条件索引缓存了</strong>。</p>
<p><strong>Q3</strong>。如图13和14，在50%和95%的查询时延上，不同并发下，Q3都要高于Q1和Q2。是因为Q3是复杂的多表join，以及group by和orde by算子的组合。虽然，由于查询较为复杂，时延较高，<strong>但是ADB还是保证了较优的执行。特别地，ADB将join转为了子查询，并使用索引来完成这些子查询。并且，进一步使用索引来执行Group By和Order By算子，避免构建hash表的开销</strong>。GreenPlum比ADB慢，就是因为承担了hash join中的hash表构建的开销。为了公平起见，我们也聘评估了在hash join模式下，ADB能够获得与GreenPlum相当的性能。</p>
<h6 id="6-2-2-10TB数据查询"><a href="#6-2-2-10TB数据查询" class="headerlink" title="6.2.2 10TB数据查询"></a>6.2.2 10TB数据查询</h6><p>我们进一步构建了10TB的数据集，并提高了并发度。这些系统比较下来，在大数据集和更高并发下，比ADB更慢，后面分析我们将跳过这些。</p>
<p><img src="/blog/e3ee66c7/50_adb_1_10_TB.png" alt></p>
<center>图15 ADB在1TB和10TB数据集上50%的时延</center>

<p>如图15所示，说明了ADB在1TB和10TB数据集上三种查询的50%时延。我们可以看到，在不同并发下，Q1和Q2的时延都在百毫秒以内。对于Q3的查询，200并发的时延比40并发要更高。原因是8个机器下的计算能力已经饱和，具体来说，64个一级分区和10个二级分区下，200并发下实际的并发线程数达到了128000个。8个机器上，cpu核数总共为48×8=384个。<strong>因为Q3查询是计算密集型的，遇到高并发时，频繁的上下文切换，导致性能急剧下降</strong>。</p>
<p>从图15中，我们也能看到，在不同并发下10TB数据的变化趋势和1TB变化趋势是类似的。随着数据量的增加，性能受到的影响不是很大。10TB的数据量查询时延仅仅是1TB的两倍，因为ADB优先检索row ids的索引，仅仅需要拉取命中的rows。借助于index cache，索引查找较好的成本收益，并降低了整个开销。<strong>总而言之，ADB受表的大小影响很小，主要受控于索引的计算以及命中的行数</strong>。</p>
<h6 id="6-2-3-写入吞吐"><a href="#6-2-3-写入吞吐" class="headerlink" title="6.2.3 写入吞吐"></a>6.2.3 写入吞吐</h6><p>为了评估ADB的写入性能，我们采用每500字节大小的数据，insert到orders表。表4给出了写入吞吐说明（每秒写入请求）。</p>
<table>
<thead>
<tr>
<th>分类</th>
<th>参数1</th>
<th>参数2</th>
<th>参数3</th>
<th>参数4</th>
<th>参数5</th>
</tr>
</thead>
<tbody>
<tr>
<td>写节点数量</td>
<td>2</td>
<td>4</td>
<td>6</td>
<td>8</td>
<td>10</td>
</tr>
<tr>
<td>写入吞吐</td>
<td>130 KB</td>
<td>250 KB</td>
<td>381 KB</td>
<td>498 KB</td>
<td>625 KB</td>
</tr>
</tbody>
</table>
<p>由于采用读写分离的架构和异步索引构建，写入吞吐随着写入节点的添加而线性增加，直到Pangu达到饱和。当写入节点达到10，写入吞吐是625000，带宽是300 MB/S。索引构建任务是在可以接受的负载下，分配到整个集群执行的，不会影响查询性能和写入吞吐。</p>
<h5 id="6-3-TPC-H基准测试"><a href="#6-3-TPC-H基准测试" class="headerlink" title="6.3 TPC-H基准测试"></a>6.3 TPC-H基准测试</h5><p>我们采用1TB数据，进行TPC-H的测试。图16描述了4种系统的性能对比，ADB、PrestoDB、Spark-SQL和GreenPlum。</p>
<p><img src="/blog/e3ee66c7/tpch_comparison.png" alt></p>
<center>图16 TPC-H测试对比</center>

<p>ADB在22个查询中20个获得了更小的运行时间，表现优于第二好的GreenPlum，是Spark-SQL的两倍。<strong>ADB采用了pipeline的处理模型和索引，比基于stage的处理更快</strong>。PrestoDB也采用了pipeline模型，但是缺乏列上的索引。虽然，GreenPlum也有pipeline处理和全列的索引，ADB具备以下4个优势：</p>
<ul>
<li>ADB采用混合行列存储，而GreenPlum采用列存，在TPC-H的测试中，表中大约有一半的字段涉及到，ADB可以通过单次IO获取到更多的列；</li>
<li>ADB基于运行时成本的index path选择可以获取更好的表执行计划，而GreenPlum采用的是基于统计信息的执行计划；</li>
<li>ADB在组合谓词下推中采用了K路归并；</li>
<li>ADB采用了向量化执行引擎和将经过优化的CodeGen应用在所有的算子和表达式上。</li>
</ul>
<p>针对Q2，ADB比PrestoDB和GreenPlum慢，是因为ADB采用了不同的JOIN顺序。</p>
<h4 id="7-总结"><a href="#7-总结" class="headerlink" title="7. 总结"></a>7. 总结</h4><p>本论文阐述了ADB，作为一个高并发、低延时和实时的OLAP数据库。ADB有一个高效的索引引擎来异步构建所有列的索引，有助于提升查询性能和隐藏索引的构建开销。经过仔细的设计，全列索引只额外占用了66%的存储。ADB扩展了混合行列布局，支持结构化和其他复杂类型数据。为了达到高吞吐写入和高并发查询，ADB采用了读写分离的架构。而且，我们增强了优化器和执行引擎，来充分利用我们的存储和索引的优势。实验表明，所有这些设计有助于ADB相比主流的OLAP系统可以获得更好的性能。</p>
<h4 id="8-参考文献"><a href="#8-参考文献" class="headerlink" title="8. 参考文献"></a>8. 参考文献</h4><p>[1] Alibaba Cloud. <a href="https://www.alibabacloud.com" target="_blank" rel="noopener">https://www.alibabacloud.com</a>.</p>
<p>[2] ANTLR ASM. <a href="https://www.antlr.org" target="_blank" rel="noopener">https://www.antlr.org</a>.</p>
<p>[3] Apache ORC File. <a href="https://orc.apache.org/" target="_blank" rel="noopener">https://orc.apache.org/</a>.</p>
<p>[4] Benchmarking Nearest Neighbours. <a href="https://github.com/erikbern/ann-benchmarks" target="_blank" rel="noopener">https://github.com/erikbern/ann-benchmarks</a>.</p>
<p>[5] Greenplum. <a href="https://greenplum.org/" target="_blank" rel="noopener">https://greenplum.org/</a>.</p>
<p>[6] MySQL. <a href="https://www.mysql.com/" target="_blank" rel="noopener">https://www.mysql.com/</a>.</p>
<p>[7] Pangu. <a href="https://www.alibabacloud.com/blog/pangu—the-high" target="_blank" rel="noopener">https://www.alibabacloud.com/blog/pangu—the-high</a> performance-distributed-file-system-by-alibaba-cloud 594059.</p>
<p>[8] PostgreSQL. <a href="https://www.postgresql.org/" target="_blank" rel="noopener">https://www.postgresql.org/</a>.</p>
<p>[9] Presto. <a href="https://prestodb.io/" target="_blank" rel="noopener">https://prestodb.io/</a>.</p>
<p>[10] Teradata Database. <a href="http://www.teradata.com" target="_blank" rel="noopener">http://www.teradata.com</a>.</p>
<p>[11] TPC-H Benchmark. <a href="http://www.tpc.org/tpch/" target="_blank" rel="noopener">http://www.tpc.org/tpch/</a>.</p>
<p>[12] D. J. Abadi, S. R. Madden, and N. Hachem. <strong>Column-stores vs. row-stores: how difffferent are they really?</strong> In <em>SIGMOD</em>, pages 967–980. ACM, 2008.</p>
<p>[13] M. Armbrust, R. S. Xin, C. Lian, Y. Huai, D. Liu, J. K. Bradley, X. Meng, T. Kaftan, M. J. Franklin, A. Ghodsi, et al. <strong>Spark sql: Relational data processing in spark.</strong> In <em>SIGMOD</em>, pages 1383–1394. ACM, 2015.</p>
<p>[14] J. Backus. <em>Can programming be liberated from the von</em> Neumann style?: a functional style and its algebra of programs. ACM, 2007.</p>
<p>[15] P. A. Bernstein and N. Goodman. <strong>Multiversion concurrency control-theory and algorithms.</strong> <em>ACM Transactions on</em> Database Systems (TODS)*, 8(4):465–483, 1983.</p>
<p>[16] D. Comer. <strong>Ubiquitous b-tree</strong>. <em>ACM Computing Surveys</em> (CSUR), 11(2):121–137, 1979.</p>
<p>[17] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. <strong><em>Introduction to algorithms</em></strong>. MIT press, 2009.</p>
<p>[18] J. Dean and S. Ghemawat. <strong>Mapreduce: simplified data processing on large clusters</strong>. <em>Communications of the ACM</em>, 51(1):107–113, 2008.</p>
<p>[19] A. Eisenberg, J. Melton, K. Kulkarni, J.-E. Michels, and F. <strong>Zemke. Sql: 2003 has been published.</strong> <em>ACM</em> SIGMOD <em>Record</em>, 33(1):119–126, 2004.</p>
<p>[20] M. Grund, J. Kr¨uger, H. Plattner, A. Zeier, P. Cudre-Mauroux, and S. Madden. <strong>Hyrise: a main memory hybrid storage engine</strong>. <em>PVLDB</em>, 4(2):105–116, 2010.</p>
<p>[21] A. Gupta, D. Agarwal, D. Tan, J. Kulesza, R. Pathak, S. Stefani, and V. Srinivasan. <strong>Amazon redshift and the case for simpler data warehouses</strong>. In <em>SIGMOD</em>, pages 1917–1923. ACM, 2015.</p>
<p>[22] K. Hajebi, Y. Abbasi-Yadkori, H. Shahbazi, and H. Zhang. <strong>Fast approximate nearest-neighbor search with k-nearest neighbor graph</strong>. In <em>IJCAI</em>, pages 1312–1317, 2011.</p>
<p>[23] S. Harizopoulos, V. Liang, D. J. Abadi, and S. Madden. <strong>Performance tradeoffs in read-optimized databases.</strong> In <em>VLDB</em>, pages 487–498. VLDB Endowment, 2006.</p>
<p>[24] P. Hunt, M. Konar, F. P. Junqueira, and B. Reed. <strong>Zookeeper: Wait-free coordination for internet-scale systems.</strong> In <em>USENIX</em> <em>ATC</em>, volume 8. Boston, MA, USA, 2010.</p>
<p>[25] J.-F. Im, K. Gopalakrishna, S. Subramaniam, M. Shrivastava, A. Tumbde, X. Jiang, J. Dai, S. Lee, N. Pawar, J. Li, et al. <strong>Pinot: Realtime olap for 530 million users</strong>. In <em>SIGMOD</em>, pages 583–594. ACM, 2018.</p>
<p>[26] H. J´egou, M. Douze, and C. Schmid. <strong>Product quantization for nearest neighbor search</strong>. <em>IEEE Trans. Pattern Anal. Mach. Intell.</em>, 33(1):117–128, 2011.</p>
<p>[27] F. V. Jensen. <strong><em>An introduction to Bayesian networks</em></strong>, volume 210. UCL press London, 1996.</p>
<p>[28] M. Kornacker, A. Behm, V. Bittorf, T. Bobrovytsky, C. Ching, A. Choi, J. Erickson, M. Grund, D. Hecht, M. Jacobs, et al. <strong>Impala: A modern, open-source sql engine for hadoop</strong>. In <em>Cidr</em>, volume 1, page 9, 2015.</p>
<p>[29] A. Lamb, M. Fuller, R. Varadarajan, N. Tran, B. Vandiver, L. Doshi, and C. Bear. <strong>The vertica analytic database: C-store 7 years later</strong>. <em>PVLDB</em>, 5(12):1790–1801, 2012.</p>
<p>[30] G. M. Lohman. <strong><em>Grammar-like functional rules for representing query optimization alternatives</em></strong>, volume 17. ACM, 1988.</p>
<p>[31] S. Melnik, A. Gubarev, J. J. Long, G. Romer, S. Shivakumar, M. Tolton, and T. Vassilakis. <strong>Dremel: interactive analysis of web-scale datasets</strong>. <em>PVLDB</em>, 3(1-2):330–339, 2010.</p>
<p>[32] T. Neumann. <strong>Efficiently compiling efficient query plans for modern hardware.</strong> <em>PVLDB</em>, 4(9):539–550, 2011.</p>
<p>[33] K. Sato. <strong>An inside look at google bigquery</strong>.(2012). <em>Retrieved Jan</em>, 29:2018, 2012.</p>
<p>[34] M. Stonebraker, D. J. Abadi, A. Batkin, X. Chen, M. Cherniack, M. Ferreira, E. Lau, A. Lin, S. Madden, E. O’Neil, et al. <strong>C-store: a column-oriented dbms.</strong> In <em>VLDB</em>, pages 553–564. VLDB Endowment, 2005.</p>
<p>[35] A. Thusoo, J. S. Sarma, N. Jain, Z. Shao, P. Chakka, S. Anthony, H. Liu, P. Wyckoffff, and R. Murthy. <strong>Hive: a warehousing solution over a map-reduce framework.</strong> <em>PVLDB</em>, 2(2):1626–1629, 2009.</p>
<p>[36] F. Yang, E. Tschetter, X. L´eaut´e, N. Ray, G. Merlino, and D. Ganguli. <strong>Druid: A real-time analytical data store</strong>. In <em>SIGMOD</em>, pages 157–168. ACM, 2014.</p>
<p>[37] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauley, M. J. Franklin, S. Shenker, and I. Stoica.</p>
<p><strong>Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing.</strong> In <em>NSDI</em>, pages 2–2. USENIX Association, 2012.</p>
<p>[38] Z. Zhang, C. Li, Y. Tao, R. Yang, H. Tang, and J. Xu. <strong>Fuxi: a fault-tolerant resource management and job scheduling system at internet scale</strong>. <em>PVLDB</em>, 7(13):1393–1404, 2014.</p>
<p>[39] M. Zukowski, S. Heman, N. Nes, and P. Boncz. <strong><em>Super-scalar</em> <em>RAM-CPU cache compression</em></strong>. IEEE, 2006.</p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>OLAP</tag>
        <tag>AnalyticDB</tag>
      </tags>
  </entry>
  <entry>
    <title>读《阿里工程师自我修养》笔记</title>
    <url>/blog/5e4c4347.html</url>
    <content><![CDATA[<p>这本手册是阿里资深专家职业生涯的真切感悟，是走出中年危机的技术人成长手册。全手册共12个章节，其中将个人感觉比较有指导意义的内容作了整理和记录，希望这些内容对当下的你也能够有所帮助。</p>
<h4 id="技术人具备“结构化思维”意味着什么"><a href="#技术人具备“结构化思维”意味着什么" class="headerlink" title="技术人具备“结构化思维”意味着什么"></a>技术人具备“结构化思维”意味着什么</h4><p>结构化思维体现在表达和分析问题的过程中，表达可以从<strong>因果顺序</strong>、<strong>时间顺序</strong>、<strong>结构顺序</strong>、<strong>重要性顺序</strong>这四个方面开展；分析问题可以先建立中心明确目标，后结构化分解（参照上述四种顺序）。</p>
<p>推荐书籍：《思维混乱，因为大脑没有结构》</p>
<hr>
<h4 id="优秀工程师必备的三大思维"><a href="#优秀工程师必备的三大思维" class="headerlink" title="优秀工程师必备的三大思维"></a>优秀工程师必备的三大思维</h4><p>工程师思维包括<strong>产品思维</strong>、<strong>技术思维</strong>和<strong>工程思维</strong>。</p>
<p>产品思维是指通过技术手段以产品或服务的形态去解决用户的痛点，因此工程师需要理清自己的工作与用户价值的联系，然后去安排优先级和精力。</p>
<p>技术思维是源自需求的实现，确认需要做什么，因此工程师需要明确需求，重视需求的质量。一个大需求也许会拆分为多个模块，每个模块让一个人或一个团队负责，工程师要避免只看到“树木”而忘记”森林“。同时，个人或团队也不能为了完成各自进度，各种赶工造成”一地鸡毛“的现象。另外，新的技术如5G、人工智能、大数据、Kubernetes等等不断冲击工程师已有的技能，工程师还需要具备对新技术的追求以及保持其先进性。</p>
<p>工程思维是一种流程或机制，通过对过程的控制，来输出令人满意的结果。工程师需要将流程和工作环境无缝结合，在实践中不断完善回顾落实，从而提高软件的质量。另外，还包括了极端情况下的风险控制、软件运行的大量资源等等。</p>
<hr>
<h4 id="优秀工程师必备技能"><a href="#优秀工程师必备技能" class="headerlink" title="优秀工程师必备技能"></a>优秀工程师必备技能</h4><p><strong>思考力</strong>可以让工程师们具备强悍的思考和学习能力，它包含了<strong>认知科学</strong>、<strong>心理学</strong>、<strong>教育学</strong>、<strong>逻辑学</strong>，是需要系统化学习的一门很深的学问。下面，作者列出在日常技术学习和项目过程中沉淀下来的思考力，以及如何培养这些思考力。</p>
<p>思考力包括哪些？</p>
<p><strong>第一点，要掌握原理性思维，找出知识背后的原理</strong>。</p>
<p>知识是爆炸性的，而原理是可控的，复用性更高。探求知识背后原理的过程，是一种思维训练的过程。比如，理解业务系统或业务的设计思想、做事方式，加强了看透本质的能力。</p>
<p>另外，针对领域性知识，可以通过看书、看文章、资深人士沟通；注重原理在实际场景中的应用之处和缘由；<strong>重要性原理还是要结合经典书籍进行系统化学习</strong>。</p>
<p><strong>第二点，要掌握结构化思维，构建知识树。</strong></p>
<p>知识树如同数据的”索引“一般，有了索引大脑中的知识也就有了结构，分析问题、表达沟通也就具备了逻辑性，这也跟第一章节《技术人具备“结构化思维”意味着什么》提到的内容相契合。</p>
<p>如何做？内化外部知识为自己的知识，构建知识树；习惯性总结（复盘），完善知识树；通过xmind记录知识树；有意识地训练自己的思维习惯和做事方式，变得结构化。</p>
<p><strong>第三点，举一反三，拓展思维。</strong></p>
<p>强调同类问题同一处理方法，以及一类问题多种处理方法。</p>
<p><strong>第四点，抓重点思维，提升效率，方便记忆和传递。</strong></p>
<p>针对构建的知识树，提炼重点，进行归纳。针对多项任务，排列优先级，找到最关键或收益最大的任务。</p>
<p><strong>第五点，反思性思维，思考哪里可以做得更好</strong></p>
<p>做到在上一次基础上的升级，提升知识深度和质量。</p>
<hr>
<h4 id="如何在工作中快速成长？"><a href="#如何在工作中快速成长？" class="headerlink" title="如何在工作中快速成长？"></a>如何在工作中快速成长？</h4><p>真正的安全感、成就感、归属感来自自我成长和自我沉淀，而不是做大家正在做的事。轻易获得的东西，带来的结果表面上懂得很多，其实理解非常浅显。</p>
<p><strong>提升注意力要专注在目标事务上，直至产出预期的效果</strong>。（比如，系统性学习一个领域知识，需要持续有计划的看相关的书，直到达到预期要理解该领域知识的目标，而不是三天打鱼两天刷网，获取几手的低成本零碎知识）</p>
<p>身边缺乏贵人或贵人离自己较远的原因：</p>
<p>一是，自己不自信，不相信自己能够影响他人，导致缺乏主动沟通，长期沟通，沟通的延续性和习惯没有建立。 </p>
<p>二是，自己心态问题，自己的心态若是不够积极正向，没有贵人敢进入你的思维空间，因为价值观不匹配，很难形成认知共识。 </p>
<p>三是，职场原因，很多时候可能你的老板就是你的贵人，但是因为职场，因为上下级，碍于面子，碍于工作，不敢多交流，多请教。 </p>
<p>四是，贵人来了又走了，有贵人帮你改变，帮你进步，但是自己不努力，抱着过去做事的心态和方法在职场上浪迹天涯，进步不明显，否定了他作为贵人的价值和意义。</p>
<p><strong>因此，用成长回报贵人，并在未来可以帮助到他；平时建立有效沟通，让贵人了解你、影响你，并且自己要主动承担一些有挑战的事儿；要借事修人（锻炼机会非常少），事情失败了，人要成，能力要得到提升。</strong></p>
<p>帮助别人不等于麻烦别人，不是你输我赢，而是共同进步的过程，巩固知识的同时发现自己理解上的偏差。</p>
<p>输出倒闭输入，常规的方式有看书、思考和反馈，而<strong>讨论过程中的输入</strong>效果是最好的。比如通过会议的形式，思考会议中的信息，训练自己的观点产出能力、总结归纳能力。聆听，讨论验证自己的观点正确与否，再次聆听和讨论验证观点。作为<strong>参与者</strong>，认真聆听，快速提炼自己想表达的逻辑，然后参与讨论。 作为<strong>聆听者</strong>，仔细聆听，认真输入，在脑中组织思路，组织逻辑。作为<strong>中断者</strong>，发现有些会议真的没有继续的必要了，出于好意，提示会议的重心或者结束会议。</p>
<p>推荐书籍：</p>
<p>李笑来：《通往财富自由之路》 </p>
<p>武志红：《武志红的心理学课》 </p>
<p>刘润：《5 分钟商学院》 </p>
<p>特奥·康普诺利：《慢思考》 </p>
<p>米哈里 . 契克森米哈赖：《心流：最优体验心里学》</p>
<hr>
<h4 id="程序员如何自我学习？"><a href="#程序员如何自我学习？" class="headerlink" title="程序员如何自我学习？"></a>程序员如何自我学习？</h4><p>纯靠经验积累行不通，<strong>技术淘汰的速度远大于你经验积累的速度</strong>。</p>
<p>软件的经验积累还会体现在个<strong>架构设计</strong>上。</p>
<p><strong>学习还需要系统化</strong>，并非单靠看一篇文章就能明白原理。</p>
<p>项目中多尝试一下你学到的新知识，不能惯性使用你熟悉的技术，要知道你熟悉的东西很快会被淘汰，被淘汰后再调整就来不及了。</p>
<hr>
<h4 id="阿里资深专家10年感悟"><a href="#阿里资深专家10年感悟" class="headerlink" title="阿里资深专家10年感悟"></a>阿里资深专家10年感悟</h4><p>今天很残酷，明天更残酷，后天很美好，熬过明天晚上，才能看到后天的太阳。</p>
<p>一个人走得快，一群人走得远。</p>
<p><strong>困境是个人成长的最好机会，放弃、逃避、拒绝思考，就意味着放弃成长。如果遇到困境不自知，不解决，则会出现昨日所不知不能者，今日仍是不知不能；去年所不知不能者，今年仍是不知不能。</strong></p>
<p><strong>学习能力与思维模式</strong>是一个人的核心竞争力：</p>
<ul>
<li>承认自己的不足；</li>
<li>掌握优秀的学习方法，学习做到<strong>目到，口到，心到</strong>。当你能完全能用自己的语言准确讲述你所学的知识，知其然，并知其所以然，你才是真正完全的掌握；</li>
<li>掌握搜索信息的有效方式。有效保证你对问题的解决方案是相对优秀的解决方案，必须有业界全局的视眼与思考；</li>
<li>具备优秀的批判性思维模型<ul>
<li>鲁莽的思考者：不能意识到思维中重要的错误；</li>
<li>质疑的思考者：开始意识到思维中存在的错误；</li>
<li>初始的思考者：尝试改变自己的思维，但没有常规练习；</li>
<li>练习中的思考者：认识到常规练习的必要性；</li>
<li>高级的思考者：随着练习不断进步；</li>
<li>完善的思考者：有技巧和判断力的思维成为我们的第二本能。</li>
</ul>
</li>
</ul>
<hr>
<h4 id="如何量化考核技术人KPI？"><a href="#如何量化考核技术人KPI？" class="headerlink" title="如何量化考核技术人KPI？"></a>如何量化考核技术人KPI？</h4><p>技术KPI</p>
<ul>
<li>业务贡献：包括需求把控，业务项目和业务创新；</li>
<li>技术贡献：包括设计重构、技术影响力、Code Review、创新提效和代码质量；</li>
<li>团队贡献：包括招聘、新人培养和团队氛围。</li>
</ul>
<hr>
<h4 id="如何成为优秀的技术主管？"><a href="#如何成为优秀的技术主管？" class="headerlink" title="如何成为优秀的技术主管？"></a>如何成为优秀的技术主管？</h4><p>第一，技术说到底是为业务服务的，除非技术就是业务本身，必须体现它的商业价值；</p>
<p>第二，我认为最最重要的是架构设计的能力，可能管理能力还次之；</p>
<p>第三，技术视野良好，解决问题能力与架构设计能力出色。知道在什么场景应用什么技术，业务发展到什么规模应该预先做哪些技术储备。</p>
<p>第四，动手能力要强，学习能力出色。<strong>技术 TL 除了管人和管事之外，其他还有很多事情要做包括建立团队研发文化、 团队人才培养与建设、跨部门协调与沟通等，这样以要求技术 TL 也同时也需要具备良好的沟通和管理能力</strong>。</p>
]]></content>
      <categories>
        <category>个人日志</category>
      </categories>
      <tags>
        <tag>职场感悟</tag>
      </tags>
  </entry>
  <entry>
    <title>数据结构与算法概览</title>
    <url>/blog/77a95856.html</url>
    <content><![CDATA[<h4 id="什么是数据结构？什么是算法？"><a href="#什么是数据结构？什么是算法？" class="headerlink" title="什么是数据结构？什么是算法？"></a>什么是数据结构？什么是算法？</h4><p>从广义上讲，数据结构就是指一组数据的存储结构。算法就是操作数据的一组方法。</p>
<p>从狭义上讲，是指某些著名的数据结构和算法，比如队列、栈、堆、二分查找、动态规划等。这些都是前人智慧的结晶，我们可以直接拿来用。我们要讲的这些经典数据结构和算法，都是前人从很多实际操作场景中抽象出来的，经过非常多的求证和检验，可以高效地帮助我们解决很多实际的开发问题。</p>
<h4 id="数据结构与算法之间的关系？"><a href="#数据结构与算法之间的关系？" class="headerlink" title="数据结构与算法之间的关系？"></a>数据结构与算法之间的关系？</h4><p>数据结构和算法是相辅相成的。数据结构是为算法服务的，算法要作用在特定的数据结构之上。 因此，我们无法孤立数据结构来讲算法，也无法孤立算法来讲数据结构。</p>
<p>数据结构是静态的，它只是组织数据的一种方式。如果不在它的基础上操作、构建算法，孤立存在的数据结构就是没用的。</p>
<h4 id="数据结构结构与算法包含哪些内容？"><a href="#数据结构结构与算法包含哪些内容？" class="headerlink" title="数据结构结构与算法包含哪些内容？"></a>数据结构结构与算法包含哪些内容？</h4><table>
<thead>
<tr>
<th>一级分类</th>
<th>二级分类</th>
<th>子类</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>线性表</td>
<td>数组</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>链表</td>
<td>单链表、双向链表、循环链表、双向循环列表、静态链表、<strong>跳表</strong></td>
<td></td>
</tr>
<tr>
<td></td>
<td>栈</td>
<td>顺序栈、链式栈</td>
<td></td>
</tr>
<tr>
<td></td>
<td>队列</td>
<td>普通队列、双端队列、阻塞队列、并发队列、阻塞并发队列</td>
<td></td>
</tr>
<tr>
<td>散列表</td>
<td>散列函数</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>动态扩容</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td><strong>位图</strong></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>冲突解决</td>
<td>链表法、开放寻址、其他</td>
<td></td>
</tr>
<tr>
<td>树</td>
<td>二叉树</td>
<td>平衡二叉树、二叉查找树、平衡二叉查找树（AVL、红黑树）、完全二叉树、满二叉树</td>
<td></td>
</tr>
<tr>
<td></td>
<td>堆</td>
<td>小顶堆、大顶堆、优先级队列、斐波那契堆、二顶堆</td>
<td></td>
</tr>
<tr>
<td></td>
<td>多路查找树</td>
<td>B树、B+树、2-3树、2-3-4树</td>
<td></td>
</tr>
<tr>
<td></td>
<td>其他</td>
<td>树状数组、线段树</td>
<td></td>
</tr>
<tr>
<td>图</td>
<td>图的存储</td>
<td>邻接矩阵、邻接表</td>
<td></td>
</tr>
<tr>
<td></td>
<td>其他</td>
<td>拓扑排序、最短路径、关键路径、最小生成树、二分图、最大流</td>
<td></td>
</tr>
<tr>
<td>复杂度分析</td>
<td>空间</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>时间</td>
<td>最好、最坏、平均、均摊</td>
<td></td>
</tr>
<tr>
<td>基本算法</td>
<td></td>
<td>贪心算法、分治算法、动态规划、回溯算法、枚举算法</td>
<td></td>
</tr>
<tr>
<td>排序</td>
<td>平方级O(n^2)</td>
<td>冒泡排序、插入排序、选择排序、希尔排序</td>
<td></td>
</tr>
<tr>
<td></td>
<td>线性对数级O(nlogn)</td>
<td>归并排序、快速排序、堆排序</td>
<td></td>
</tr>
<tr>
<td></td>
<td>线性级O(n)</td>
<td>计数排序、基数排序、桶排序</td>
<td></td>
</tr>
<tr>
<td>搜索</td>
<td>深度优先搜索</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>广度优先搜索</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>A*启发式搜索</td>
<td></td>
<td></td>
</tr>
<tr>
<td>查找</td>
<td>线性表查找</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>树结构查找</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>散列查找</td>
<td></td>
<td></td>
</tr>
<tr>
<td>字符串匹配</td>
<td></td>
<td>朴素、KMP、Robin-Karp、Boyer-Moore、AC自动机、Trie、后缀数组</td>
<td></td>
</tr>
<tr>
<td>其他</td>
<td>数论</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>计算几何</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>概率分析</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>并查集</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>拓扑网路</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>矩阵运算</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>线性规划</td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>补充：</strong>大数据领域的算法和数据结构，很多都是采用多种数据结构和算法设计组合而成，例如：布隆过滤器、跳表、LSM树、Merkle哈希树、Snappy与LZSS算法、Cuckoo哈希。</p>
<h4 id="重点掌握哪些？"><a href="#重点掌握哪些？" class="headerlink" title="重点掌握哪些？"></a>重点掌握哪些？</h4><p><strong>10个数据结构</strong>：数组、链表、栈、队列、散列表、二叉树、堆、跳表、图、Trie树（前缀树或字典树）</p>
<p><strong>10个算法</strong>：递归、排序、二分查找、搜索、哈希算法、贪心算法、分治算法、回溯算法、动态规划、字符串匹配算法</p>
<p><strong>算法时间复杂度</strong>：</p>
<ul>
<li>多项式阶：常数级O(1)、对数级O(logn)、线性级O(n)、线性对数级O(nlogn)、平方级O(n^2)、立方级O(n^3)</li>
<li>非多项式阶：指数级O(2^n)、阶乘级O(n!)。性能极差</li>
</ul>
<p><strong>算法空间复杂度</strong>：主要有O(1)、O(n)、O(n^2)，对数级或线性对数级不常见</p>
<p><strong>要学习它的“来历”、“自身的特点”、“适合解决的问题”以及“实际的应用场景“。</strong>工作中遇到实际需求，能够想到它们并做出选择。</p>
<h4 id="学习这些有哪些技巧？"><a href="#学习这些有哪些技巧？" class="headerlink" title="学习这些有哪些技巧？"></a>学习这些有哪些技巧？</h4><p>边学边练，适度刷题</p>
<p>多问、多思考、多互动</p>
<p>设立切实可行的目标，针对知识点的学习，输出学习笔记或心得</p>
<p>知识需要沉淀、反复迭代</p>
<h4 id="推荐资源"><a href="#推荐资源" class="headerlink" title="推荐资源"></a>推荐资源</h4><p>刷题网站：leetcode</p>
<p>入门级书：《大话数据结构》、《算法图解》</p>
<p>进阶级书：《算法导论》、《算法第四版》</p>
]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>算法复杂度</tag>
      </tags>
  </entry>
  <entry>
    <title>服务器硬件基础知识介绍</title>
    <url>/blog/d8653a22.html</url>
    <content><![CDATA[<h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><p>服务器相比于普通的PC，在扩展性、稳定性上要求比较高，比如支持各种扩展卡、7×24小时不间断运行等等。本文主要针对硬件服务器的分类、组成以及结构进行介绍说明。对服务器硬件比较感兴趣的小伙伴可以了解一下。</p>
<a id="more"></a>
<h4 id="服务器分类"><a href="#服务器分类" class="headerlink" title="服务器分类"></a>服务器分类</h4><h5 id="1-X86架构"><a href="#1-X86架构" class="headerlink" title="(1) X86架构"></a>(1) X86架构</h5><ul>
<li>CISC，复杂指令集</li>
<li>操作系统：linux、windows</li>
<li>非国产代表：Intel、AMD</li>
<li>国产：海光</li>
<li>通用型服务器</li>
<li>cpu的引脚分别为硬盘、通道卡、网卡等预留</li>
<li>单核双线程（超线程技术）</li>
</ul>
<h5 id="2-非X86架构"><a href="#2-非X86架构" class="headerlink" title="(2) 非X86架构"></a>(2) 非X86架构</h5><ul>
<li>RISC，精简指令集</li>
<li>操作系统：linux for arm</li>
<li>ARM、Power</li>
<li>国产：华为鲲鹏、飞腾（固化）、龙芯、兆芯、申威</li>
<li>系统小，响应快，专用型，使用在终端设备上</li>
<li>cpu和硬盘、通道卡、网卡等能力全部集成在一起，SOC</li>
<li>单核单线程</li>
</ul>
<h4 id="服务器组成"><a href="#服务器组成" class="headerlink" title="服务器组成"></a>服务器组成</h4><p>服务器的组成主要包含：CPU、主板( motherboard )、内存(memory)、硬盘(HD, hard disk)、网络(network)、扩展卡、机箱、电源、风扇等组件。</p>
<h5 id="1-CPU"><a href="#1-CPU" class="headerlink" title="(1) CPU"></a>(1) CPU</h5><p>拿<a href="https://www.intel.cn/content/www/cn/zh/products/processors/xeon/scalable.html" target="_blank" rel="noopener">Intel至强可扩展处理器</a>来说，分为铂金、金牌、银牌、铜牌（性能由高到低），型号系列如下：</p>
<ul>
<li>铂金：Intel Xeon Scaleable 81xx/82xx</li>
<li>金牌：Intel Xeon Scaleable 61xx/62xx/51xx/52xx (可用于人工智能)</li>
<li>银牌：Intel Xeon Scaleable 41xx/42xx (可用于数据实时计算)</li>
<li>铜牌：Intel Xeon Scaleable 31xx/32xx</li>
</ul>
<h5 id="2-主板"><a href="#2-主板" class="headerlink" title="(2) 主板"></a>(2) 主板</h5><p>主板上主要包括CPU槽位、内存槽位、PCIE槽位、硬盘接口。不同厂家主板布局可能稍有不同。</p>
<h6 id="内存"><a href="#内存" class="headerlink" title="内存"></a>内存</h6><ul>
<li>DDR内存（3代、4代）：常用容量8g/16g/32g/64g不等；</li>
<li>Intel傲腾数据中心级持久内存模块（DCPIMM），常用容量128g/256g/512g。</li>
</ul>
<p>其中，说明一下傲腾的两种模式：  APP Direct模式和内存模式 。APP Direct模式表示作为存储来使用，访问速度介于DDR和PCIE SSD（SATA SSD…）之间。内存模式仅作为内存的缓存使用。</p>
<p>另外，为何傲腾的内存容量会比DDR大很多？主要是采用了3D XPoint的技术，将存储介质以3D方式堆叠，增加密度，提供仅次于DRAM访问速度的大容量持久化内存，之前Intel还提出过将<a href="https://mp.weixin.qq.com/s/GQeVyR8qMnvHl14xJgaUWA" target="_blank" rel="noopener">3D XPoint引入HBase，移除WAL的问题单</a>。</p>
<p>另外，要在内存扩展槽中插入傲腾，必须先要插满内存卡，才可以使用。</p>
<h6 id="硬盘"><a href="#硬盘" class="headerlink" title="硬盘"></a>硬盘</h6><ul>
<li>SATA: 机械硬盘，<strong>大容量，适合存储非结构化数据</strong>，比如视频、图片。7200r/m，传输速率6Gbps，IOPS在150-200M；</li>
<li>SAS: 机械硬盘，SATA和SCSI的结合，容量一般小于SATA，读写性能更高，兼容SATA，用途类似。10000-15000r/m，传输速率6Gbps，通常高于SATA，IOPS在200-300M；</li>
<li>SATA SSD：固态硬盘，支持SATA接口接入的固态硬盘。</li>
<li>Nvme SSD：NVME是一种协议，针对PCIE通道设计的，数据从硬盘到内存或CPU的通道，支持多个数据同时通过。<strong>适合存储结构化数据，热数据。</strong>其中， <strong>U.2固态硬盘</strong>支持NVMe协议，走PCI-E 3.0 x4通道 。 SATA固态硬盘一般的带宽速度是6Gbps，而支持NVMe协议的U.2速度是32Gbps 。当然，直接走PICE则速度跟快了。</li>
</ul>
<p>补充，SAN属于存储专用硬件，分为FC-SAN和IP-SAN。内部是做RAID的SAS盘，可挂载到多个主板上，FC-SAN通过QLE光纤卡接入，IP-SAN通过网卡接入。</p>
<h6 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h6><ul>
<li>万兆以太网卡（ethernet card）</li>
<li>Intel Omni-path</li>
<li>Mellanox Infiniband</li>
</ul>
<p>其中，后两者都支持RDMA。</p>
<h6 id="扩展卡"><a href="#扩展卡" class="headerlink" title="扩展卡"></a>扩展卡</h6><p>常用见的扩展卡有QLE光纤卡、GPU卡、PCIE SSD、FPGA卡、<a href="https://mp.weixin.qq.com/s/zmNyb4dcOertJ3nEBUrUgg" target="_blank" rel="noopener">RAID卡</a>、Cavium多核卡等。<strong>走主板的PCIE槽位插入，不同的卡使用的pcie lanes不同</strong>。其中，PCIE×16一般用于GPU卡，PICE×8用于FPGA卡等。PCIE是一种物理接口：Peripheral Component Interconnect Express。一般可以提供等扩展卡插入使用。。其他还有：</p>
<ul>
<li>U.2是一种物理接口 ：使用PCIE×4（4个PICE lanes），用于连接SSD。</li>
<li>AHCI是一种逻辑接口（协议）：Advanced Host Controller Interface。Intel发明用于管理SATA设备。 </li>
<li>NVME是一种逻辑接口（协议）： Non-Volatile Memory Host Controller Interface Specification (NVMHCIS) 。</li>
</ul>
<h4 id="服务器结构"><a href="#服务器结构" class="headerlink" title="服务器结构"></a>服务器结构</h4><h5 id="1-机架式服务器"><a href="#1-机架式服务器" class="headerlink" title="(1) 机架式服务器"></a>(1) 机架式服务器</h5><p>外形类似交换机，有1U（1U=1.75英寸=44.45毫米）、2U、3U、4U等规格。机架式服务器安装在标准的19英寸机柜里面。1U、2U最为常用。<strong>需要额外提供机柜放置</strong>。<strong>一般大型企业使用</strong>。</p>
<ul>
<li>国外，Facebook发起open computing project中，提出OU标准=44.45mm，19英寸机柜中，功耗1200多w；</li>
<li>国内，BAT标准，提出RU标准=46.5mm，21英寸机柜中，功耗3000w左右，重量1.5T。</li>
</ul>
<h5 id="2-刀片式服务器"><a href="#2-刀片式服务器" class="headerlink" title="(2) 刀片式服务器"></a>(2) 刀片式服务器</h5><p>在标准高度的机架式机箱内可<strong>插装多个卡式的服务器单元</strong>，是一种实现HAHD(High Availability High Density，高可用高密度)的低成本服务器平台，为特殊应用行业和高密度计算环境专门设计。刀片服务器就像“刀片”一样，每一块“刀片”实际上就是一块系统主板。 </p>
<p>每块刀片都可以热插拔，替换快，维护时间短。低功耗、空间小，适用于高性能计算集群。相比于机架服务器密度高，但散热差，需要强力风扇。<strong>一般用于建设数据中心</strong>。</p>
<h5 id="3-塔式服务器"><a href="#3-塔式服务器" class="headerlink" title="(3) 塔式服务器"></a>(3) 塔式服务器</h5><p>外形及结构都与普通的PC机差不多，只是个头稍大一些，其外形尺寸并无统一标准。  塔式服务器的机箱内部往往会预留很多空间，以便进行硬盘，电源等的冗余扩展。 <strong>一般中小企业使用。</strong></p>
<h5 id="4-机柜式服务器"><a href="#4-机柜式服务器" class="headerlink" title="(4) 机柜式服务器"></a>(4) 机柜式服务器</h5><p>在一些高档企业服务器中由于内部结构复杂，内部设备较多，有的还具有许多不同的设备单元或几个服务器都放在一个机柜中，这种服务器就是机柜式服务器。 内部单元以机架为单位。</p>
<h4 id="参考链接-amp-扩展阅读"><a href="#参考链接-amp-扩展阅读" class="headerlink" title="参考链接&amp;扩展阅读"></a>参考链接&amp;扩展阅读</h4><ul>
<li><a href="http://www.brofive.org/?p=1118" target="_blank" rel="noopener">PCIe、M.2、U.2、AHCI和NVMe</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MjM5Nzk2MDU5NA==&amp;mid=2652554324&amp;idx=2&amp;sn=c9daf40371c2f3fd1a1b338895789874&amp;chksm=bd3c6aa98a4be3bf4680853a6066aebada981d50959e90ceca121e4c7218f969a37e0dd53fad&amp;scene=0&amp;xtrack=1#rd" target="_blank" rel="noopener">浅谈几种常见 RAID 的异同</a></li>
<li><a href="https://mp.weixin.qq.com/s/zmNyb4dcOertJ3nEBUrUgg" target="_blank" rel="noopener">关于Raid0,Raid1,Raid5,Raid10的总结</a></li>
<li><a href="https://blog.csdn.net/qq_21125183/article/details/80563463" target="_blank" rel="noopener">深入浅出全面解析RDMA</a></li>
<li><a href="https://yq.aliyun.com/articles/74471" target="_blank" rel="noopener">阿里云麒麟液冷</a></li>
<li><a href="https://mp.weixin.qq.com/s/E16ODnFTh0rwLeH1jpaX5w" target="_blank" rel="noopener">阿里X-Engine存储引擎借助FPGA加速Compaction</a></li>
<li><a href="https://mp.weixin.qq.com/s/GQeVyR8qMnvHl14xJgaUWA" target="_blank" rel="noopener">3D XPoint引入HBase移除WAL的问题单</a></li>
</ul>
]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>Intel</tag>
        <tag>ARM</tag>
        <tag>X86</tag>
        <tag>硬件</tag>
        <tag>RAID</tag>
        <tag>服务器</tag>
      </tags>
  </entry>
  <entry>
    <title>每周算法之TopK问题</title>
    <url>/blog/6c12eb0c.html</url>
    <content><![CDATA[<h5 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h5><p>海量数据处理中经常会遇到统计出现频次最高的K个关键词或者从数字中统计出最大的前K个数，这就是经典的TopK问题。</p>
<h5 id="2-解决思路"><a href="#2-解决思路" class="headerlink" title="2. 解决思路"></a>2. 解决思路</h5><ul>
<li>方案一：通过快速排序进行全排序，找到最大的前k个数。复杂度为O(n*lgn)</li>
<li>方案二：避免全排序，只对最大的K个排序。比如通过冒泡排序的方式，冒k次泡，找出k个最大值。复杂度为O(n*k)</li>
<li>方案三：找出k个最大值，不排序。通过建立小根堆，建立大小为k的小根堆，然后遍历剩下的n-k个数。每次跟堆顶的数比较，大于堆顶的值则覆盖，重新调整堆为小顶堆，直到遍历完n-k个数为止。复杂度为O(n*lgk)</li>
<li>方案四：针对方案一快速排序的改进。快排属于分治算法，这里通过减治的方式，将只针对包含最大k个值的这一侧进行快排处理。</li>
</ul>
<p><strong>备注：</strong></p>
<blockquote>
<p>分治法：大问题分解为小问题，小问题都要递归各个分支，例如：快速排序</p>
<p>减治法：大问题分解为小问题，小问题只要递归一个分支，例如：二分查找，随机选择</p>
</blockquote>
<p>考虑到数据集很大，内存无法全部容纳。</p>
<h5 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h5><ul>
<li><a href="https://www.cnblogs.com/qlky/p/7512199.html" target="_blank" rel="noopener">海量数据中找出前k大数（topk问题）</a></li>
<li><a href="https://mp.weixin.qq.com/s/FFsvWXiaZK96PtUg-mmtEw" target="_blank" rel="noopener">拜托，面试别再问我TopK了</a></li>
</ul>
]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>TopK</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>大数据平台体系介绍</title>
    <url>/blog/8600073b.html</url>
    <content><![CDATA[<h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>大数据除了体现数据5V特性外，代表了一种理念、一种问题解决的思路和一系列技术的集合。随着分布式技术的发展、处理能力的增强，数据已经从过去的采样处理转为全量化、实时化处理。在大数据技术体系方面可以分为数据收集、数据存储、资源管理与服务协调、计算引擎和数据分析这五个方面。本文从基于围绕这五个方面，介绍大数据平台整体结构、开源系和商业公司的技术栈体系，从而帮助大家对大数据技术有一个全面的认识。</p>
<a id="more"></a>
<h3 id="技术体系"><a href="#技术体系" class="headerlink" title="技术体系"></a>技术体系</h3><p><img src="/blog/8600073b/bd_pf_archi.png" alt></p>
<ul>
<li>数据收集层：数据源具有分布式、异构、多样化、流式产生等特点，因此收集组件需要具备扩展性、适配不同数据源、数据传输不或少量丢失、敏感数据安全性、入库延迟性要低。</li>
<li><strong>数据存储层</strong>：数据量增加具备线性扩展、利用廉价机器具备容错性、数据多样性具备支持多种数据模型存储。</li>
<li>资源管理与服务协调层：<ul>
<li>轻量级弹性资源管理平台：资源管理方面，基于一套集群和轻量级隔离方案实现资源共享，错峰运行多个应用；</li>
<li>服务协调组件：leader选举、服务命名、分布式队列、分布式锁、发布订阅功能等。</li>
</ul>
</li>
<li><strong>计算引擎层</strong>：不同应用场景，对数据处理的吞吐率和延迟要求不同。例如：<ul>
<li>批处理：分钟、小时、天级别（10s-1h+），追求高吞吐率，单位时间处理数据尽可能大，如离线构建索引、批量数据分析等；</li>
<li>交互式处理：秒级（1-10s），提供类SQL便于用户使用，如数据查询、报表生成、OLAP等；</li>
<li>实时处理：秒级以内（0-1s），如舆情监测、广告系统等。</li>
</ul>
</li>
<li>数据分析层：对接用户应用程序，提供类SQL、API、数据挖掘SDK等。一般基于批处理对原始数据做处理，形成小规模数据集，之后采用交互式处理工具堆积数据集快速查询和获取结果。</li>
<li>数据可视化层：展示大数据价值的门户。</li>
</ul>
<h4 id="谷歌技术栈"><a href="#谷歌技术栈" class="headerlink" title="谷歌技术栈"></a>谷歌技术栈</h4><p>事实上，大数据技术来源于互联网行业，尤其是谷歌公司发表的<a href="https://pan.baidu.com/s/1dkW1h_-P_b6AG64MZOpyVg" target="_blank" rel="noopener">三篇大数据论文</a>（Google FileSystem，Google BigTable，Google MapReduce），提取码：cuvq，对大数据技术的发展起到了重要作用。技术栈如下图所示：</p>
<p><img src="/blog/8600073b/bd_pf_google.png" alt></p>
<h4 id="开源技术栈"><a href="#开源技术栈" class="headerlink" title="开源技术栈"></a>开源技术栈</h4><p><img src="/blog/8600073b/bd_pf_os.png" alt></p>
<h4 id="阿里技术栈"><a href="#阿里技术栈" class="headerlink" title="阿里技术栈"></a>阿里技术栈</h4><p><img src="/blog/8600073b/bd_pf_ali.png" alt></p>
<h4 id="腾讯技术栈"><a href="#腾讯技术栈" class="headerlink" title="腾讯技术栈"></a>腾讯技术栈</h4><p><img src="/blog/8600073b/bd_pf_tecent.png" alt></p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul>
<li><a href="http://blog.bizcloudsoft.com/?p=292" target="_blank" rel="noopener">Google中文版论文来源</a></li>
<li><a href="https://www.aliyun.com/" target="_blank" rel="noopener">阿里云官网</a></li>
<li><a href="https://cloud.tencent.com/" target="_blank" rel="noopener">腾讯云官网</a></li>
<li>《大数据架构详解：从数据数据获取到深度学习》，朱洁、罗华霖编著，2016.10。</li>
<li>《大数据技术体系详解：原理、架构与实战》，董西成编著，2018.01.01。</li>
</ul>
]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>架构</tag>
      </tags>
  </entry>
  <entry>
    <title>InnoDB和RocksDB存储引擎解析</title>
    <url>/blog/2b3a4d7d.html</url>
    <content><![CDATA[<h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>数据库存储引擎为数据库提供了数据的读（查询）和写（创建、更新、删除）操作，不同的存储引擎提供了不同的存储机制、索引技巧、事务操作等功能。同时，存储的数据达到一定体量，存储引擎性能也是各不相同。本文将围绕InnoDB和LevelDB两种存储引擎，从数据读写应用场景入手分析各自的架构特点、采用的数据结构以及各自的优缺点，并以此作为存储引擎选型的依据。</p>
<a id="more"></a>
<h3 id="InnoDB"><a href="#InnoDB" class="headerlink" title="InnoDB"></a>InnoDB</h3><h4 id="基本介绍"><a href="#基本介绍" class="headerlink" title="基本介绍"></a>基本介绍</h4><p>InnoDB存储引擎支持事务，设计目标用于在线事务处理(OLTP)的应用，作为MySQL数据库最为常用的存储引擎之一。该存储引擎是第一个完整支持ACID事务的MySQL存储引擎，具有行锁设计、MVCC(多版本控制并发)、外键、一致性非锁定读的特点，可以有效利用内存和CPU资源。</p>
<h4 id="体系结构"><a href="#体系结构" class="headerlink" title="体系结构"></a>体系结构</h4><p>InnoDB存储引擎架构主要由多个后台处理线程+内存缓冲池+本地磁盘文件构成。如下图所示：</p>
<p><img src="/blog/2b3a4d7d/innodb_archi.png" alt></p>
<p><img src="/blog/2b3a4d7d/innodb_mm.png" alt></p>
<p>（1）线程划分</p>
<ul>
<li>Master Thread: 负责将缓冲池的数据异步刷新到磁盘，保证数据一致性，包括脏页（发生修改的页）刷新、合并插入缓冲、undo页回收等；</li>
<li>IO Thread: 采用异步IO处理写IO请求，提高数据库性能，分布式read、write、insert buffer、log这四个IO线程；</li>
<li>Purge Thread: 事务调提交后，undo页可能不再需要，通过Purge Thread线程进行回收；</li>
<li>Page Cleaner Thread: 为了提升存储引擎性能，将脏页刷新操作独立出来，减轻Master Thread工作量和查询阻塞问题。</li>
</ul>
<p>（2）内存划分</p>
<ul>
<li>缓冲池：由于CPU速度远高于磁盘读取速度，故采用缓冲池技术提高数据库性能，数据以页的形式组织管理。<ul>
<li>缓冲池中页被修改后，通过checkpoint机制刷到磁盘；</li>
<li>缓冲池支持多实例，通过hash方式将页均匀加载到不同实例，提高并发能力；</li>
<li>通过改进的LRU算法管理数据，通过old和new划分，避免偶尔的数据扫描替换掉热点数据；</li>
<li>支持对页的压缩，将原本默认16 KB的页压缩到1 KB、2 KB、4 KB和8 KB。</li>
</ul>
</li>
<li>重做日志缓冲：重做日志先写入缓冲，通过一定机制再写入磁盘重做日志文件。重做日志文件记录了存储引擎的事务日志（write ahead log），当发生宕机时用于数据恢复，保证数据完整性。</li>
<li>额外内存池：用于缓冲区内部一些其他控制信息或数据结构需要的额外内存。</li>
</ul>
<h4 id="索引机制"><a href="#索引机制" class="headerlink" title="索引机制"></a>索引机制</h4><p>InnoDB存储引擎支持的常见索引有B+树索引、全文索引以及哈希索引（根据表适用情况自适应创建，不对外）。其中，B+树索引是目前关系型数据库中查询最为常用和最为有效的索引。</p>
<p>（1）B+树结构</p>
<p>B+树由二叉查找树-&gt;平衡二叉树-&gt;B树（多路平衡查找树）演化而来，为基于磁盘或其他存储设备之上的文件系统所需而设计的一种多路平衡查找树。</p>
<p><img src="/blog/2b3a4d7d/BTree-structure.jpg" alt></p>
<p><img src="/blog/2b3a4d7d/B+Tree-structure.jpg" alt></p>
<p>在B+树中所有记录节点都是按照键值大小顺序放在同一层叶子节点上，由各节点指针进行连接。相对B树来说，非叶子节点上读取一次磁盘可以存放更多的关键词元素，从而降低了IO的读写次数。</p>
<p>（2）关键特性</p>
<ul>
<li>支持范围查询，基于索引节点，定位到叶子节点链表起始点和结束点直接遍历；</li>
<li>适合OLTP的场景，根据索引条件过滤出少量数据；</li>
<li>任何查询都需要从根节点到叶子节点，路径相同，查询稳定；</li>
</ul>
<p>（3）B+索引应用</p>
<p>B+索引是B+树在数据库中的实现，在MySQL中分为聚集索引（clustered index）和辅助索引（secondary index）。聚集索引中叶子节点记录了整个表行记录，因此一个表只有一个聚集索引。但是每张表可以多个辅助索引，辅助索引中的叶子节点存储了bookmark，指向行记录在聚集索引中的索引键。</p>
<h3 id="RocksDB"><a href="#RocksDB" class="headerlink" title="RocksDB"></a>RocksDB</h3><h4 id="基本介绍-1"><a href="#基本介绍-1" class="headerlink" title="基本介绍"></a>基本介绍</h4><p>RocksDB是使用C++编写的、LSM结构的、嵌入式KV存储引擎，由Facebook基于LevelDB开发，还借鉴了许多HBase的设计理念。RocksDB依靠大量灵活的配置，使之能针对不同的生产环境进行调优，包括直接使用内存，使用Flash，使用硬盘或者HDFS。支持使用不同的压缩算法，并且有一套完整的工具供生产和调试使用。</p>
<h4 id="体系结构-1"><a href="#体系结构-1" class="headerlink" title="体系结构"></a>体系结构</h4><p>RocksDB存储引擎是基于LSM（Log Structure Merge Tree）思想实现的，内存结构memtable和immutable memtable，immutable memtable用于flush到磁盘形成SST文件，不同SST、Level之间的merge操作称为compaction。manifest记录了引擎的状态和SST修改快照信息，current标识当前正在使用的manifest（SST生成新的，manifest也会生成新的）。由于RocksDB是基于LevelDB构建，下面给出两个引擎的架构图作为对比。</p>
<p><img src="/blog/2b3a4d7d/leveldb_archi.png" alt></p>
<p>RocksDB在LevelDB基础上引入列簇的概念，每个ColumnFamily有自己的Memtable， SST文件，所有ColumnFamily共享WAL、Current、Manifest文件。</p>
<p><img src="/blog/2b3a4d7d/rocksdb_archi2.png" alt></p>
<p>（1）写入流程</p>
<p>数据写入时，先追加到WAL再写入memtable。memtable写满后转为immutable memtable，等待<strong>flush</strong>到第0层SST，触发条件后再<strong>compaction</strong>到第1层SST，依次类推到第N层SST。</p>
<p>flush操作将多个immutable memtable合并排序后，持久化到第0层SST文件中，由于该层SST文件并没有做compaction操作，因此不同SST之间存在key重复。</p>
<p>compaction操作将本层的SST和上一次层的SST进行合并操作，因此第1层以上的SST文件key都是唯一的。</p>
<p>（2）读取流程</p>
<p>读取的顺序为memtable-&gt;immutable memtable-&gt;level 0 SST-&gt;…-&gt;level n SST。其中，memtable和immutable memtable采用了跳表特性进行查询，SST文件中有过滤器（布隆过滤器）决定是否包含某个key再加载至内存，基于有序KV进行二分查找。同时，在memtable和SST之上还设置了Block Cache，提高查询性能。</p>
<h4 id="索引机制-1"><a href="#索引机制-1" class="headerlink" title="索引机制"></a>索引机制</h4><p>RocksDB采用的内存结构memtable和外存结构SST，决定了RocksDB能够支持点查和范围查询。</p>
<p>（1）Memtable</p>
<p>内存中的数据结构，在数据flush到SST之前用来保存数据的，可用于读写。写入数据时首先插入memtable，写满后转为immutable memtable，等待flush到SST中。读取数据时也是优先读取memtable中，数据相对于SST比较新。支持如下两类数据结构：</p>
<ul>
<li>Skiplist MemTable：基于skiplist的memtable为读写、随机访问和顺序扫描提供了良好的性能，支持范围查询</li>
<li>HashSkiplist MemTable：hash和skiplist的结合，按照key的前缀做hash，每个hash桶中都是一个skiplist。单独访问一个key时性能更好，相对于skiplist减少了比较次数，夸多个前缀进行扫描需要复制和排序，范围查询性能也会差一些。</li>
</ul>
<p>（2）SST</p>
<p>SST是Sorted Sequence Table（排序队列表），是排好序的数据文件。在这些文件里，所有键都按照排序好的顺序组织，一个键或者一个迭代位置可以通过二分查找进行定位，支持两种结构：</p>
<ul>
<li>Block-based Table格式：该方式是RocksDB的默认SST格式。内部结构按照块的方式组织，详情见<a href="https://github.com/facebook/rocksdb/wiki/Rocksdb-BlockBasedTable-Format" target="_blank" rel="noopener">github wiki</a>。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;文件开始&gt;</span><br><span class="line">[data block 1]                                (排好序的KV，二分查找)</span><br><span class="line">[data block 2]</span><br><span class="line">...</span><br><span class="line">[data block N]</span><br><span class="line">[meta block 1: filter block]                  (过滤器)</span><br><span class="line">[meta block 2: stats block]                   (属性)</span><br><span class="line">[meta block 3: compression dictionary block]  (压缩字典)</span><br><span class="line">[meta block 4: range deletion block]          (see section: &quot;range deletion&quot; Meta Block)</span><br><span class="line">...</span><br><span class="line">[meta block K: future extended block]  (后期会添加更多的元数据块)</span><br><span class="line">[metaindex block]                      (元数据索引块，指向多个元数据块位置)</span><br><span class="line">[index block]                          (数据索引块)</span><br><span class="line">    [index block - partition 1]        (按照Key的范围创建的索引)</span><br><span class="line">    [index block - partition 2]</span><br><span class="line">    ...</span><br><span class="line">    [index block - partition N]</span><br><span class="line">    [index block - top-level index]    (先将顶级索引加载到内存，然后按需加载对应分区索引)</span><br><span class="line">[Footer]                               (固定大小; 指定数据索引块的top-level index和元数据索引块)</span><br><span class="line">&lt;文件结束&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>PlainTable格式：RocksDB针对纯内存或低延迟介质上的低查询延迟进行了优化。<a href="https://github.com/facebook/rocksdb/wiki/PlainTable-Format" target="_blank" rel="noopener">详情见github wiki</a>。</li>
</ul>
<h3 id="对比分析"><a href="#对比分析" class="headerlink" title="对比分析"></a>对比分析</h3><p>InnoDB属于B树存储引擎，RocksDB属于LSM树存储引擎。存储引擎不同的存储结构、策略，决定了不同的读写应用场景。</p>
<p>（1）B树存储引擎</p>
<p>针对InnoDB，按照页来组织数据，每个页对应B+树的一个节点。读取操作B+树一次检索最多需要h-1次磁盘IO（h为树的高度）。写入操作需要对内存中的B+树进行修改，修改的页超过比率后在持久到磁盘。因此，InnoDB适合写少读多的场景。</p>
<p>（2）LSM树存储引擎</p>
<p>针对RocksDB，LSM结构体现在对数据修改增量保持在内存，达到阈值后将修改操作批量写入磁盘，读取时需要合并磁盘中的历史数据和内存中最近的修改操作。因此，RocksDB的LSM机制牺牲了读的性能，提高了写入的性能，适合写多读少的场景。</p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul>
<li>《大规模分布式存储系统·原理解析与架构实战》</li>
<li>《MySQL技术内幕 InnoDB存储引擎第2版》</li>
<li><a href="https://blog.csdn.net/v_JULY_v/article/details/6530142/" target="_blank" rel="noopener">从B树、B+树、B*树谈到R树</a></li>
<li><a href="https://rocksdb.org.cn/doc/getting-started.html" target="_blank" rel="noopener">RocksDB中文网</a></li>
<li><a href="https://github.com/facebook/rocksdb/wiki/" target="_blank" rel="noopener">https://github.com/facebook/rocksdb/wiki/</a></li>
<li><a href="https://yq.aliyun.com/articles/669316" target="_blank" rel="noopener">看图了解RocksDB</a></li>
</ul>
]]></content>
      <categories>
        <category>存储引擎</category>
      </categories>
      <tags>
        <tag>B+树</tag>
        <tag>InnoDB</tag>
        <tag>RocksDB</tag>
        <tag>LSM树</tag>
        <tag>存储引擎</tag>
      </tags>
  </entry>
  <entry>
    <title>Java并发编程知识体系概览</title>
    <url>/blog/8dea6a69.html</url>
    <content><![CDATA[<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ul>
<li><p>并发：当有多个线程在操作时，如果系统只有一个CPU，则它根本不可能真正同时进行一个以上的线程，它只能把CPU运行时间划分成若干个时间段,再将时间 段分配给各个线程执行，在一个时间段的线程代码运行时，其它线程处于挂起状。这种方式我们称之为并发(Concurrent)。</p>
</li>
<li><p>并行：当系统有一个以上CPU时，则线程的操作有可能非并发。当一个CPU执行一个线程时，另一个CPU可以执行另一个线程，两个线程互不抢占CPU资源，可以同时进行。这种方式我们称之为并行(Parallel)。</p>
</li>
</ul>
<h3 id="并发目的"><a href="#并发目的" class="headerlink" title="并发目的"></a>并发目的</h3><p>由于CPU和I/O天然存在的矛盾，传统顺序的同步工作模式导致任务阻塞，CPU处于空闲状态，浪费资源。多线程为了突破同步工作模式的情况下CPU资源的浪费，即使单核情况下也能将时间片拆分成单位给更多的线程来轮询使用。多线程在不同享状态的情况下非常高效，不管协同式还是抢占式都能在单位时间内执行更多的任务，从而更好地榨取CPU资源。因此，并发的目的就是为了提供程序运行的性能。</p>
<h3 id="并发技术"><a href="#并发技术" class="headerlink" title="并发技术"></a>并发技术</h3><h4 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h4><p>为了能够充分利用CPU资源，需要采用多线程机制，在CPU上进行调度。何为线程？是操作系统能够进行运算调度的最小单位。它被包含在进程之中，是进程中的实际运作单位。一条线程指的是进程中一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同的任务。</p>
<h4 id="线程交互"><a href="#线程交互" class="headerlink" title="线程交互"></a>线程交互</h4><ul>
<li>交互方式：线程交互也就是线程直接的通信，最直接的办法就是线程直接通信传值，而间接方式则是通过共享变量来达到彼此的交互。如：等待、通知、中断、织入。</li>
<li>线程安全：我们最关注的还是通过共享变量来达到交互的方式。如果线程执行的任务相互独立则不存在安全问题，但多数情况下线程直接需要打交道，而且需要分享共享资源，那么这个时候最核心的就是线程安全。</li>
</ul>
<h4 id="性能优化"><a href="#性能优化" class="headerlink" title="性能优化"></a>性能优化</h4><p>常见的性能指标包括：<code>QPS</code>、<code>TPS</code>、<code>RT</code>。这里不考虑外部网络、分布式架构、各级缓存、数据冗余等设计，主要针对单节点的性能优化。</p>
<p>随着用户激增，请求次数的增加，服务也对应着需要并发模型来支撑。<strong>但是一个节点的并发量有个上限，当达到这个上限后，响应时间就会变长，所以我们需要探索并发到什么程度才是最优的，才能保证最高的并发数，同时响应时间又能保持在理想情况</strong>。</p>
<p>服务接收到一个请求后，主要经历CPU等待、执行和IO等待、读写的时间。</p>
<h5 id="单线程"><a href="#单线程" class="headerlink" title="单线程"></a>单线程</h5><p><code>RT = T(cpu) + T(io)</code><br><code>QPS = 1000ms / RT</code></p>
<h5 id="多线程-1"><a href="#多线程-1" class="headerlink" title="多线程"></a>多线程</h5><p>cpu核数为M，利用率为P。最优并发数<code>N = [T(cpu) + T(io)] / T(cpu) * M * P</code></p>
<p><code>QPS = 1000ms / RT * N</code><br><code>QPS = 1000ms / [T(cpu) + T(io)] * [T(cpu) + T(io)] / T(cpu) * M * P</code><br><code>QPS = 1000ms / T(cpu) * M * P</code></p>
<p>在M固定的情况下，如果cpu利用率负载不高，利用率p上不去。大部分情况跟共享资源的使用有关，即<strong>锁的使用需要优化</strong>。</p>
<h4 id="并发模型"><a href="#并发模型" class="headerlink" title="并发模型"></a>并发模型</h4><p>采用多线程方式实现并发，需要合理使用各种锁。除了使用多线程，下面介绍两类并发模型流水线模型和函数式模型。</p>
<ul>
<li>流水线模型：总体的思想就是纵向切分任务，把任务里面耗时过久的环节单独隔离出来，避免完成一个任务需要耗费等待的时间。在实现上又分为<code>Actors</code>和<code>Channels</code>模型。比如基于<code>Java</code>的<code>Akka</code>/<code>Reator</code>或者<code>golang</code>就是为流水线模式而生的并发语言，还有<code>nodeJS</code>等等。</li>
<li>函数式模型：类似流水线模型，单一的函数是无状态的，所以避免了资源竞争的复杂度，同时每个函数类似流水线里面的单一环境，彼此直接通过函数调用传递参数副本，函数之外的数据不会被修改。函数式模式跟流水线模式相辅相成逐渐成为更为主流的并发架构。</li>
</ul>
<h3 id="java并发体系"><a href="#java并发体系" class="headerlink" title="java并发体系"></a>java并发体系</h3><p>java中主要以多线程方式实现并发，主要内容有：</p>
<ul>
<li>并发基础：<ul>
<li>AQS:<ol>
<li>AbstractqueuedSynchronizer同步器</li>
<li>CLH同步队列</li>
<li>同步状态的获取和释放</li>
<li>线程阻塞和唤醒</li>
</ol>
</li>
<li>CAS: Compare and Swap 缺陷</li>
</ul>
</li>
<li>Java内存模型JMM：线程通信、消息传递</li>
<li>内存模型：<ul>
<li>重排序</li>
<li>顺序一致性</li>
<li>happens-before</li>
<li>as if serial</li>
</ul>
</li>
<li>synchronized：<ul>
<li>同步、重量级锁、<code>synchronized</code>原理</li>
<li>锁优化：<ol>
<li>自旋锁</li>
<li>轻量级锁</li>
<li>重量级锁</li>
<li>偏向锁</li>
</ol>
</li>
</ul>
</li>
<li>原子操作：<ul>
<li>基本类型：<code>AtomicBoolean</code>、<code>AtomicInteger</code>、<code>AtomicLong</code></li>
<li>数组：<code>AtomicIntegerArray</code>、<code>AtomicLongArray</code>、<code>AtomicReferenceArray</code></li>
<li>引用类型：<code>AtomicReference</code>、<code>AtomicReferenceArrayFieldUpdater</code></li>
</ul>
</li>
<li>线程池：<ul>
<li><code>ThreadPoolExecutor</code>（拒绝策略、参数优化）、<code>ScheduledExecutorService</code></li>
<li><code>Callable</code>和<code>Future</code></li>
</ul>
</li>
<li>并发集合：<code>ConcurrentHashMap</code>、<code>CopyOnWriteArrayList</code>、<code>ConcurrentLinkedQueue</code>、<code>BlockingQueue</code>、<code>ConcurrentSkipListMap</code>等</li>
<li>并发工具类：<code>Semaphore</code>、<code>CyclicBarrier</code>、<code>CountDownLatch</code></li>
<li>锁：<code>ReentrantLock</code>、<code>Condition</code>、<code>ReentrantReadWriteLock</code>、<code>LockSupport</code></li>
<li>volatile：<ul>
<li><code>volatile</code>实现机制</li>
<li>内存语义</li>
<li>内存模型</li>
</ul>
</li>
<li>其他：<ul>
<li><code>ThreadLocal</code></li>
<li><code>Fork/Join</code></li>
</ul>
</li>
</ul>
<h3 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h3><ul>
<li>《Java并发编程实战》Doug Lea</li>
<li>《Java并发编程的艺术》讲解并发包内部实现原理</li>
<li>《Java多线程编程核心技术》高洪沿</li>
<li>《图解Java多线程设计模式》并发编程设计模式方面的经典书籍</li>
<li>《操作系统：精髓与设计原理》经典操作系统教材</li>
<li><a href="http://ifeve.com" target="_blank" rel="noopener">http://ifeve.com</a> 国内专业并发编程网站</li>
<li><a href="http://www.cs.umd.edu/~pugh/java/memoryModel/" target="_blank" rel="noopener">http://www.cs.umd.edu/~pugh/java/memoryModel/</a></li>
<li><a href="https://mp.weixin.qq.com/s/qI04Z5dm5dcLtOsKIzHrVw" target="_blank" rel="noopener">多线程小抄集</a> （公众号：朱小厮的博客）</li>
<li><a href="https://mp.weixin.qq.com/s/5pNBSvIcT8I1_MCJs1ewAA" target="_blank" rel="noopener">高并发知识体系</a> (公众号：云时代架构)</li>
</ul>
]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>并发编程</tag>
      </tags>
  </entry>
  <entry>
    <title>Helm使用简明教程</title>
    <url>/blog/29dc945b.html</url>
    <content><![CDATA[<p>Helm是由Deis发起的一个开源工具，有助于简化部署和管理Kubernetes应用。</p>
<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>Helm可以理解为Kubernetes的包管理工具，可以方便地发现、共享和使用Kubernetes构建的应用。包含如下概念：</p>
<ul>
<li>Chart: 一个helm包，包含一个应用运行所需的镜像、依赖以及k8s资源编排等定义，类似yum的rpm文件。</li>
<li>Release: 在k8s上运行的一个chart实例。每次安装chart都会创建一个新的release。</li>
<li>Repository: 用于发布和存储Chart仓库。</li>
</ul>
<h3 id="基本组件"><a href="#基本组件" class="headerlink" title="基本组件"></a>基本组件</h3><ul>
<li>Helm CLI: helm客户端，可以在本地执行</li>
<li>Tiller: 服务器端组件，运行在k8s上，管理k8s应用程序生命周期</li>
<li>Repository: chart仓库，helm cli通过http协议访问chart仓库中的索引文件和压缩包</li>
</ul>
<p><img src="/blog/29dc945b/helm_architecture.jpeg" alt></p>
<h3 id="helm命令"><a href="#helm命令" class="headerlink" title="helm命令"></a>helm命令</h3><p>通过helm cli能够实现k8s应用的管理操作。顶级options:<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--debug: 打开日志显示</span><br><span class="line">--home string: helm配置路径，默认为~/.helm</span><br><span class="line">--host string: tiller地址，覆盖 $HELM_HOST</span><br><span class="line">--kube-context string: 指定使用的kubeconfig上下文</span><br><span class="line">--kubeconfig string: 指定kubeconfig的绝对路径</span><br><span class="line">--tiller-connection-timeout int： helm和tiller建立连接超时，单位s，默认300</span><br><span class="line">--tiller-namespace: tiller命名空间，默认为kube-system</span><br></pre></td></tr></table></figure></p>
<h4 id="应用仓库"><a href="#应用仓库" class="headerlink" title="应用仓库"></a>应用仓库</h4><p>添加、展示、移除、更新和索引应用仓库。<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 添加一个chart仓库</span></span><br><span class="line">helm repo add [flags] [NAME] [URL]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 给定包含chart包的目录生成索引文件</span></span><br><span class="line">helm repo index [flags] [DIR]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 列出已有的仓库</span></span><br><span class="line">helm repo list [flags]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 移除指定仓库</span></span><br><span class="line">helm repo remove [flags] [NAME]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 更新每个chart仓库的最新信息缓存到本地，供helm search检测仓库中的chart使用</span></span><br><span class="line">helm repo update [flags]</span><br></pre></td></tr></table></figure></p>
<h4 id="检索应用"><a href="#检索应用" class="headerlink" title="检索应用"></a>检索应用</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在管理的所有仓库中检索应用</span></span><br><span class="line">helm search [keyword] [flags]</span><br></pre></td></tr></table></figure>
<h4 id="创建应用"><a href="#创建应用" class="headerlink" title="创建应用"></a>创建应用</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在指定目录中创建应用</span></span><br><span class="line">helm create [/dir/]chartname [flags]</span><br><span class="line"></span><br><span class="line">mycharts/</span><br><span class="line">    Chart.yaml          #描述关于chart的信息</span><br><span class="line">    LICENSE             #可选项， 描述chart的license</span><br><span class="line">    README.md           #可选项，可读性高的介绍文档</span><br><span class="line">    requirements.yaml   #可选项，列出chart的依赖</span><br><span class="line">    values.yaml         #chart的默认配置</span><br><span class="line">    charts/             #存放chart依赖的其他chart包</span><br><span class="line">    templates/          #模板目录，基于values.yaml生成有效的k8s清单文件</span><br><span class="line">    template/NOTES.txt  #可选项，描述简短用法说明。</span><br></pre></td></tr></table></figure>
<h4 id="应用依赖"><a href="#应用依赖" class="headerlink" title="应用依赖"></a>应用依赖</h4><p>通过编辑mycharts/requirements.yaml，声明依赖的子应用。<br><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">dependencies:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">apache</span> <span class="comment">#chart的名称</span></span><br><span class="line">    <span class="attr">version:</span> <span class="number">1.2</span><span class="number">.3</span>  <span class="comment"># chart的版本</span></span><br><span class="line">    <span class="attr">repository:</span> <span class="string">http://example.com/charts</span> <span class="comment">#chart的仓库url</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mysql</span></span><br><span class="line">    <span class="attr">version:</span> <span class="number">3.2</span><span class="number">.1</span></span><br><span class="line">    <span class="attr">repository:</span> <span class="string">http://another.example/charts</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 根据requirements.yaml配置，将依赖的应用包从仓库中拉取到charts目录,移除旧的</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 同时会生成requirements.lock</span></span><br><span class="line">helm dependency update [flags] CHART</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 基于requirements.lock，重新构建charts中的应用</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果没有lock文件，类似update操作</span></span><br><span class="line">helm dependency build [flags] CHART</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 展示应用依赖的所有子应用包</span></span><br><span class="line">helm dependency list [flags] CHART</span><br></pre></td></tr></table></figure>
<h4 id="应用归档"><a href="#应用归档" class="headerlink" title="应用归档"></a>应用归档</h4><p>把chart应用打包成归档文件<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">helm package [flags] [ChART-PATH] [...]</span><br><span class="line"></span><br><span class="line">--app-version string: 打包时设置chart的appVersion</span><br><span class="line">--version string: 打包时设置chart的version</span><br><span class="line">--dependency-update: 打包时更新依赖</span><br><span class="line">--destination string: 指定打包应用存放的目录</span><br><span class="line">--save: 保存打包应用放在本地仓库，默认为true</span><br><span class="line"></span><br><span class="line">--sign: 开启pgp私钥签名打包的应用 使用sign时，指定签名要使用的密钥名称和公钥环路径</span><br><span class="line">--key string</span><br><span class="line">--keyring string</span><br></pre></td></tr></table></figure></p>
<h4 id="应用检测"><a href="#应用检测" class="headerlink" title="应用检测"></a>应用检测</h4><p>检测chart应用开发完成后，潜在的问题<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 验证指定路径下的应用格式是否正确</span></span><br><span class="line">helm lint [flags] PATH</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 打印应用的Chart.yaml READEME values.yaml中的内容</span></span><br><span class="line">helm inspect [chart | readme | values] CHARTNAME</span><br></pre></td></tr></table></figure></p>
<h4 id="应用安装"><a href="#应用安装" class="headerlink" title="应用安装"></a>应用安装</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 验证应用包是否为可信任方提供的</span></span><br><span class="line">helm verify [flags] PATH</span><br><span class="line">--keyring: 指定公钥环路径</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> chart支持:</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> reponame/chartname</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> path/xxx.tgz</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> repo-url/xx.tgz</span></span><br><span class="line">helm install [CHART] [flags]</span><br><span class="line">helm install --repo repourl chartname</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 额外指定配置，覆盖values.yaml</span></span><br><span class="line">helm install -f myvalues.yaml CHART</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置级别覆盖</span></span><br><span class="line">helm install --set k1=v1,k2=v2 CHART</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 正式安装前的调试，检查release生成的manifests是否正确</span></span><br><span class="line">helm install CHART --dry-run --debug</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 将指定版本的chart应用安装到指定的命名空间下</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装前：verify验证是否有有效来源文件，dep-up更新依赖</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> name为release默认自动生成，version默认为latest，namespace默认为default</span></span><br><span class="line">helm install chart --verfiy --dep-up --name string --repo string --version string --namespace string</span><br></pre></td></tr></table></figure>
<h4 id="release查看"><a href="#release查看" class="headerlink" title="release查看"></a>release查看</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">helm get [flags] RELEASE-NAME</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">下载指定release的所有钩子、配置清单、noteh和values文件</span></span><br><span class="line">helm get hooks[ manifest | notes | values ] releasename</span><br></pre></td></tr></table></figure>
<h4 id="release测试"><a href="#release测试" class="headerlink" title="release测试"></a>release测试</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 为release运行测试命令</span></span><br><span class="line">helm test [RELEASE] [flags]</span><br><span class="line">--cleanup: 完成删除测试pod</span><br><span class="line">--parallel: 并行运行测试pod</span><br></pre></td></tr></table></figure>
<h4 id="release状态"><a href="#release状态" class="headerlink" title="release状态"></a>release状态</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看release的运行状态</span></span><br><span class="line">helm status [flags] RELEASE-NAME</span><br><span class="line">--output string: 状态信息输出格式，json or yaml</span><br></pre></td></tr></table></figure>
<h4 id="release升级"><a href="#release升级" class="headerlink" title="release升级"></a>release升级</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">helm upgrade [RELEASE] [CHAR] [flags]</span><br><span class="line"></span><br><span class="line">--version string: 选用指定版本的chart来升级release</span><br><span class="line">--set k1=v1,k2=v2: 升级时，覆盖values.yaml中的部分值</span><br></pre></td></tr></table></figure>
<h4 id="release回滚"><a href="#release回滚" class="headerlink" title="release回滚"></a>release回滚</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 将release回滚到指定版本</span></span><br><span class="line">helm rollback [flags] [RELEASE] [REVISION]</span><br></pre></td></tr></table></figure>
<h4 id="release卸载"><a href="#release卸载" class="headerlink" title="release卸载"></a>release卸载</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">helm delete [flags] RELEASE-NAME [...]</span><br><span class="line"></span><br><span class="line">--purge: 清理干净，使得release可以重用</span><br></pre></td></tr></table></figure>
<h4 id="查看所有release"><a href="#查看所有release" class="headerlink" title="查看所有release"></a>查看所有release</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">helm list [flags] [FILTER]</span><br><span class="line"></span><br><span class="line">--chart-name: 根据应用名称排序</span><br><span class="line">--date: 按安装时间排序</span><br><span class="line">--deleted: 显示删除的release</span><br><span class="line">--deleting： 显示正在删除中的</span><br><span class="line">--deployed: 显示部署的</span><br><span class="line">--failed: 显示失败的</span><br><span class="line">--pending: 显示挂起的</span><br><span class="line"></span><br><span class="line">--namespace string: 指定命名空间</span><br><span class="line"></span><br><span class="line">helm list 'ara[a-z]+'： 支持release name正则的过滤</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>云原生</category>
      </categories>
      <tags>
        <tag>helm</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Nginx之正向代理与反向代理</title>
    <url>/blog/7a63f2f5.html</url>
    <content><![CDATA[<h3 id="正向代理"><a href="#正向代理" class="headerlink" title="正向代理"></a>正向代理</h3><p>客户端访问的目标服务明确，但是无法直接访问，需要通过代理服务器跳转，才可以访问。结构图如下：<br><img src="/blog/7a63f2f5/nginx-zx.png" alt></p>
<p>正向代理典型应用，如我们日常的翻墙操作，访问google网站，在浏览中设置代理服务器。在nginx中配置示例如下：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">http &#123;</span><br><span class="line">  include mime.types; #文件扩展名与文件类型映射表</span><br><span class="line">  default_type application/octet-stream; #默认文件类型</span><br><span class="line">  tcp_nopush on; #防止网络阻塞</span><br><span class="line">  tcp_nodelay on; #防止网络阻塞</span><br><span class="line">  keepalive_timeout 120; #长连接超时时间，单位是秒</span><br><span class="line"><span class="meta">  #</span><span class="bash"> 其他配置省略</span></span><br><span class="line"></span><br><span class="line"><span class="meta">  #</span><span class="bash">正向代理</span></span><br><span class="line">  server &#123;</span><br><span class="line">    resolver 114.114.114.114; #dns服务器</span><br><span class="line">    listen 80;  </span><br><span class="line">    location / &#123;</span><br><span class="line"></span><br><span class="line">      # 用户请求/时，在html/下找依次index.htm、index.html</span><br><span class="line">      root html;</span><br><span class="line">      index index.htm index.html;</span><br><span class="line"></span><br><span class="line">      proxy_pass http://$http_host$request_uri; #设定代理服务器的协议和地址  </span><br><span class="line">      proxy_set_header HOST $http_host;</span><br><span class="line">      proxy_buffering on; #开启后端服务响应内容缓冲</span><br><span class="line">      proxy_buffers 256 4k; # 设置缓冲区大小和数量,proxy_buffering开启才会生效</span><br><span class="line">      proxy_busy_buffers_size 8k; #proxy_buffers中处于busy状态（buffer被写入往客户端传）的buffer大小设置</span><br><span class="line"></span><br><span class="line">      proxy_buffer_size 4k; #存储后端服务响应的header，跟proxy_buffering开启无关</span><br><span class="line">      proxy_max_temp_file_size 0k;#后端服务响应内容较大，会临时写到这这里</span><br><span class="line"></span><br><span class="line">      #超时设置</span><br><span class="line">      proxy_connect_timeout 30; #代理跟后端服务器连接超时时间</span><br><span class="line">      proxy_read_timeout 60; #代理读超时，等待后端服务的响应时间</span><br><span class="line">      proxy_send_timeout 60; #代理发送超时，代理两次发送之间的时间，不是一次发送的时间</span><br><span class="line">      #代理和后端服务发生超时或错误时，返回空或非法响应头，状态码为502</span><br><span class="line">      proxy_next_upstream error timeout invalid_header http_502;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    # 发生错误时，请求50x.html</span><br><span class="line">    error_page 500 502 503 504 /50x.html;</span><br><span class="line">    location = /50x.html &#123;</span><br><span class="line">      root html;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="反向代理"><a href="#反向代理" class="headerlink" title="反向代理"></a>反向代理</h3><p>客户端访问的目标服务不明确，客户端直接访问代理服务器，代理服务器内部将客户端请求转发到后方某个目标服务。结构图如下：<br><img src="/blog/7a63f2f5/nginx-fx.png" alt></p>
<p>反向代理典型应用，比如负载均衡机制。为了支持海量用户访问，通过代理将用户请求分散到后端不同服务器上，而用户侧并不知感知。在nginx中配置示例如下：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">http &#123;</span><br><span class="line"><span class="meta">  #</span><span class="bash">前置配置省略</span></span><br><span class="line"></span><br><span class="line"><span class="meta">  #</span><span class="bash">负载均衡配置</span></span><br><span class="line">  upstream app_severs &#123;</span><br><span class="line">    server 127.0.0.1:9090;#server1</span><br><span class="line">    server 127.0.0.1:9091;#server2</span><br><span class="line">    server 127.0.0.1:9092;#server3</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">  #</span><span class="bash">反向代理</span></span><br><span class="line">  server &#123;</span><br><span class="line">    listen 80; #监听的端口</span><br><span class="line">    server_name 127.0.0.1; #代理服务器地址</span><br><span class="line"></span><br><span class="line">    location &#123;</span><br><span class="line">      proxy_pass http://app_servers; #设置定义的upstream</span><br><span class="line">      proxy_next_upstream error timeout invalid_header http_502;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    error_page 500 502 503 504 /50x.html;</span><br><span class="line">    location = /50x.html &#123;</span><br><span class="line">      root html;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul>
<li><a href="http://tengine.taobao.org/book/" target="_blank" rel="noopener">淘宝Tengine·Nginx开发从入门到精通</a></li>
<li><a href="https://www.cnblogs.com/wawahaha/p/4645668.html" target="_blank" rel="noopener">Nginx配置文件nginx.conf中文详解（总结）</a></li>
</ul>
]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>正向代理</tag>
        <tag>反向代理</tag>
      </tags>
  </entry>
  <entry>
    <title>JVM-GC机制详解</title>
    <url>/blog/625f1969.html</url>
    <content><![CDATA[<h3 id="需要管理和回收的内存"><a href="#需要管理和回收的内存" class="headerlink" title="需要管理和回收的内存"></a>需要管理和回收的内存</h3><h4 id="堆内存"><a href="#堆内存" class="headerlink" title="堆内存"></a>堆内存</h4><p>python等语言采用“<strong>引用计数法</strong>”来确定堆内存中的对像是否被使用，计数为0则回收。该方法实现简单效率高，但不能解决java中对象间相互“<strong>循环引用</strong>”的问题，故<strong>引入“可达性分析算法”</strong>，任何对象和GC Roots之间必须存在一个“引用链”，否则该对象不可用。</p>
<p>无论哪种方式，关键还是在“引用”。JDK1.2+，引用分为如下四类：</p>
<ul>
<li>强引用（不会回收）</li>
<li>软引用（SoftReference类，第二次gc时内存紧张会回收）</li>
<li>弱引用（WeakReference类，无论内存是否紧张下一次gc前都会回收）</li>
<li>虚引用（PhantomReference类，用于回收时接收系统通知）</li>
</ul>
<p>对象确定死亡前，需要经过“<strong>两次标记过程</strong>”。两次过程如下：</p>
<ul>
<li>可达性分析确定对象和GC Roots之间没有引用链则进行第一次标记，并将需要执行finalize()方法的对象放入F-queue中，等待jvm的finalizer线程去执行；</li>
<li>对F-queue中的对象进行第二次标记，仍然没有引用链则被真正回收。</li>
</ul>
<h4 id="方法区"><a href="#方法区" class="headerlink" title="方法区"></a>方法区</h4><p>这部分主要回收废弃常量和无用的类。</p>
<h3 id="典型GC算法"><a href="#典型GC算法" class="headerlink" title="典型GC算法"></a>典型GC算法</h3><ul>
<li><p>标记清除算法（Mark-Sweep）</p>
<p>首先标记出所有需要回收的对象，然后统一回收所有被标记的对象。</p>
<p>优点：支持用户线程和GC线程一起工作；</p>
<p>缺点：标记和清除效率低下，其次<strong>产生大量内存碎片</strong>导致分配大对象时没有足够的连续空间而再次触发GC操作。</p>
</li>
<li><p>复制算法（Copy）</p>
<p><strong>为了解决“Mark-Sweep”的缺陷</strong>，将可用内存分为两半，其中一块内存用完将存活的对象复制到另外一块内存依次存放。</p>
<p>优点：不产生碎片，适用于生命周期短的对象（98%以上）；</p>
<p>缺点：内存牺牲掉一半，<strong>空间利用率不高</strong>。其次存活对象较多导致复制的数据量大，效率低下。</p>
</li>
<li><p>标记整理算法（Mark-Compact）</p>
<p><strong>为了解决“Copy”的缺陷</strong>，标记过程和Mark-Sweep的标记过程类似，回收阶段主要是将存活的对象都向一端移动，然后端边界以外的（一端最后一个存活的对象内存之后的部分）内存直接清除。</p>
<p>优点：内存空间连续；</p>
<p>缺点：效率不高，存在<strong>STW</strong>；</p>
</li>
<li><p>分代收集（Generation-Collection）</p>
<p>将Java堆内存分为<strong>新生代</strong>（Young）和<strong>老年代</strong>（Old），根据各个年代的特点采用适当的算法。如：</p>
<p>新生代对象“朝生夕灭”，故采用<strong>copy算法</strong>；</p>
<p>老年代对象存活率高、没有格外空间对其分配担保，故采用<strong>Mark-Compact算法</strong>。  </p>
<p>Hotspot将新生代分为两个较小的survivor空间（S0、S1）和一个较大的Eden空间，每次只使用其中一个<strong>survivor和eden空间，默认比例为8:1</strong>。也就是，S0和S1各占用10%，Eden占用80%。主要步骤：</p>
<ol>
<li>在eden区申请对象（对象头、实例数据）；</li>
<li>一次YGC，少量对象从eden区和S1区（from）进入到S0区（to），age+1；</li>
<li>再经过多次YGC后，S0区满了，会将S1切换为to，S0区切换为from；</li>
<li>再次YGC，少量对象从eden区和S0区（from）进入到S1区（to）；</li>
<li>当上一次回收后存活的对象，在另一个survivor空间放不下的话，老年代可以进行<strong>分配担保（Handle promotion）</strong>，使得这些存活对象进入老年代。</li>
</ol>
</li>
</ul>
<h3 id="Hotspot垃圾收集器"><a href="#Hotspot垃圾收集器" class="headerlink" title="Hotspot垃圾收集器"></a>Hotspot垃圾收集器</h3><p>JDk7u14+的Hotspot虚拟机中的收集器，下图表示了不同分代的收集器可以搭配使用的关系。</p>
<ul>
<li>并发：垃圾收集和用户线程同时工作；</li>
<li>并行：多个垃圾收集，用户线程等待。<br><img src="/blog/625f1969/jvm-gc-structure.jpg" alt></li>
</ul>
<h4 id="Serial-Serial-old"><a href="#Serial-Serial-old" class="headerlink" title="Serial/Serial old"></a>Serial/Serial old</h4><p>Serial： </p>
<ul>
<li>Young区Minor GC（复制算法）</li>
<li><strong>Serial是client模式下的默认新生代收集器</strong>，简单高效，单cpu下没有线程交互开销，收集效率高。</li>
</ul>
<p>Serial Old：</p>
<ul>
<li>负责老年代（标记整理算法）</li>
<li>Serial Old收集器可与Parallel Scavenge搭配使用，同时可作为CMS在并发收集发生<code>Concurrent Mode Failure</code>时使用的后备方案。</li>
</ul>
<p>两者都是单线程方式进行垃圾回收，需要暂停所有用户线程（Stop the world）。</p>
<h4 id="ParNew-CMS"><a href="#ParNew-CMS" class="headerlink" title="ParNew/CMS"></a>ParNew/CMS</h4><p>ParNew收集器：</p>
<ul>
<li><strong>是Serial的多线程版本</strong>，其他控制参数和Serial一样；</li>
<li><strong>ParNew是Server模式下的首选新生代收集器</strong>，可与CMS（并发收集，垃圾收集和用户线程基本同时工作）搭配使用</li>
<li><strong>ParNew在单cpu下，效果没有Serial好</strong>。CPU数量添加，默认开启收集线程数和cpu数量相同。</li>
</ul>
<p>CMS收集器：</p>
<ul>
<li><p>是一种获取<strong>最短回收提顿时间</strong>为目标的收集器<strong>，侧重服务的响应速度</strong>，也被称为<strong>并发低停顿收集器</strong>。</p>
<ul>
<li><strong>初始标记</strong>：找到GCRoot第一层扫描，<strong>STW很短</strong>，一个GC线程；</li>
<li>并发标记：GC和用户线程同时，一个GC线程；</li>
<li><strong>重新标记</strong>：<strong>有STW</strong>，<strong>多个GC线程</strong>；</li>
<li>并发清理：GC和用户线程同时，一个GC线程。</li>
</ul>
</li>
<li><p>CMS默认启动回收线程数为<code>(cpu nums + 3) / 4</code>，cpu越多效果越好，但cpu不足4个时，一半运算能力分到执行收集器线程，导致用户线程执行速度降低；</p>
</li>
<li>CMS处理<strong>浮动垃圾</strong>。CMS在并发清理时用户线程同时产生垃圾，这些垃圾（浮动垃圾）需等到下次GC再清理。因此，CMS触发时机（<code>-XX:CMSInitiatingOccupancyFraction</code>，触发百分比）不能等到老年代空间用得太满，否则CMS运行需要内存，再加上同时产生的浮动垃圾，会出现<code>”Concurrent Mode Failure”</code>，之后会临时启用Serial Old，stop the world时间就很长了。</li>
<li>CMS毕竟是<strong>标记清除算法实现</strong>，<strong>会产生空间碎片</strong>。通过<code>-XX:UseCMSCompactAtFullCollection</code>开启内存碎片整理（默认开启），无法并发但停顿时间变长，通过<code>-XX:CMSFullGCsBeforeCompaction=0</code>（默认为0），开启每次进入FullGC时都进行碎片整理。</li>
</ul>
<h4 id="Parallel-Scavenge-Parallel-Old"><a href="#Parallel-Scavenge-Parallel-Old" class="headerlink" title="Parallel Scavenge/Parallel Old"></a>Parallel Scavenge/Parallel Old</h4><p>Parallel Scavenge：</p>
<ul>
<li><p>负责新生代收集，收集器也经常被称为“<strong>吞吐量优先</strong>”收集器，适合后台运算而不需要太多交互的任务，<code>-XX:MaxGCPauseMillis</code>设置停顿时间和<code>-XX:GCTimeRatio</code>设置吞吐量大小。而CMS是尽可能缩短stop the world的时候，适合与用户交互的任务。</p>
</li>
<li><p>Parallel Scavenge在复制算法、并行方面<strong>和ParNew类似</strong>，主要区别是Parallel Scavenge具有自适应调节策略，使用<code>-XX:+UseAdaptiveSizePolicy</code>，无需手工指定<code>-Xmn</code>、<code>-XX:SurvivorRatio</code>、<code>-XX:PretenureSizeThreshold</code>等细节参数。</p>
</li>
</ul>
<p>Parallel Old：</p>
<ul>
<li>是Parallel Scavenge的老年代版本，使用<strong>多线程和标记整理算法</strong>；</li>
<li>该搭配组合<strong>适合注重吞吐量和CPU资源敏感</strong>的场合。</li>
</ul>
<h4 id="G1"><a href="#G1" class="headerlink" title="G1"></a>G1</h4><p>G1垃圾收集器在JDK7u4及之后的版本开始提供，通过参数<code>-XX:+UseG1GC</code>开启。 它的设计目标是为了<strong>适应现在不断扩大的内存和不断增加的处理器数量</strong>，进一步降低暂停时间（pause time），同时兼顾良好的吞吐量。</p>
<p>其特点有：</p>
<ul>
<li>一款面向服务端应用的垃圾收集器，基于Region的GC，适用于大内存机器。即使内存很大，Region扫描，性能还是很高的；</li>
<li>具有并行并发、分代收集、空间整合以及可预测停顿的特点：<ul>
<li><strong>初始标记</strong>：标记一下GC Roots能直接关联到的对象，伴随着一次普通的Young GC发生，并修改NTAMS（Next Top at Mark Start）的值，让下一阶段用户程序并发运行时，能在正确可用的Region中创建新对象，<strong>存在STW操作</strong>；</li>
<li>并发标记：是从GC Roots开始堆中对象进行可达性分析，找出存活的对象，这阶段耗时较长，但可与用户程序并发执行，该阶段可以被Young GC中断;</li>
<li><strong>最终标记</strong>：是<strong>为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录</strong>，虚拟机将这段时间对象变化记录在线程Remembered Set Logs里面，最终标记阶段需要把Remembered Set Logs的数据合并到Remembered Set中，<strong>此阶段是stop-the-world操作</strong>，使用snapshot-at-the-beginning (SATB) 算法;</li>
<li>筛选回收：<strong>首先对各个Region的回收价值和成本进行排序，根据用户所期望的GC停顿时间来制定回收计划，回收没有存活对象的Region并加入可用Region队列</strong>。这个阶段也可以做到与用户程序一起并发执行，但是因为只回收一部分Region，时间是用户可控制的，而且停顿用户线程将大幅提高收集效率。</li>
</ul>
</li>
<li>将整个Java堆分为多个大小相等的<strong>Region</strong>（1-32M，2的次幂，通过参数<code>-XX:G1HeapRegionSize</code>设置），统一进行管理。<strong>Region可以是S、Eden、Old等角色</strong>，底层保留新生代和老年代的概念，不再物理隔离，在运行中动态调整，不用预期按比例划分；</li>
<li>优先回收价值最大的Region，使得在有限时间内尽可能提高收集效率（Garbage First）。</li>
</ul>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul>
<li><a href="http://book.douban.com/subject/6522893/" target="_blank" rel="noopener">深入理解Java虚拟机：JVM高级特性与最佳实践</a></li>
<li><a href="http://www.cnblogs.com/zuoxiaolong/p/jvm9.html" target="_blank" rel="noopener">JVM内存管理</a></li>
<li><a href="https://www.jianshu.com/p/aef0f4765098" target="_blank" rel="noopener">G1垃圾回收器详解</a></li>
<li><a href="https://snailclimb.gitee.io/javaguide/#/docs/java/jvm/JVM%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6?id=_47-g1-%e6%94%b6%e9%9b%86%e5%99%a8" target="_blank" rel="noopener">JVM垃圾回收</a>，JavaGuide</li>
<li><a href="https://mp.weixin.qq.com/s/fhC5X7eNLmZxeCH9b9cS6A" target="_blank" rel="noopener">一篇文章彻底搞懂CMS与G1</a></li>
<li><a href="https://www.jianshu.com/p/ab54489f5d71?utm_campaign=haruki&amp;utm_content=note&amp;utm_medium=reader_share&amp;utm_source=weixin" target="_blank" rel="noopener">CMS与G1的区别</a></li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>GC</tag>
      </tags>
  </entry>
  <entry>
    <title>Calcite原理和经验总结</title>
    <url>/blog/b227762f.html</url>
    <content><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p><a href="http://calcite.apache.org/" target="_blank" rel="noopener">Calcite</a>（最初被命名为optiq，由Julian Hyde编写，之后成为apache项目）是一个动态数据管理框架，不考虑数据的存储、处理数据的算法以及元数据的保存问题，只保留了重要的数据库管理功能，成为应用程序和多个数据源交互的中介。</p>
<p>Optiq起初在<a href="http://hive.apache.org/" target="_blank" rel="noopener">Hive</a>项目中，为其提供成本优化模型，即CBO(Cost Based Optimization)。它是面向<a href="http://hadoop.apache.org/" target="_blank" rel="noopener">Hadoop</a>新的查询引擎，提供了OLAP和流SQL查询引擎。当前，还应用于<a href="https://flink.apache.org/" target="_blank" rel="noopener">Flink</a>解析和流SQL处理、<a href="http://drill.apache.org/" target="_blank" rel="noopener">Drill</a>的解析和JDBC接口等、<a href="http://kylin.apache.org/" target="_blank" rel="noopener">Kylin</a>的OLAP。<br>Calcite的目标是一种方案适应所有需求场景（one size fits all），希望能够为不同计算平台提供统一查询引擎，让访问hadoop上的数据跟传统数据库访问方式一样（SQL和高级查询优化）。</p>
<a id="more"></a>
<h3 id="主要特性"><a href="#主要特性" class="headerlink" title="主要特性"></a>主要特性</h3><ul>
<li>SQL解析、验证和优化，支持标准函数和聚合函数，提供JDBC驱动查询能力；</li>
<li>连接不同前端（SQL、Pig等翻译为关系代数）和不同后端（适配器对接各种存储接口）；</li>
<li>支持关系代数、定制逻辑规则和基于CBO和RBO优化的查询引擎；</li>
<li>物化视图管理以及物化视图Lattice和Tile机制来应用于OLAP分析；</li>
<li>支持对流数据的查询。</li>
</ul>
<h3 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h3><p><img src="/blog/b227762f/calcite_architect.png" alt></p>
<p>Calcite框架中的主要接口，都可以单独被集成使用。例如：</p>
<ul>
<li>提供JDBC接口实现、SQL的解析和元数据校验、关系代数转换为执行计划（CBO和RBO）等；</li>
<li>JDBC Client对外通过驱动类加载访问JDBC Server, Server服务通过Jetty对外提供；</li>
<li>SQLParser将SQL解析成SqlNode，并通过Validator验证SqlNode信息；Operator Exp将SqlNode转为RelNode树</li>
<li>QueryOptimizer将RelNode基于规则或成本优化执行计划。</li>
</ul>
<h3 id="示例解析"><a href="#示例解析" class="headerlink" title="示例解析"></a>示例解析</h3><p>执行SQL：select a,b,c from tab where a = 1<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">--LogicalFilter</span><br><span class="line">----LogicalProject</span><br><span class="line">------LogicalTableScan</span><br></pre></td></tr></table></figure></p>
<p>优化方式：<br>1）RBO：预先定义一些规则，来优化执行计划（HepPlanner）<br>比如先过滤，再投影，可以减小数据量，优化如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">--LogicalProject</span><br><span class="line">---- LogicalFilter</span><br><span class="line">------LogicalTableScan</span><br></pre></td></tr></table></figure></p>
<p>2）CBO: 计算SQL所有可能执行的代价，选择一个代价较低(VolcanoPlanner)<br>计算LogicalProject、LogicalFilter、LogicalTableScan转为不同的执行计划所具有的代价，选择不是最坏的、相对较小的。</p>
<h3 id="实战总结"><a href="#实战总结" class="headerlink" title="实战总结"></a>实战总结</h3><p>以上介绍了Calcite的基本特性和原理，以及在大数据计算引擎领域的应用情况。以下是本人在实际使用过程中（1.5版本），遇到的一些坑，方便遇到同样问题的开发者排查问题。</p>
<ul>
<li>对于应用程序已经打开的JDBC连接，新增的表无法感知，需要重新建立连接，重新加载元数据；</li>
<li>对于长度很长的大SQL，在翻译为物理执行计划（对接底层存储引擎的处理逻辑）时，有可能会超过Janino约束的64KB大小，导致物理计划生成失败；</li>
<li>对于大SQL中的组合条件嵌套不可以太深，否则会导致SQL解析器报栈溢出；</li>
<li>Calcite作为计算框架的SQL引擎，除了下推到底层存储的算子操作，其他都是在内存中计算的，因此对资源的要求比较高；</li>
<li>同样的SQL或者预编译SQL生成的物理执行计划，目前发现JVM装载完成后并没有复用，持续的查询反而造成了方法区Class暴增，导致permgen overflow。然后，修改源码将每次创建的Statement强制关闭，使得FullGC时Class可以被unload，避免permgen overflow。（该问题17年csdn上提问，18年<a href="https://ask.csdn.net/questions/361852?spm=1001.2014.3001.5501" target="_blank" rel="noopener">有网友回复</a>）</li>
</ul>
<h3 id="扩展阅读"><a href="#扩展阅读" class="headerlink" title="扩展阅读"></a>扩展阅读</h3><ul>
<li><a href="https://biz.51cto.com/art/201906/598280.htm" target="_blank" rel="noopener">一次性搞清楚线上CPU100%，频繁FullGC排查套路</a></li>
</ul>
]]></content>
      <categories>
        <category>计算引擎</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>Calcite</tag>
        <tag>优化器</tag>
      </tags>
  </entry>
  <entry>
    <title>一个技术人30岁的自白</title>
    <url>/blog/c6c421ae.html</url>
    <content><![CDATA[<p>2018年度过了30岁生日，总结过去，展望未来，这次也打算写点东西，也算是对过去的一个交代，希望自己今后能够更加明确自己的道路，全身心地投入，平衡好工作和生活。2008年进入大学校园，从事计算机相关学习，算上本科和研究生，一共是七年的时间。2015年进入到现在的公司工作，到现在也有了三年半的时间。从求学到工作再到家庭，一路走来算是平平淡淡，有过开心也有过失落，如今又多了几分责任，2019年初即将迎接我的宝宝。<br><a id="more"></a></p>
<p><strong>人生中非常重要的一个阶段–20岁到30岁，这是一个学习和积累的时间段，也是积累高效的学习能力和解决问题的能力的时期</strong>。</p>
<p>2008年开始四年的本科学习生涯，当时的重心都放在了一些课程的学习和考试上，并没有什么职业规划的概念。记得当时，大家都在考公务员、教师、参加各种培训来选择毕业后的工作和发展。而我不知道是否是逃避就业，还是想有更好的发展平台，总之当时和几个同学一起加入了考研的队列。备战考研，始于大三的暑假，起早贪黑，找教室座位，复习知识点，做题目，日复一日，等待考试的那一刻。由于全部是自主学习，不像高考时老师带着，有点苦学的感觉，复习的效果和时间进度并不是很理想，最终还是坚持了下来。毕业设计也同样顺利地完成系统开发、论文撰写，帮助了身边很多的同学。<strong>这一阶段，我最大的收获，并不是在专业理论上，而是干一件事的专注度、进取心的增强</strong>。</p>
<p>2012年进入研究生阶段后，第一学年是以基本理论课程为主，同样延续着本科的学习节奏完成了这一阶段的任务，和一直没过的CET6。第二学年开始论文选题、阅读核心期刊文献、发表论文、写毕业论文，<strong>这一阶段的收获不在于论文的撰写，而是经历了根据给定课题，如何去检索前沿文献、对比总结已有研究成果，得出自己的一些思考和想法</strong>。</p>
<p>当然，最重要的事情还是要能够找到心仪的、感兴趣的工作。本科研究生阶段对编程还是很感兴趣的，课程设计、毕业设计、上机操作、程序设计比赛等等，都有参与过并能够完成得很好。当时自己对行业的区别、业务场景并没有什么概念，校园期间只要是感兴趣的技术，都会去了解并学习。本科期间，对Spring技术栈用得比较多，当时主要围绕Java开发做了一些项目，前后端都能基本应付。读研期间，由于课题实验要求，主要围绕兴起的大数据技术和经典的数据挖掘算法来完成论文实验需要。平时，也会关注一些大牛的博客、GITHUB等技术网站。所以，当时给自己的定位就是找Java或大数据相关的开发岗位。临近毕业，就开启了跑各种宣讲会的模式，夸校园、夸城市，到处地跑。<strong>这个阶段，给我最大的感受：以前学的计算机基础课程，技术书籍的阅读量，框架源码研究，算法等方面都是非常重要的，要持续地积累，形成自己的技术优势，才能符合一线大厂的要求</strong>。</p>
<p>2015年开始正式参加工作，进入了数据分析部门，遇到同校的学姐（也是我的组长），还蛮开心的。主要工作是设计和开发一个数据分析平台，从RP框图、技术选型、系统设计到最后编码出demo，在师傅的带领下，入职的这一年确实成长了很多，还要再次感谢部门领导和组长的栽培。由于客观原因，项目中止，另外考虑到个人发展，我申请调到数据应用平台，从事RPC服务框架、大数据平台SQL组件等方面的开发。这三年多的工作经历，使我在编码能力上有了一定的提升，但我知道还要多看优秀项目的源代码，现在想想GITHUB就是程序员的天堂，设计能力和解决问题的能力还需要进一步加强；技术广度上，实现一套高可用、高性能、可扩展的系统架构、技术选型已经是很高的要求了，这是我一直努力的目标；技术深度上，对分布式系统充满了浓厚的兴趣，比如在通信机制、存储结构、检索算法等方面。<strong>这个阶段，我的收获是：更加明确了个人的发展道路，技术深度还有待提高，制定计划并有效地实施显得至关重要，毕竟机会都是留给有准备的人的。沟通和表达能力也需要提高，建立个人影响力、推销自己、赢得领导的信任是一门学问</strong>。</p>
<p>十年的时间，走过一些弯路，悟到一些道理。希望下一个5到10年，能够走好自己的路，做一个懂得“投资”的人。2019年我就要当爸爸了，感恩生活，珍惜生活，希望我们每个技术人都能把握新的机遇，创造更多的可能性。</p>
<p>附赠: <a href="http://naotu.baidu.com/file/276fd9839e8c28f0e121b1334464d798?token=595143b48bb3296d" target="_blank" rel="noopener">架构师技能图谱思维导图链接</a>, 来自<a href="https://www.geekbang.org/" target="_blank" rel="noopener">极客专栏</a>整理。</p>
]]></content>
      <categories>
        <category>个人日志</category>
      </categories>
      <tags>
        <tag>职场感悟</tag>
      </tags>
  </entry>
  <entry>
    <title>基于Arthas的Java应用线上诊断方法</title>
    <url>/blog/8c700280.html</url>
    <content><![CDATA[<h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>针对Java应用在生产环境下出现的问题，通常开发者想通过远程debug的方式来排查问题是不可行的。一是私有云的客户系统无法连接去调试，连接上也会由于debug断点导致所有业务请求被暂停。二是需要额外添加日志来部署到线上观察，效率非常低下。当然，线上问题远不止这些，还需要结合Linux和JVM指令来监控系统整体运行指标。</p>
<p>因此，为了能够解决上述常规排查手段的弊端，这里介绍一款开源的Java诊断工具<a href="https://alibaba.github.io/arthas/" target="_blank" rel="noopener">Arthas</a>，该工具提供了jvm整体监控、线程堆栈、类加载器检查、方法级别的监控等丰富的操作，来定位在线Java应用问题。另外，值得注意的是由于该工具采用字节码植入方式，对于应用的运行性能和安全性需要考虑一下。<br><a id="more"></a></p>
<h3 id="使用说明"><a href="#使用说明" class="headerlink" title="使用说明"></a>使用说明</h3><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>下载<code>arthas-packing-xxx-bin.zip</code>直接解压，进行本地安装，默认安装在<code>user.home</code>的用户目录下面。</p>
<ul>
<li>启动：<br>执行<code>java –jar arthas-boot.jar</code>，进入交互式命令行界面，选择需要监控的java应用进程号，控制台打印attach process xxx success 表示连接成功，输入help显示指令帮助说明，表示可以开始正式使用了。注意：如果没有第一步的本地安装，执行启动命令，会访问网络下载安装，内网用户将会无法使用。</li>
<li>卸载：<br>标准安装的话，安装目录在<code>~/.arthas</code>，执行<code>rm –rf ~/.arthas</code>即可。否则直接删除当前所在目录的sh和jar。</li>
</ul>
<h4 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h4><ul>
<li><p>基础命令<br><code>help</code>——查看命令帮助信息<br><code>cls</code>——清空当前屏幕区域<br><code>session</code>——查看当前会话的信息<br><code>reset</code>——重置增强类，将被 Arthas 增强过的类全部还原，Arthas 服务端关闭时会重置所有增强过的类<br><code>version</code>——输出当前目标 Java 进程所加载的 Arthas 版本号<br><code>history</code>——打印命令历史<br><code>quit</code>——退出当前 Arthas 客户端，其他 Arthas 客户端不受影响<br><code>shutdown</code>——关闭 Arthas 服务端，所有 Arthas 客户端全部退出<br><code>keymap</code>——Arthas快捷键列表及自定义快捷键  </p>
</li>
<li><p>jvm相关<br><code>dashboard</code>——当前系统的实时数据面板<br><code>thread</code>——查看当前 JVM 的线程堆栈信息<br><code>jvm</code>——查看当前 JVM 的信息<br><code>sysprop</code>——查看和修改JVM的系统属性<br><code>sysenv</code>——查看JVM的环境变量<br><code>getstatic</code>——查看类的静态属性</p>
</li>
<li><p><code>class</code>/<code>classloader</code>相关<br><code>sc</code>——查看JVM已加载的类信息<br><code>sm</code>——查看已加载类的方法信息<br><code>dump</code>——dump 已加载类的 byte code 到特定目录<br><code>redefine</code>——加载外部的.class文件，redefine到JVM里<br><code>jad</code>——反编译指定已加载类的源码<br><code>classloader</code>——查看classloader的继承树，urls，类加载信息，使用classloader去getResource</p>
</li>
<li><p><code>monitor/watch/trace</code>相关<br>请注意，这些命令，都通过字节码增强技术来实现的，会在指定类的方法中插入一些切面来实现数据统计和观测，因此在线上、预发使用时，请尽量明确需要观测的类、方法以及条件，诊断结束要执行 <code>shutdown</code> 或将增强过的类执行 <code>reset</code> 命令。<br><code>monitor</code>——方法执行监控<br><code>watch</code>——方法执行数据观测<br><code>trace</code>——方法内部调用路径，并输出方法路径上的每个节点上耗时<br><code>stack</code>——输出当前方法被调用的调用路径<br><code>tt</code>——方法执行数据的时空隧道，记录下指定方法每次调用的入参和返回信息，并能对这些不同的时间下调用进行观测  </p>
</li>
<li><p><code>options</code><br><code>options</code>——查看或设置Arthas全局开关</p>
</li>
<li><p>管道<br>Arthas支持使用管道对上述命令的结果进行进一步的处理，如<code>sm org.apache.log4j.Logger | grep &lt;init&gt;</code><br>grep——搜索满足条件的结果<br>plaintext——将命令的结果去除颜色<br>wc——按行统计输出结果  </p>
</li>
<li><p>后台异步任务<br>当线上出现偶发的问题，比如需要watch某个条件，而这个条件一天可能才会出现一次时，异步后台任务就派上用场了<br>使用 <code>&gt;</code> 将结果重写向到日志文件，使用 <code>&amp;</code> 指定命令是后台运行，session断开不影响任务执行（生命周期默认为1天）<br><code>jobs</code>——列出所有job<br><code>kill</code>——强制终止任务<br><code>fg</code>——将暂停的任务拉到前台执行<br><code>bg</code>——将暂停的任务放到后台执行  </p>
</li>
<li><p>ognl相关<a href="https://commons.apache.org/proper/commons-ognl/language-guide.html" target="_blank" rel="noopener">官方文档</a><br>一般配合watch指令使用，其中的express、condition-express 采用ognl表达式<br>用于监控成员方法和类级别静态方法的入参(<code>params</code>)、返回值(<code>returnObj</code>)、抛出异常(<code>throwExp</code>)等。示例：  </p>
<p>执行一次查看Test类的test方法的第一个参数(list)的第一个对象(pojo)中的age属性<br><code>watch Test test params[0].get(0).age -n 1</code><br>//还可以通过下标的方式访问params[0][0][“age”] ，这个写法等效于params[0][0].age</p>
<p> 执行一次查看Test类的test方法的第一个参数(list)的所有对象(pojo)中age&gt;5的姓名<br> <code>watch Test test &quot;params[0].{? #this.age &gt; 5}.{name}&quot; -n 1</code><br> //那如果要找到第一个age大于5的Pojo的name，可以用^ 或$ 来进行第一个或最后一个的匹配</p>
<p> 执行一次查看Test类中的静态变量m<br> <code>watch Test test &#39;@Test@m&#39; -n 1</code></p>
</li>
</ul>
<h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><ul>
<li><p>定位内存泄露、资源未释放等问题：<br>查看系统整体运行情况，包括线程概览、堆与非堆内存使用、GC次数与耗时、操作系统等信息<br><code>dashboard –i 10000</code> //十秒钟刷新一次</p>
</li>
<li><p>定位有问题的处理逻辑<br>查看前3个占用cpu比较高的线程堆栈<br><code>thread –n 3</code></p>
</li>
<li><p>定位jar包冲突的问题<br>查看使用的类所在的jar以及类加载器<br><code>sc –d 全路径类名`</code></p>
</li>
<li><p>性能测试，统计方法的执行平均耗时、成功/失败次数统计<br>确认要统计的业务类中的方法名<br><code>monitor 全路径类名 方法名 -c 1</code></p>
</li>
<li><p>定位耗时的业务逻辑处理，从而有针对性地优化<br>通过trace方法查看指定方法的调用堆栈和最耗时的方法<br><code>trace全路径类名 方法名</code></p>
</li>
<li><p>查看方法的入参和返回值，确认业务逻辑处理的正确性<br>通过watch和ognl表达式来检测结果<br><code>watch 全路径类名 方法名 ‘params[0] + “, ”+ returnObj’</code></p>
</li>
</ul>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>以上介绍了Arthas是如何解决以往线上Java应用程序诊断的不足带来的问题，用好这些命令还需要多多结合实际问题去实践。最重要的，还是需要具备程序运行诊断的思路，工具只是辅助我们采集想要的信息，并且需要具备从这些信息中分析出问题，给出解决方案的能力。</p>
]]></content>
      <categories>
        <category>运维工具</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>故障诊断</tag>
      </tags>
  </entry>
  <entry>
    <title>24个职场感悟-阿里技术专家至简</title>
    <url>/blog/55ff8bff.html</url>
    <content><![CDATA[<p>至简以自传的形式讲述了他的<a href="https://mp.weixin.qq.com/s/kI1u0ENe1tDTI2DXUJG4JA" target="_blank" rel="noopener">成长历程</a>，并提炼出24个职场感悟。这些经历和感悟，本人读完后觉得对日常的工作、学习和写作具有一定的指导意义，故整理如下和读者们共同学习成长。</p>
<ul>
<li><strong> 自学能力是竞争力之本 </strong></li>
<li>自信能让你与众不同，尽管有时的自信有点莫名其妙</li>
<li>兴趣是学习效率的催化剂，培养自己的职业兴趣</li>
<li><strong> 学习应给自己设置虚拟的项目目标，以做项目的形式提升学习效果，只有这样学到的内容才会深入而实用，切忌无目标地学到哪算哪 </strong><a id="more"></a></li>
<li>话语权首先来自能力，而不是职位权力</li>
<li><strong> 难学的技能一旦掌握更具竞争优势 </strong></li>
<li>用阶段性成果不断增强自己的自信，且最终支持自信的是能力，而不是自大</li>
<li>做自己喜欢的事，如果那是自己的兴趣最好</li>
<li><strong> 不论身处多么困难的环境，即使觉得前途渺茫，也不要放弃学习，否则就是“自断筋脉” </strong></li>
<li><strong> 长期安逸的工作意味着将来更大的风险 </strong></li>
<li><strong> 机遇很重要，但你得有能力才能抓住它 </strong></li>
<li>职场首先比拼的不是智商，而是坚持与好习惯</li>
<li>当短期利益与长远利益无法得兼时，选择长远利益</li>
<li>学历是很重要的敲门砖，即便你的能力很强；学历尽管很重要，但能力才是最终的通行证</li>
<li>技术细节掌握得越深，解决问题时就越能游刃有余</li>
<li><strong> 技能的发展应采取深度先于广度且交替进行的方式，只有这样，面对大量的新知识才能更淡定 </strong></li>
<li>越难的技术问题，其所蕴藏的知识越丰富，也越具学习价值</li>
<li>每次积累的点滴知识，一定会在将来不知不觉地发挥效能</li>
<li><strong> 通过文档化的方式传承知识给后继者是你的基本责任，因为你作为后继者时也希望如此，这也是对自己负责的一种表现 </strong></li>
<li><strong> 别人对你价值的认可，其实不是简单地根据你的自身能力，而是根据你对他人和团队的贡献 </strong></li>
<li>英语的听说能力只要有合适的环境，并勇于张嘴练习的情况下能快速地提高，不必担心</li>
<li>在软件开发活动中，应设法通过有效的技术途径去解决工程困境</li>
<li><strong> 不要用沉默的方式一味地迎合别人的要求，据理力争或许才是作为的表现 </strong></li>
<li><strong> 流程、文档的作用，不只是引导我们做完事，更能规范我们的行为和帮助培养工作习惯 </strong></li>
</ul>
<blockquote><p>尽管我们常将“职业规划”挂在嘴边，实际上职场发展真的是一种“布朗运动”。你不知道下一站会是哪、也不知道后面将要从事什么工作、更不清楚后面会碰到怎样的老板。在众多不确定因素面前，或许以上总结出的职场感悟能让你不断地朝好的方向发展。</p>
<footer><strong>至简</strong><cite>阿里巴巴高级技术专家集团Service Mesh方向的重要参与者和推动者</cite></footer></blockquote>
]]></content>
      <categories>
        <category>个人日志</category>
      </categories>
      <tags>
        <tag>职场感悟</tag>
        <tag>技术管理</tag>
      </tags>
  </entry>
  <entry>
    <title>工作中如何做好技术积累</title>
    <url>/blog/759b03e6.html</url>
    <content><![CDATA[<p><strong>文章转自：<a href="https://tech.meituan.com/study_vs_work.html" target="_blank" rel="noopener">美团技术团队</a> @ 刘丁</strong></p>
<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>古人云：“活到老，学到老。”互联网算是最辛苦的行业之一，“加班”对工程师来说已是“家常便饭”，同时互联网技术又日新月异，很多工程师都疲于应付，叫苦不堪。以至于长期以来流传一个很广的误解：35岁是程序员工作的终点。</p>
<p>如何在繁忙的工作中做好技术积累，构建个人核心竞争力，相信是很多工程师同行都在思考的问题。本文是我自己的一些总结，试图从三个方面来解答：</p>
<ul>
<li><p>第一部分阐述了一些学习的原则。任何时候，遵循一些经过检验的原则，都是影响效率的重要因素，正确的方法是成功的秘诀。</p>
</li>
<li><p>提升工作和学习效率的另一个重要因素是释惑和良好心态。第二部分分析了我在工作中碰到和看到的一些典型困惑。</p>
</li>
<li><p>成为优秀的架构师是大部分初中级工程师的阶段性目标。第三部分剖析架构师的能力模型，让大家对目标所需能力有一个比较清晰的认知。</p>
</li>
</ul>
<a id="more"></a>
<h2 id="如何学习"><a href="#如何学习" class="headerlink" title="如何学习"></a>如何学习</h2><p>在繁忙的工作中，持之以恒、不断学习和进步是一件艰巨的任务，需要坚强的毅力和坚定的决心。如果方法不得当，更是事倍功半。幸好我们的古人和现在哲人已经总结了很多优秀的学习方法论，这里汇总了一些重要原则。遵循这些方法必会对大家的工作学习大有裨益。</p>
<h3 id="贵在坚持"><a href="#贵在坚持" class="headerlink" title="贵在坚持"></a>贵在坚持</h3><p>有报道指出，过去几十年的知识量超过之前人类几千年的知识量总和。而计算机领域绝对是当代知识更新最快的领域之一，因此，工程师必须要接受这样一个现实，现在所掌握的深厚知识体系很快就会被淘汰。要想在计算机领域持续做优秀架构师，就必须不停的学习，掌握最新技术。总之，学不可以已。</p>
<p>所谓“冰冻三尺，非一日之寒，水滴石穿，非一日之功”，通往架构师的道路漫长而又艰巨，轻易放弃，则所有付出瞬间付之东流。要想成为优秀的架构师，贵在坚持！</p>
<p>虽然知识更新很快，但是基础理论的变化却非常缓慢。这就是“道”和“象”关系，纵是世间万象，道却万变不离其宗。对于那些非常基础的理论知识，我们需要经常复习，也就是“学而时习之”。</p>
<h3 id="重视实践"><a href="#重视实践" class="headerlink" title="重视实践"></a>重视实践</h3><p>古人云：“纸上得来终觉浅，绝知此事要躬行。” 学习领域有所谓721模型：个人的成长70%来自于岗位实践，20%来自向他人学习，10%来自于培训。虽然这种理论存在争议，但对于工程师们来说，按照实践、学习和培训的方式进行重要性排序，大致是不错的。所以重视实践，在实践中成长是最重要的学习原则。</p>
<p>人类的认知有两种：感性认知和理性认知。这两种认知互相不可替代性。实践很大程度来自于感性学习，看书更像是理性学习。以学开汽车做例子，很难想象什么人能够仅仅通过学习书本知识就会开汽车。</p>
<p>书本知识主要是传道——讲述抽象原型，而对其具体应用场景的讲述往往含糊其辞，对抽象原型之间的关系也是浅尝辄止。采用同样精确的语言去描述应用场景和关联关系将会失去重点，让人摸不着头脑。所以，仅仅通过看书来获得成长就像是用一条腿走路。</p>
<p>重视实践，充分运用感性认知潜能，在项目中磨炼自己，才是正确的学习之道。在实践中，在某些关键动作上刻意练习，也会取得事半功倍的效果。</p>
<h3 id="重视交流"><a href="#重视交流" class="headerlink" title="重视交流"></a>重视交流</h3><p>牛顿说：“如果说我看得比别人远一些，那是因为我站在巨人的肩膀上。”我们需要从别人身上学习。从老师、领导、同事、下属甚至对手身上学习，是快速成长的重要手段。</p>
<p>向老师和领导学习已经是人们生活习惯的一部分了。但是从同事甚至对手那里学习也很重要，因为这些人和我们自身更相似。所以要多多观察，取其所长，弃其所短。对于团队的小兄弟和下属，也要“不耻下问”。</p>
<p>此外，在项目中积极参与具体方案讨论也非常重要。参与者先验感知了相关背景，并且讨论的观点和建议也是综合了发言者多种知识和技能。所以，讨论让参与者能够非常全面，立体地理解书本知识。同时，和高手讨论，他们的观点就会像修剪机剪树枝一样，快速的剪掉自己知识领域里面的疑惑点。</p>
<h3 id="重视总结和输出"><a href="#重视总结和输出" class="headerlink" title="重视总结和输出"></a>重视总结和输出</h3><p>工程师在实践中会掌握大量细节，但是，即使掌握了所有细节，却没有深刻的总结和思考，也会陷入到“学而不思则罔”的境地。成长的“量变”来自于对细节的逐渐深入地把控，而真正的“质变”来自于对“道”的更深层次的理解。</p>
<p>将经验输出，接受别人的检验是高层次的总结。这种输出不仅帮助了别人，对自身更是大有裨益。总结的方式有很多，包括组织分享，撰写技术文章等等。当然“日三省吾身”也是不错的总结方式。总之，多多总结，多多分享，善莫大焉！</p>
<p>解答别人的问题也是个人成长的重要手段。有时候，某个问题自己本来不太懂，但是在给别人讲解的时候却豁然开朗。所以，“诲人不倦”利人惠己。</p>
<h3 id="重视规划"><a href="#重视规划" class="headerlink" title="重视规划"></a>重视规划</h3><p>凡事预则立，不预则废。对于漫长的学习生涯而言，好的计划是成功的一半。</p>
<h4 id="长期规划"><a href="#长期规划" class="headerlink" title="长期规划"></a>长期规划</h4><p>长期规划的实施需要毅力和决心，但是做正确的长期规划还需要高瞻远瞩的眼界、超级敏感的神经和中大奖的运气。对于大部分人来说，长期规划定主要是“定方向”。但遵循如下原则能够减少犯方向性错误的概率：</p>
<ul>
<li>远离日暮西山的行业。</li>
<li>做自己感兴趣的事情。</li>
<li>做有积累的事情。</li>
<li>一边走一边看，切勿一条道走到黑。</li>
</ul>
<h4 id="短期规划"><a href="#短期规划" class="headerlink" title="短期规划"></a>短期规划</h4><p>良好的短期规划应该在生活、成长、绩效和晋升之间取得平衡。大部分公司都会制定一个考核周期——少则一个月，多则一年。所以不妨以考核周期作为短期学习规划周期。本质上，规划是一个多目标优化问题，它有一系列的理论方案，这里不一一细说。基于相关理论，我给出一个简单易行的方案：</p>
<ul>
<li>确定目标优先级。比如：成长、生活、绩效。</li>
<li>确定每个目标的下限。从优化理论的角度来看，这被称为约束。比如绩效必须在一般以上，之前已经规划好的旅行不能更改，必须读完《Effective Java》等等。</li>
<li>优先为下限目标分配足够的资源。比如，事先规划好的旅行需要10天，这10天就必须预算出去。</li>
<li>按照各主目标的顺序依次分配资源。比如，最终分配给学习的时间是10天。</li>
<li>在给定的学习预算下，制定学习目标，要激进。然后给出执行方案。比如，学习目标是掌握基本的统计学知识，并成为Java专家。具体方案为：完成《Effective Java》、《Java Performance》、《Design Pattern》、《Head First Statistics》四本书的阅读。</li>
<li>对规划中的各学习任务按目标优先级进行排序，并最先启动优先级最高的任务。比如，最高优先级是掌握统计理论，那么就要先看《Head First Statistics》。</li>
</ul>
<p>对于该方案，要注意以下几点：</p>
<ul>
<li>最低目标必须能够轻松达成的目标，否则，从优化理论的角度来讲，该命题无解。比如，类似“半年内完成晋级两次、绩效全部S、从菜鸟成为Java专家”就不太合适作为最低目标。总之，要区分理想和梦想。</li>
<li>主要目标规划必须具备一定的挑战性，需要规划出不可能完成的目标。过度规划本质上是一种贪婪算法，目的是目标价值最大化。因为一切皆有变数，如果其他目标能够提前完成，就不妨利用这些时间去完成更多的学习目标。总之，前途必须光明，道路必须坎坷。</li>
<li>各目标之间不一定共享资源，规划不一定互有冲突。</li>
</ul>
<p>此外，短期规划还可以从如下几个方面进行优化：</p>
<ul>
<li>学习计划最好能结合工作计划，理论联系实际结合，快速学以致用。比如，本季度规划去做一些数据分析工作，那么不妨把学习目标设置为学习统计知识。</li>
<li>要灵活对待规划的目标和具体执行步骤，需要避免“郑人买履”式的笑话。面临新的挑战和变化，规划需要不断地调整。</li>
</ul>
<h2 id="那些令人纠结的困惑"><a href="#那些令人纠结的困惑" class="headerlink" title="那些令人纠结的困惑"></a>那些令人纠结的困惑</h2><p>人生是一场马拉松，在漫长的征途中，难免有很多困惑。困惑就像枷锁，使我们步履蹒跚，困惑就像死锁，让我们停滞不前。</p>
<p>接下来我将总结自己在工作中碰到和看到的一些典型困惑。这些困惑或者长期困扰作者本人，或者困扰我身边的同事和朋友。当这些困惑被释然之后，大家都感觉如重获释，为下一阶段的征程提供满满的正能量。人生就像一场旅途，不必在乎目的地，在乎的，应该是沿途的风景，以及看风景的心情。良好的心态是技术之旅最好的伴侣。期望通过这个解惑之旅，让大家拥有一个愉快的心情去感受漫长的学习旅途。</p>
<h3 id="学无止境吗"><a href="#学无止境吗" class="headerlink" title="学无止境吗"></a>学无止境吗</h3><p>必须要承认一个残酷的现实：人的生命是有限的，知识却是无限的。用有限的生命去学习无限的知识是不可能完成的任务。一想到此，有些工程师不免产生一些悲观情绪。如果方法得当并且足够勤奋，悲伤大可不必。</p>
<p>虽然，人类的整体知识体系一直在扩张。但是就很多重要的工程细分领域，基础理论并不高深。计算机的很多重要领域，工程师有能力在有限时间内抓住核心要害。</p>
<p>比如，密码学被认为是门非常高深的学科，但是一大类密码技术的基础是数论中一个非常简单的理论——素因数分解：给出两个素数，很容易算出它们的积，然而反过来给定两个素数的积，分解的计算量却非常惊人。</p>
<p>“一致性”算得上是计算机领域里面最经典的难题，它是所有分布式系统的基础，从多核多CPU到多线程，从跨机器到跨机房，无所不在，几乎所有的计算机从业人员都在解决这个问题，但是Paxos给出了一个很优雅的解决方案。</p>
<p>权限管理是很多工程师的噩梦，但如果你能搞定“Attribute Based Access Control(ABAC)”和“Role-Based Access Control(RBAC)”，也能达到相当高度。</p>
<p>另外，技术学习是一场对抗赛，虽然学无止境，超越大部分对手就是一种胜利。所以，以正确的学习方式，长时间投入就会形成核心竞争力。</p>
<h3 id="没有绝对高明的技术，只有真正的高手"><a href="#没有绝对高明的技术，只有真正的高手" class="headerlink" title="没有绝对高明的技术，只有真正的高手"></a>没有绝对高明的技术，只有真正的高手</h3><p>致力于在技术上有所成就的工程师，都梦想有朝一日成为技术高手。但技术高手的标准却存在很大的争议。这是一个有着悠久历史的误解：以某种技术的掌握作为技术高手的评判标准。我经常碰到这样一些情景：因为掌握了某些技术，比如Spring、Kafka、Elasticsearch等，一些工程师就自封为高手。有些工程师非常仰慕别的团队，原因竟是那个团队使用了某种技术。</p>
<p>这种误解的产生有几个原因：首先，技多不压身，技术自然是掌握的越多越好，掌握很多技术的人自然不是菜鸟。其次，在互联网时代来临之前，信息获取是非常昂贵的事情。这就导致一项技能的掌握可以给个人甚至整个公司带来优势地位。互联网时代，各种框架的出现以及开源的普及快速淘汰或者降低了很多技能的价值，同时降低了很多技术的学习门槛。所以，在当前，掌握某项技能知识只能是一个短期目标。怀揣某些技能就沾沾自喜的人需要记住：骄傲使人退步。</p>
<p>所谓麻雀虽小，五脏俱全。如果让你来做造物主，设计麻雀和设计大象的复杂度并没有明显区别。一个看起来很小的业务需求，为了达到极致，所需要的技术和能力是非常综合和高深的。真正的高手不是拿着所掌握的技术去卡客户需求，而是倾听客户的需求，给出精益求精的方案。完成客户的需求是一场擂台赛，真正的高手，是会见招拆招的。</p>
<h3 id="不做项目就无法成长吗"><a href="#不做项目就无法成长吗" class="headerlink" title="不做项目就无法成长吗"></a>不做项目就无法成长吗</h3><p>在项目中学习是最快的成长方式之一，很多工程师非常享受这个过程。但是一年到头都做项目，你可能是在一家外包公司。对于一个做产品的公司，如果年头到年尾都在做项目，要不然就是在初步创业阶段，要不然就是做了大量失败的项目，总之不算是特别理想的状态。正常情况，在项目之间都会有一些非项目时间。在这段时间，有些同学会产生迷茫，成长很慢。</p>
<p>项目真的是越多越好吗？答案显然是否定的。重复的项目不会给工程师们带来新的成长。不停的做项目，从而缺乏学习新知识的时间，会导致“做而不学则殆”。真正让工程师出类拔萃的是项目的深度，而不是不停地做项目。所以，在项目之间的空档期，工程师们应该珍惜难得的喘息之机，深入思考，把项目做深，做精。</p>
<p>如何提高项目的深度呢？一般而言，任何项目都有一个目标，当项目完成后，目标就算基本达成了。但是，客户真的满意了吗？系统的可用性、可靠性、可扩展性、可维护性已经做到极致了吗？这几个问题的答案永远是否定的。所以，任何一个有价值的项目，都可以一直深挖。深挖项目，深度思考还可以锻炼工程师的创造力。期望不停地做项目的人，就像一个致力于训练更多千里马的人是发明不出汽车的。锻炼创造力也不是一蹴而就的事情，需要长时间地思考。总之，工程师们应该总是觉得时间不够用，毕竟时间是最宝贵的资源。</p>
<h3 id="职责真的很小吗"><a href="#职责真的很小吗" class="headerlink" title="职责真的很小吗"></a>职责真的很小吗</h3><p>很多时候，一个工程师所负责系统的数量和团队规模与其“江湖地位”正相关。但是，江湖地位与技术成长没有必然关联。提升技术能力的关键是项目深度以及客户的挑剔程度。项目越多，在单个项目中投入的时间就越少，容易陷入肤浅。特别需要避免的是“ 在其位不谋其政”的情况。团队越大，在管理方面需要投入的精力就越多。在管理技巧不成熟，技术眼界不够高的前提强行负责大团队，可能会导致个人疲于应付，团队毫无建树。最终“ 一将无能，累死三军”，效果可能适得其反。</p>
<p>从技术发展的角度来说，技术管理者应该关注自己所能把控的活跃项目的数量，并致力于提高活跃项目的影响力和技术深度。团队人数要与个人管理能力、规划能力和需求把控能力相适应。一份工作让多个人来干，每个人的成长都受限。每个人都做简单重复的工作，对技术成长没有任何好处。团队管理和项目管理需要循序渐进，忌“拔苗助长”。</p>
<h3 id="一定要当老大吗"><a href="#一定要当老大吗" class="headerlink" title="一定要当老大吗"></a>一定要当老大吗</h3><p>有一些工程师的人生理想是做团队里的技术老大，这当然是一个值得称赞的理想。可是，如果整个团队技术能力一般，发展潜力一般，而你是技术最强者，这与其说是幸运，不如说是悲哀。这种场景被称之为“武大郎开店”。 团队里的技术顶尖高手不是不能做，但为了能够持续成长，需要满足如下几个条件：</p>
<ul>
<li>首先你得是行业里面的顶尖专家了——实在很难找到比你更强的人了！</li>
<li>其次，你经常需要承担对你自己的能力有挑战的任务，但同时你拥有一批聪明能干的队友。虽然你的技术能力最高，但是在你不熟悉的领域，你的队友能够进行探索并扩展整个团队的知识。</li>
<li>最后，你必须要敏而好学，不耻下问。</li>
</ul>
<p>否则，加入更强的技术团队或许是更好的选择，最少不是什么值得骄傲的事情。</p>
<h3 id="平台化的传说"><a href="#平台化的传说" class="headerlink" title="平台化的传说"></a>平台化的传说</h3><p>平台化算得上是“高大上”的代名词了，很多工程师挤破头就为了和“平台化”沾点边。然而和其他业务需求相比，平台化需求并没有本质上的区别。无论是平台化需求还是普通业务需求，它的价值都来自于客户价值。不同点如下：</p>
<ul>
<li>很多平台化需求的客户来自于技术团队，普通需求的客户来自于业务方。</li>
<li>产品经理不同。普通业务需求来自于产品经理，平台化需求的产品经理可能就是工程师自己。长期被产品经理“压迫”的工程师们，在平台化上终于找到“翻身农奴把歌唱”的感觉。</li>
<li>很多平台化的关注点是接入能力和可扩展性，而普通业务的关注点更多。</li>
</ul>
<p>归根结底，平台化就是一种普通需求。在实施平台化之前，一定要避免下面两个误区：</p>
<ul>
<li><p>平台化绝对不是诸如“统一”、“全面”之类形容词的堆砌。是否需要平台化，应该综合考虑：客户数量，为客户解决的问题，以及客户价值是否值得平台化的投入。</p>
</li>
<li><p>平台化不是你做平台，让客户来服务你。一些平台化设计者的规划设计里面，把大量的平台接入工作、脏活累活交给了客户，然后自己专注于所谓“最高大上”的功能。恰恰相反，平台化应该是客户什么都不做，所有的脏活累活都由平台方来做。本质上讲，平台化的价值来自于技术深度。真正体现技术深度的恰恰是设计者能够很轻松的把所有的脏活累活搞定。</p>
</li>
</ul>
<h3 id="搞基础技术就一定很牛吗"><a href="#搞基础技术就一定很牛吗" class="headerlink" title="搞基础技术就一定很牛吗"></a>搞基础技术就一定很牛吗</h3><p>经常听到同学们表达对基础技术部同学的敬仰之情，而对搞业务技术的同学表现出很轻视，认为存储、消息队列、服务治理框架（比如美团点评内部使用的OCTO）、Hadoop等才能被称为真正的技术。事实并非如此，更基础的并不一定更高深。</p>
<p>比如下面这个流传很久的段子：越高级的语言就越没有技术含量。但真是这样吗，就拿Java和C来说，这是完全不同的两种语言，所需要的技能完全不同。C或许跟操作系统更加接近一点，和CPU、内存打交道的机会更多一点。但是为了用好Java，程序员在面向对象、设计模式、框架技术方面必须要非常精通。Java工程师转到C方向确实不容易，但作者也见过很多转到Java语言的C工程师水土不服。</p>
<p>基础技术和业务应用技术必然会有不同的关注点，没有高低之分。之所以产生这种误解，有两个原因：</p>
<ul>
<li>基础技术相对成熟，有比较完整的体系，这给人一个高大上的感觉。业务应用技术相对来说，由于每个团队使用的不一样，所以成熟度参差不齐，影响力没有那么大。</li>
<li>基础技术的门槛相对来说高一点，考虑到影响面，对可靠性、可用性等有比较高的最低要求。但是门槛高不代表技术含量高，另外成熟技术相对来说在创新方面会受到很大的约束。但是最先进的技术都来自活跃的创新。</li>
</ul>
<p>对比下来，业务技术和基础技术各有千秋。但真正的高手关注的是解决问题，所有的技术都是技能而已。</p>
<h3 id="可行性调研的那些坑"><a href="#可行性调研的那些坑" class="headerlink" title="可行性调研的那些坑"></a>可行性调研的那些坑</h3><p>工作中开展可行性调研时有发生。做可行性调研要避免如下情况：</p>
<ul>
<li>把可行性调研做成不可行性调研。这真的非常糟糕。不可行性的结论往往是：因为这样或者那样的原因，所以不可行。</li>
<li>避免“老鼠给猫挂铃铛”式的高风险可行性方案。“天下大事必作于细”，可行性调研一定要细致入微，避免粗枝大叶。</li>
<li>避免调研时间过长。如果发现调研进展进入到指数级复杂度，也就是每前进一步需要之前两倍的时间投入，就应该果断的停止调研。</li>
</ul>
<p>可行性调研的结论应该是收益与成本的折衷，格式一般如下：</p>
<ul>
<li>首先明确预期的结果，并按照高中低收益进行分级。</li>
<li>阐述达成每种预期结果需要采取的措施和方案。</li>
<li>给出实施各方案需要付出的成本。</li>
</ul>
<h3 id="工程师天生不善沟通吗"><a href="#工程师天生不善沟通吗" class="headerlink" title="工程师天生不善沟通吗"></a>工程师天生不善沟通吗</h3><p>实际工作中，沟通所导致的问题层出不穷。工程师有不少是比较内向的，总是被贴上“不善沟通”的标签。实际上，沟通能力是工程师最重要的能力之一，良好的沟通是高效工作学习的基础，也是通过学习可以掌握的。下面我按工程师的语言说说沟通方面的经验。</p>
<p>第一类常见的问题是沟通的可靠性。从可靠性的角度来讲，沟通分为TCP模式和UDP模式。TCP模式的形象表述是：我知道你知道。UDP模式的形象表述是：希望你知道。TCP模式当然比较可靠，不过成本比较高，UDP模式成本低，但是不可靠。在沟通可靠性方面，常见错误有如下两种：</p>
<ul>
<li>经常听到的这样的争论。一方说：“我已经告诉他了”，另一方说：“我不知道这个事情呀”。把UDP模式被当作TCP模式来使用容易产生扯皮。</li>
<li>过度沟通。有些同学对沟通的可靠性产生了过度焦虑，不断的重复讨论已有结论问题。把TCP模式当成UDP来使用，效率会比较低。</li>
</ul>
<p>第二类沟通问题是时效性问题。从时效性讲，沟通分为：同步模式和异步模式。同步沟通形象地说就是：你现在给我听好了。异步沟通的形象表述是：记得给我做好了。在沟通时效性方面，有如下两种常见错误：</p>
<ul>
<li>已经出现线上事故，紧急万分。大家你一言，我一语，感觉事故可能和某几个人有关，但是也不能完全确定，所以没有通知相关人员。最终，一个普通的事故变成了严重事故。对于紧急的事情，必须要同步沟通。</li>
<li>半夜三点你正在熟睡，或者周末正在逛街，接到一个电话：“现在有个需求，能否立刻帮忙做完。”这会非常令人郁闷，因为那并不是紧急的事情。不是所有的需求都需要立刻解决。</li>
</ul>
<p>有效沟通的一个重要原则是提前沟通。沟通本质是信息交流和处理，可以把被沟通对象形象地比喻成串行信息处理的CPU。提前沟通，意味着将处理请求尽早放入处理队列里面。下面的例子让很多工程师深恶痛绝：一个需求策划了1个月，产品设计了2周。当开发工程是第一次听说该需求的时候，发现开发的时间是2天。工程师据理力争，加班加点1周搞定。最后的结论是工程师非常不给力，不配合。就像工程师讨厌类似需求一样。要协调一个大项目，希望获得别人的配合，也需要尽早沟通。</p>
<p>有效沟通的另外一个重点是“不要跑题”。很多看起来很接近的问题，本质上是完全不同的问题。比如：一个会议的主题是“如何实施一个方案”，有人却可能提出“是否应该实施该方案”。 “如何实施”和“是否应该实施”是完全不同的两个问题，很多看起来相关的问题实际上跑题很远。“跑题”是导致无效沟通的重要原因。</p>
<p>良好沟通的奥秘在于能掌握TCP模式和UDP模式精髓，正确判断问题的紧急性，尽量提前沟通，避免跑题。</p>
<h3 id="带人之道"><a href="#带人之道" class="headerlink" title="带人之道"></a>带人之道</h3><p>有些初为导师的工程师由于担心毕业生的能力太弱，安排任务时候谆谆教诲，最后感觉还是有所顾虑，干脆自己写代码。同样的事情发生在很多刚刚管理小团队的工程师身上。最终的结果他们：写完所有的代码，让下属无代码可写。“ 事必躬亲”当然非常糟糕，最终的往往是团队的整体绩效不高，团队成员的成长很慢，而自己却很累。</p>
<p>古人说：“用人不疑，疑人不用。”这句话并非“放之四海而皆准”。在古代，受限于通信技术，反馈延迟显著，而且信息在传递过程中有大量噪音，变形严重。在这种情况下，如果根据短期内收集的少量变形的信息做快速决断，容易陷于草率。在公司里，这句话用于选人环节更为恰当，应该改为：录用不疑，疑人不录。</p>
<p>考虑到招聘成本，就算是在录用层面，有时候也无法做到。作为一个小团队的管理者，能够快速准确的获取团队成员的各种反馈信息，完全不需要“用人不疑，疑人不用”。用人的真正理论基础来自于“探索和利用”(Exploration and Exploitation )。不能因为下属能做什么就只让他做什么，更不能因为下属一次失败就不给机会。</p>
<p>根据经典的“探索和利用”(Exploration and Exploitation )理论，良好的用人方式应该如下：</p>
<ul>
<li>首选选择相信，在面临失败后，收缩信任度。</li>
<li>查找失败的原因，提供改进意见，提升下属的能力。</li>
<li>总是给下属机会，在恰当地时机给下属更高的挑战。 总之，苍天大树来自一颗小种子，要相信成长的力量。</li>
</ul>
<h3 id="效率、效率、效率"><a href="#效率、效率、效率" class="headerlink" title="效率、效率、效率"></a>效率、效率、效率</h3><p>经常看到有些同学给自己的绩效评分是100分——满分，原因是在过去一段时间太辛苦了，但最终的绩效却一般般。天道酬勤不错，但是天道更酬巧。工程师们都学过数据结构，不同算法的时间复杂度的差距，仅仅通过更长的工作时间是难以弥补的。为了提升工作学习效率，我们需要注意以下几点：</p>
<ul>
<li>主要关注效率提升。很多时候，与效率提升所带来的收益相比，延长时间所带来的成果往往不值得一提。</li>
<li>要有清晰的结果导向思维。功劳和苦劳不是一回事。</li>
<li>做正确的事情，而不仅仅正确地做事情。这是一个被不断提起的话题，但是错误每天都上演。为了在规定的时间内完成一个大项目，总是要有所取舍。如果没有重点，均匀发力，容易事倍功半。如果“南辕北辙”，更是可悲可叹。</li>
</ul>
<h2 id="架构师能力模型"><a href="#架构师能力模型" class="headerlink" title="架构师能力模型"></a>架构师能力模型</h2><p>前面我们已经讲完了原则和一些困惑，那么工程师到底应该怎么提升自己呢？</p>
<p>成为优秀的架构师是大部分初中级工程师的阶段性目标。优秀的架构师往往具备八种核心能力：编程能力、调试能力、编译部署能力、性能优化能力、业务架构能力、在线运维能力、项目管理能力和规划能力。</p>
<p>这几种能力之间的关系大概如下图。编程能力、调试能力和编译部署能力属于最基础的能力。不能精通掌握这三种能力，很难在性能优化能力和业务架构能力方面有所成就。具备了一定的性能优化能力和业务架构能力之后，才能在线运维能力和项目管理能力方面表现优越。团队管理能力是最高能力，它对项目管理能力的依赖度更大。</p>
<h3 id="编程能力"><a href="#编程能力" class="headerlink" title="编程能力"></a>编程能力</h3><p>对工程师而言，编程是最基础的能力，必备技能。其本质是一个翻译能力，将业务需求翻译成机器能懂的语言。</p>
<p>提升编程能力的书籍有很多。精通面向对象和设计模式是高效编程的基础。初级工程师应该多写代码、多看代码。找高手做Code Review，也是提升编程水平的捷径。</p>
<h3 id="调试能力"><a href="#调试能力" class="headerlink" title="调试能力"></a>调试能力</h3><p>程序代码是系统的静态形式，调试的目的是通过查看程序的运行时状态来验证和优化系统。本质上讲，工程师们通过不断调试可以持续强化其通过静态代码去预测运行状态的能力。所以调试能力也是工程师编程能力提升的关键手段。很早之前有个传说：“调试能力有多强，编程能力就有多强。”不过现在很多编辑器的功能很强大，调试能力的门槛已经大大降低。</p>
<p>调试能力是项目能否按时、高质量提交的关键。即使一个稍具复杂度的项目，大部分工程师也无法一次性准确无误的完成。大项目都是通过不断地调试进行优化和纠错的。所以调试能力是不可或缺的能力。</p>
<p>多写程序，解决Bug，多请教高手是提升调试能力的重要手段。</p>
<h3 id="编译部署能力"><a href="#编译部署能力" class="headerlink" title="编译部署能力"></a>编译部署能力</h3><p>编译并在线上部署运行程序是系统上线的最后一个环节。随着SOA架构的普及以及业务复杂度的增加，大部分系统只是一个完整业务的一个环节，因此，本地编译和运行并不能完全模拟系统在线运行。为了快速验证所编写程序的正确性，编译并在线上部署就成了必要环节。所以编译部署能力是一个必备技能。</p>
<p>让盘根错节的众多子系统运行起来是个不小的挑战。得益于SOA架构的普及以及大量编译、部署工具的发展，编译部署的门槛已经大大降低。基于应用层进行开发的公司，已经很少有“编译工程师”的角色了。但是对于初级工程师而言，编译部署仍然不是一个轻松的事情。</p>
<h3 id="性能优化能力"><a href="#性能优化能力" class="headerlink" title="性能优化能力"></a>性能优化能力</h3><p>衡量一个系统成功的一个重要指标是使用量。随着使用量的增加和业务复杂度的增加，大部分系统最终都会碰到性能问题。 性能优化能力是一个综合能力。因为：</p>
<ul>
<li>影响系统性能的因素众多，包括：数据结构、操作系统、虚拟机、CPU、存储、网络等。为了对系统性能进行调优，架构师需要掌握所有相关的技术。</li>
<li>精通性能优化意味着深刻理解可用性、可靠性、一致性、可维护性、可扩展性等的本质。</li>
<li>性能优化与业务强耦合，最终所采取的手段是往往折衷的结果。所以，性能优化要深谙妥协的艺术。</li>
</ul>
<p>可以说，性能优化能力是工程师们成长过程中各种技能开始融会贯通的一个标志。这方面可以参考之前的博客文章“常见性能优化策略的总结”。市场上还有很多与性能优化相关的书籍，大家可以参考。多多阅读开源框架中关于性能优化方面的文档和代码也不失为好的提升手段。动手解决线上性能问题也是提升性能优化能力的关键。如果有机会，跟着高手学习，分析性能优化解决方案案例（我们技术博客之前也发表了很多这方面的文章），也是快速提升性能优化能力的手段。</p>
<h3 id="在线运维能力"><a href="#在线运维能力" class="headerlink" title="在线运维能力"></a>在线运维能力</h3><p>如果说性能优化能力体现的是架构师的静态思考能力，在线运维能力考验的就是动态反应能力。残酷的现实是，无论程序多么完美，Bug永远存在。与此同时，职位越高、责任越大，很多架构师需要负责非常重要的在线系统。对于线上故障，如果不能提前预防以及快速解决，损失可能不堪设想，所以在线运维能力是优秀架构师的必备技能。</p>
<p>为了对线上故障进行快速处理，标准化的监控、上报、升级，以及基本应对机制当然很重要。通过所观察到的现象，快速定位、缓解以及解决相关症状也相当关键。这要求架构师对故障系统的业务、技术具备通盘解读能力。解决线上故障的架构师就好比一个在参加比赛F1的车手。赛车手必须要了解自身、赛车、对手、同伴、天气、场地等所有因素，快速决策，不断调整。架构师必须要了解所有技术细节、业务细节、处理规范、同伴等众多因素，快速决断，迅速调整。</p>
<p>在线运维本质上是一个强化学习的过程。很多能力都可以通过看书、查资料来完成，但在线运维能力往往需要大量的实践来提升。</p>
<h3 id="业务架构能力"><a href="#业务架构能力" class="headerlink" title="业务架构能力"></a>业务架构能力</h3><p>工程师抱怨产品经理的故事屡见不鲜，抱怨最多的主要原因来自于需求的频繁变更。需求变更主要有两个来源：第一个原因是市场改变或战略调整，第二个原因是伪需求。对于第一个原因，无论是工程师还是产品经理，都只能无奈的接受。优秀的架构师应该具备减少第二种原因所导致的需求变更的概率。</p>
<p>伪需求的产生有两个原因：</p>
<p>第一个原因是需求传递变形。从信息论的角度来讲，任何沟通都是一个编码和解码的过程。典型的需求从需求方到产品经理，最终到开发工程师，最少需要经历三次编码和解码过程。而信息的每一次传递都存在一些损失并带来一些噪音，这导致有些时候开发出来的产品完全对不上需求。此外，需求方和产品经理在需求可行性、系统可靠性，开发成本控制方面的把控比较弱，也会导致需求变形。</p>
<p>第二个原因就是需求方完全没有想好自己的需求。</p>
<p>优秀的架构师应该具备辨别真伪需求的能力。应该花时间去了解客户的真实业务场景，具备较强的业务抽象能力，洞悉客户的真实需求。系统的真正实施方是工程师，在明确客户真实需求后，高明的架构师应该具备准确判断项目对可行性、可靠性、可用性等方面的要求，并能具备成本意识。最后，由于需求与在线系统的紧耦合关系，掌握在线系统的各种细节也是成功的业务架构的关键。随着级别的提升，工程师所面对的需求会越来越抽象。承接抽象需求，提供抽象架构是架构师走向卓越的必经之途。</p>
<p>市场上有一些关于如何成为架构师的书，大家可以参考。但是架构能力的提升，实践可能是更重要的方式。业务架构师应该关注客户的痛点而不是PRD文档，应该深入关注真实业务。掌握现存系统的大量技术和业务细节也是业务架构师的必备知识。</p>
<h3 id="项目管理能力"><a href="#项目管理能力" class="headerlink" title="项目管理能力"></a>项目管理能力</h3><p>作为工业时代的产物，分工合作融入在互联网项目基因里面。架构师也需要负责几个重大项目才能给自己正名。以架构师角色去管理项目，业务架构能力当然是必备技能。此外，人员管理和成本控制意识也非常重要。</p>
<p>项目管理还意味着要有一个大心脏。重大项目涉及技术攻关、人员变动、需求更改等众多可变因素。面临各种变化，还要在确保目标顺利达成，需要较强的抗压能力。</p>
<p>人员管理需要注意的方面包括：知人善用，优化关系，简化沟通，坚持真理。</p>
<ul>
<li>知人善用意味着架构师需要了解每个参与者的硬技能和软素质。同时，关注团队成员在项目过程中的表现，按能分配。</li>
<li>优化关系意味着管理团队的情绪，毕竟项目的核心是团队，有士气的团队才能高效达成目标。</li>
<li>简化沟通意味着快速决策，该妥协的时候妥协，权责分明。</li>
<li>坚持真理意味着顶住压力，在原则性问题上绝不退步。</li>
</ul>
<p>成本控制意味着对项目进行精细化管理，需要遵循如下几个原则：</p>
<ul>
<li>以终为始、确定里程碑。为了达成目标，所有的计划必须以终为始来制定。将大项目分解成几个小阶段，控制每个阶段的里程碑可以大大降低项目失败的风险。</li>
<li>把控关键路径和关键项目。按照关键路径管理理论（CPM）的要求，架构师需要确定每个子项目的关键路径，确定其最早和最晚启动时间。同时，架构师需要关注那些可能会导致项目整体延期的关键节点，并集中力量攻破。</li>
<li>掌控团队成员的张弛度。大项目持续时间会比较长，也包含不同工种。项目实施是一个不断变化的动态过程，在这个过程中不是整个周期都很紧张，不是所有的工种都一样忙。优秀的架构师必须要具备精细阅读整体项目以及快速反应和实时调整的能力。这不仅仅可以大大降低项目成本，还可以提高产出质量和团队满意度。总体来说，“前紧后松”是项目管理的一个重要原则。</li>
</ul>
<p>项目管理方面的书籍很多。但是，提高业务架构能力同样重要。积极参与大项目并观察别人管理项目的方式也是非常重要的提升手段。</p>
<h3 id="团队管理能力"><a href="#团队管理能力" class="headerlink" title="团队管理能力"></a>团队管理能力</h3><p>不想做CTO的工程师不是一个好的架构师。走向技术管理应该是工程师的一个主流职业规划。团队管理的一个核心能力就是规划能力，这包括项目规划和人员规划。良好的规划需要遵循如下原则：</p>
<ul>
<li>规划是利益的博弈。良好的规划上面对得起老板，中间对得起自己，下面对得起团队。在三者利益者寻找平衡点，实现多方共赢考验着管理者的智慧和精细拿捏的能力。</li>
<li>任何规划都比没有规划好。没有规划的团队就是没头的苍蝇，不符合所有人的利益。</li>
<li>规划不是本本主义。市场在变，团队在变，规划也不应该一成不变。</li>
<li>客户至上的是项目规划的出发点。</li>
<li>就人员规划而言，规划需要考量团队成员的能力、绩效、成长等多方面的因素。</li>
</ul>
<p>市场上有很多规划管理方面的书籍，值得阅读。最优化理论虽然是技术书籍，但它是规划的理论基础，所以不妨多看看翻阅一下。从自我规划开始，多多学习别人的规划也是规划能力提升的重要手段。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>因为受邀去做一个关于“一边工作，一边学习”的分享，作者花了一段时间去思考和汇总学习方法论，接着每天不断地采集谣言并尝试解惑，再根据个人经验绘制出优秀架构师的能力模型，最后汇集成文。</p>
<p>文章系统性地阐述了学习原则、分析了常见困惑，并制定明确学习目标，期望对工程师们的工作学习有所帮助。需要申明的是，文章内容挂一漏万，所谓的架构师能力模型也是作者的个人观点。欢迎大家在评论中分享自己在学习成长方面的心得。</p>
]]></content>
      <categories>
        <category>个人日志</category>
      </categories>
      <tags>
        <tag>技术管理</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis持久化机制</title>
    <url>/blog/199092f6.html</url>
    <content><![CDATA[<h4 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h4><p>由于<a href="https://redis.io/" target="_blank" rel="noopener">Redis</a>（Remote Dictionary Server）将所有数据都存放在内存，因此读写性能非常高（同时也取决于机器性能）。存放在内存就需要考虑机器断电或故障带来的数据丢失问题，这里Redis提供不同等级的磁盘持久化方式来保证数据不会丢失。</p>
<h4 id="持久化方案"><a href="#持久化方案" class="headerlink" title="持久化方案"></a>持久化方案</h4><p>持久化可以有效避免宕机带来数据丢失问题，重启时利用之前持久化的文件即可实现数据恢复。Redis支持RDB和AOF两种持久化机制。</p>
<h5 id="RDB"><a href="#RDB" class="headerlink" title="RDB"></a>RDB</h5><p>把当前进程数据形成快照后保存在磁盘，命令有save和bgsave。重点说一下bgsave，触发后redis进程fork出一个子进程（fork过程中父进程会阻塞），用于完成rdb文件的写入，保证原进程能正常读写，不被阻塞（redis是单线程模型）。save配置中，”save m n”表示m秒内数据集发生n次修改后，触发bgsave。</p>
<ul>
<li>优点：</li>
</ul>
<ol>
<li>适合备份、全复制场景，定时备份压缩，同步到从节点等；</li>
<li>数据恢复数据快于AOF。</li>
</ol>
<ul>
<li>缺点：</li>
</ul>
<ol>
<li>fork重量级操作，频繁执行成本高，定期执行做不到实时持久化；</li>
<li>rdb二进制格式，redis版本迭代无法兼容新版的rdb。</li>
</ol>
<h5 id="AOF"><a href="#AOF" class="headerlink" title="AOF"></a>AOF</h5><p>Append only file(类似hbase的WAL、mysql的binlog)，以独立日志的方式记录每次写命令，重启时REDO AOF中的命令，达到恢复数据的目的，该方式解决了持久化的实时性问题。配置文件开启：appendonly yes，默认不开启。</p>
<p>由于AOF实时记录每次操作的命令，需要加入缓冲区来保证性能。同时，AOF文件会越来越大，需要进行压缩重写。具体操作流程如下图：</p>
<p><img src="https://img-blog.csdnimg.cn/20181117145604317.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1Y2FiaXQ=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p> 同步：对于缓冲区同步数据到AOF文件的策略，appendfsync=always/everysec/no，建议配置everysec，命令写入aof_buf，调用系统write操作返回，write写入系统缓冲区，有单独线程一秒调用一次fsync，写入硬盘。保证写入性能和数据安全。</p>
<p> 重写：采用copy-on-write机制，子进程新写的aof_rewrite_buf文件，父进程接收的写入命令也同样记录，保证数据不丢失。同时，将无效命令去除缩小文件大小，完成后替换旧AOF。</p>
<ul>
<li>优点：</li>
</ul>
<ol>
<li>解决实时性持久化</li>
</ol>
<ul>
<li>缺点：</li>
</ul>
<ol>
<li>AOF不断追加命令，容易造成阻塞，受限硬盘资源；</li>
<li>AOF文件体积逐渐变大，需要定期重写，其中还需要维护重写缓冲区。</li>
</ol>
<h4 id="实际应用"><a href="#实际应用" class="headerlink" title="实际应用"></a>实际应用</h4><p>Redis是否要开启持久化，取决于实际的应用场景。例如：</p>
<ul>
<li><p>为了降低数据库的读取压力，redis作为读缓存使用，就不需要开启持久化。可以考虑开启从DB向Redis的定期同步任务；</p>
</li>
<li><p>为了提供写入性能，引入缓存机制。需要考虑数据的丢失风险，必须开启持久化。此时，需要考虑机器的配置，包括CPU、内存、硬盘。fork进程的开销、AOF和RDB写入硬盘压力等。</p>
</li>
</ul>
<h4 id="扩展链接"><a href="#扩展链接" class="headerlink" title="扩展链接"></a>扩展链接</h4><ul>
<li><a href="https://github.com/linyiqun/Redis-Code" target="_blank" rel="noopener">redis键值数据库源码分析</a></li>
<li>《Redis开发与运维》付磊 张益军</li>
</ul>
]]></content>
      <categories>
        <category>数据库与大数据</category>
      </categories>
      <tags>
        <tag>NoSQL</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title>业界大牛博客收藏</title>
    <url>/blog/ec4cdf4c.html</url>
    <content><![CDATA[<p>收藏中间件、算法、大数据、人工智能等领域业界大牛、团队博客、论坛、站点等链接，研究、探索和跟踪前沿技术。持续更新中…</p>
<p><img src="/blog/ec4cdf4c/fczlm.jpg" alt></p>
<h4 id="个人博客"><a href="#个人博客" class="headerlink" title="个人博客"></a>个人博客</h4><ol>
<li><a href="https://icyfenix.cn/" target="_blank" rel="noopener">凤凰架构</a> | <a href="https://icyfenix.cn/introduction/about-me.html" target="_blank" rel="noopener">周志明</a>，从事大型企业级软件的架构与研发</li>
<li><a href="https://mofanpy.com/" target="_blank" rel="noopener">莫烦Python</a> | 机器学习,深度学习, 腾讯</li>
<li><a href="https://chasezhang.me" target="_blank" rel="noopener">张茄子</a> | 算法、 分布式技术和函数式编程爱好者</li>
<li><a href="http://www.gatorsmile.io/" target="_blank" rel="noopener">Lixiao</a> | Spark Committer In Databricks |专注SparkSQL等</li>
<li><a href="https://www.linkedin.com/in/liancheng/" target="_blank" rel="noopener">连城</a> |Spark PMC/Committer In Databricks | 专注SparkSQL</li>
<li><a href="http://wuchong.me/" target="_blank" rel="noopener">Jark’s Blog</a>  伍翀（花名：云邪） | 阿里巴巴高级开发工程师 Apache Flink Committer</li>
<li><a href="https://matt33.com/" target="_blank" rel="noopener">Matt’s Blog</a> 王蒙 蚂蚁金服 | Kafka，Java, 分布式存储，分布式计算，AI</li>
<li><a href="http://aleiwu.com/" target="_blank" rel="noopener">Aylei’s Blog</a>  | 吴叶磊 | PingCAP Cloud方向工程师 </li>
<li><a href="https://ericfu.me/" target="_blank" rel="noopener">Eric Fu</a> | 专注于数据库及大数据系统</li>
<li><a href="http://neoremind.com" target="_blank" rel="noopener">neoremind</a>  | <a href="https://www.zhihu.com/people/neoremind" target="_blank" rel="noopener">知乎</a> | 阿里专家 | 关注Service Backend、Big Data、Storage、Ads技术 </li>
<li><a href="www.cnkirito.moe">Kirito的技术分享</a> | 阿里云 | 关注Java生态，布式服务治理，微服务，性能调优，各类源码分析</li>
<li><a href="https://blog.csdn.net/v_JULY_v/" target="_blank" rel="noopener">July </a> | 深入数据结构与算法  <a href="https://www.julyedu.com/" target="_blank" rel="noopener">七月在线</a>创始人，专注AI教育</li>
<li><a href="https://www.geekxh.com/" target="_blank" rel="noopener">geekxh</a> | 公众号-小浩算法 | 漫画图解算法</li>
<li><a href="https://www.iteblog.com" target="_blank" rel="noopener">过往记忆</a> 吴阳平（花名：明慧 ） | 阿里云HBASE业务架构 热衷于大数据技术（Hadoop、HBase、Spark）等</li>
<li><a href="http://dongxicheng.org/" target="_blank" rel="noopener">董的博客</a> 董西成 | 前Hulu基础架构部负责人，现就职快手 | 大数据架构hadoop/spark/flink</li>
<li><a href="http://hbasefly.com" target="_blank" rel="noopener">hbasefly</a> 范欣欣  | 网易杭州研究院数据科学中心，负责HBase以及分布式时序数据库的内核开发运维工作</li>
<li><a href="http://openinx.github.io/" target="_blank" rel="noopener">Openinx Blog</a> 胡争  | 小米公司HBase工程师，Apache HBase PMC成员，负责Apache HBase项目研发及小米HBase集群维护，对HBase及相关分布式存储系统有很多独到的见解</li>
<li><a href="https://github.com/oldratlee" target="_blank" rel="noopener">李鼎的github</a> 李鼎(花名：哲良)  | 淘宝高级技术专家  （paas、dubbo、rpc）</li>
<li><a href="http://bluedavy.me" target="_blank" rel="noopener">bluedavy的blog</a> | <a href="http://hellojava.info/" target="_blank" rel="noopener">hellojava</a> 毕玄 （java、分布式）</li>
<li><a href="http://www.rowkey.me" target="_blank" rel="noopener">后端技术杂谈</a> （微鲤技术团队 | 著有《Java工程师修炼之道》一书）</li>
<li><a href="http://www.ityouknow.com/" target="_blank" rel="noopener">纯洁的微笑</a> （spring boot/cloud 深度使用和分享者）</li>
<li><a href="https://coolshell.cn" target="_blank" rel="noopener">左耳朵耗子 </a>（陈皓 | 专注高可用、高性能、分布式等底层技术 | 创业中）</li>
<li><a href="http://www.javastack.cn/" target="_blank" rel="noopener">Java技术栈</a> （多年Java开发和架构实践经验）</li>
<li><a href="https://moelove.info/" target="_blank" rel="noopener">MoeLove </a>(张晋涛 TaoBeier) 网易有道资深运维 | Docker/K8s深度使用者</li>
<li><a href="https://www.phodal.com/" target="_blank" rel="noopener">Phodal</a>   ThoughtWorks咨询师 | InfoQ编辑 | Serverless深度使用者</li>
<li><a href="https://www.yangcs.net/" target="_blank" rel="noopener">Ryan Yang</a> Cloud Native工程 | 目前就职上海DaoCloud | K8s深度使用者</li>
<li><a href="http://www.huaxiaozhuan.com/" target="_blank" rel="noopener">huaxiaozhuan</a> (AI算法工程师手册) 资深算法工程师 | 智易科技首席算法研究员</li>
<li><a href="https://blog.csdn.net/u013256816" target="_blank" rel="noopener">朱小厮的博客</a> 深入中间件、消息队列 </li>
</ol>
<h4 id="公司-amp-团队博客"><a href="#公司-amp-团队博客" class="headerlink" title="公司&amp;团队博客"></a>公司&amp;团队博客</h4><ol>
<li><a href="https://cn.kyligence.io/blog-zh/" target="_blank" rel="noopener">Kyligence博客</a> | Apache Kylin（开源分布式 OLAP 分析引擎）核心团队创立 | 下一代企业级数据管理与分析平台 </li>
<li><a href="https://www.sofastack.tech/blog/" target="_blank" rel="noopener">蚂蚁金服SofaStack博客</a></li>
<li><a href="http://jm.taobao.org/" target="_blank" rel="noopener">阿里中间件团队博客</a></li>
<li><a href="https://tech.meituan.com/" target="_blank" rel="noopener">美团技术团队博客</a></li>
<li><a href="https://tech.youzan.com/" target="_blank" rel="noopener">有赞技术团队博客</a></li>
<li><a href="https://pingcap.com/blog-cn/" target="_blank" rel="noopener">PingCAP博客</a> 开源的新型分布式数据库公司  | 代表作品TiDB、</li>
<li><a href="https://damo.alibaba.com/labs/database-and-storage?lang=zh" target="_blank" rel="noopener">阿里达摩院-数据库与存储实验室</a></li>
<li><a href="http://mysql.taobao.org/" target="_blank" rel="noopener">mysql.taobao</a> | Aliyun ApsaraDB team</li>
</ol>
<h4 id="知乎专栏"><a href="#知乎专栏" class="headerlink" title="知乎专栏"></a>知乎专栏</h4><ol>
<li>ETIN  | <a href="https://zhuanlan.zhihu.com/hotspotvm" target="_blank" rel="noopener">JVM进阶之路</a> | Java 虚拟机（JVM）、HotSpot VM、Java</li>
<li>Ed Huang等人 | <a href="https://zhuanlan.zhihu.com/newsql" target="_blank" rel="noopener">TiDB 的后花园</a> | 分布式系统、数据库、SQL</li>
<li>温正湖 | <a href="https://zhuanlan.zhihu.com/c_206071340" target="_blank" rel="noopener">数据库内核</a> | 网易数据库内核技术专家</li>
<li>张茄子,Eric Fu | <a href="https://zhuanlan.zhihu.com/io-meter" target="_blank" rel="noopener">分布式数据系统小菜</a>  | 分布式系统与数据平台相关技术的总结</li>
</ol>
<h4 id="社区"><a href="#社区" class="headerlink" title="社区"></a>社区</h4><ol>
<li><a href="https://qsctech.github.io/zju-icicles/" target="_blank" rel="noopener">浙江大学课程攻略共享计划</a> | 大学课程资料经验总结</li>
<li><a href="http://dblab.xmu.edu.cn/" target="_blank" rel="noopener">厦门大学数据库实验室</a></li>
<li><a href="https://github.com/databricks" target="_blank" rel="noopener">Databricks Github</a> | Spark商业公司</li>
<li><a href="https://github.com/apachecn" target="_blank" rel="noopener">ApacheCN</a> 可能是东半球最大的 AI 社区 | <a href="http://apachecn.org/" target="_blank" rel="noopener">iBooker 布客</a></li>
<li><a href="https://kylin.apache.org/cn/blog/" target="_blank" rel="noopener">Apache Kylin博客</a></li>
<li><a href="https://www.infoq.cn/" target="_blank" rel="noopener">InfoQ</a> | 极客邦 知识创新与传播</li>
</ol>
]]></content>
      <categories>
        <category>个人日志</category>
      </categories>
      <tags>
        <tag>博客</tag>
        <tag>IT资讯</tag>
      </tags>
  </entry>
  <entry>
    <title>HBase原理与实战</title>
    <url>/blog/662f2d37.html</url>
    <content><![CDATA[<h4 id="基础理论"><a href="#基础理论" class="headerlink" title="基础理论"></a>基础理论</h4><ol>
<li><a href="https://blog.csdn.net/Yaokai_AssultMaster/article/details/72877127" target="_blank" rel="noopener">深入理解HBase的系统架构</a></li>
</ol>
<h4 id="大会前沿"><a href="#大会前沿" class="headerlink" title="大会前沿"></a>大会前沿</h4><ol>
<li><a href="https://mp.weixin.qq.com/s/Qqz3dSwyxwR4D915OFybpA" target="_blank" rel="noopener">漫谈HBaseCon Asia 2018大会精华总结</a></li>
<li><a href="https://mp.weixin.qq.com/s/GQeVyR8qMnvHl14xJgaUWA" target="_blank" rel="noopener">从HBase中移除WAL？3D XPoint技术带来的变革</a></li>
<li><a href="https://mp.weixin.qq.com/s/3Bhwn-019LcpkuUxNAfkAw" target="_blank" rel="noopener">HBase2.0重新定义小对象实时存取</a> （天引 阿里巴巴 技术专家）</li>
<li><a href="https://mp.weixin.qq.com/s/yHvOmf_HK_5qy7C5X1Ukaw" target="_blank" rel="noopener">NoSQL漫谈：2018年文章汇总</a> (毕杰山之HBase&amp;OpenTSDB博客2018年终总结)</li>
<li><a href="https://yq.aliyun.com/articles/685217?spm=a2c4e.11153940.0.0.6ada72deuOHiYj" target="_blank" rel="noopener">HBase生态介绍</a></li>
<li><a href="https://yq.aliyun.com/articles/684011?spm=a2c4e.11153940.0.0.35992815p1Iq9e" target="_blank" rel="noopener">2018年HBase生态社群画像 +最全资料汇总下载</a></li>
</ol>
<h4 id="应用实战"><a href="#应用实战" class="headerlink" title="应用实战"></a>应用实战</h4><ol>
<li><p>RIT(Region-In-Transition)问题解决方案</p>
<p>​    a) <a href="https://mp.weixin.qq.com/s/sLZo7a23BJUtOiapLnncgQ" target="_blank" rel="noopener">HBase应用实践专场-HBase问题排查思路</a></p>
<p>​    b) <a href="https://mp.weixin.qq.com/s/viDy2_qpFqZz1Me5ALr8LQ" target="_blank" rel="noopener">HBase运维实践－聊聊RIT的那点事</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/uvr9mdTci-Qw3IuFbEuj4Q" target="_blank" rel="noopener">一种HBase表数据迁移方法的改进</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/mwllZtK_nAEqNv0VZq3ULw" target="_blank" rel="noopener">HBase基本知识介绍及典型案例分析</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/Jf4FVUJWroRgHLTLquAOOQ" target="_blank" rel="noopener">从MySQL到HBase：数据存储方案转型的演进</a> (HBASE技术社区)</p>
</li>
<li><p>阿里云云<a href="https://help.aliyun.com/document_detail/49501.html?spm=5176.10695662.1996646101.searchclickresult.75da4648pjXq4y" target="_blank" rel="noopener">HBase X-Pack生态应用</a> （一站式数据处理平台，涵盖关系Phoenix SQL、时序OpenTSDB、全文Solr、时空GeoMesa、图GraphDB、分析Spark）</p>
</li>
</ol>
]]></content>
      <categories>
        <category>数据库与大数据</category>
      </categories>
      <tags>
        <tag>HBase</tag>
        <tag>NoSQL</tag>
      </tags>
  </entry>
  <entry>
    <title>SQL标准演化</title>
    <url>/blog/43ea8e84.html</url>
    <content><![CDATA[<h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>SQL是在1974年有Boyce和Chamberlin提出，最初叫Sequel[ˈsiːkwəl]。由于简单易学，功能丰富，大受欢迎，被各大数据库厂商所采用。在1986年10月，美国国家标准局（American National Standard Institue，ANSI）的数据库委员会X3H2将SQL作为关系数据库语言美国标准，同年公布SQL-86标准，并在次年通过国际标准化组织（International Organization for Standardization，ISO)标准。</p>
<h3 id="标准演化"><a href="#标准演化" class="headerlink" title="标准演化"></a>标准演化</h3><p>随着数据库技术的不断发展，SQL标准也在不断发展和改进，见下表。</p>
<table>
<thead>
<tr>
<th style="text-align:left">标准</th>
<th>页数</th>
<th style="text-align:left">说明</th>
<th>发布日期</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">SQL-86</td>
<td></td>
<td style="text-align:left"></td>
<td>1986</td>
</tr>
<tr>
<td style="text-align:left">SQL-89(FIPS 127-1)</td>
<td>120</td>
<td style="text-align:left"></td>
<td>1989</td>
</tr>
<tr>
<td style="text-align:left">SQL-92</td>
<td>622</td>
<td style="text-align:left">除SQL基础部分外，增加SQL调用接口、SQL永久存储模块</td>
<td>1992</td>
</tr>
<tr>
<td style="text-align:left">SQL-99(SQL 3)</td>
<td>1700</td>
<td style="text-align:left">进一步扩展为框架、SQL基础部分、SQL调用接口、SQL永久存储模块、SQL宿主语言绑定、SQL外部数据管理、SQL对象语言绑定</td>
<td>1999</td>
</tr>
<tr>
<td style="text-align:left">SQL-2003</td>
<td>3600</td>
<td style="text-align:left"></td>
<td>2003</td>
</tr>
<tr>
<td style="text-align:left">SQL-2008</td>
<td>3777</td>
<td style="text-align:left">针对2003的修改和补充</td>
<td>2006</td>
</tr>
<tr>
<td style="text-align:left">SQL-2011</td>
<td></td>
<td style="text-align:left">针对2003的修改和补充</td>
<td>2010</td>
</tr>
</tbody>
</table>
<p>目前，没有一个数据库是能够支持SQL标准中所有概念和特性的。大多数支持SQL-92标准大部分功能以及SQL-99、SQL-2003中的部分新概念。并且，各数据库厂商也对SQL基本命令集进行了不同程序的扩充和修改，支持标准以外的一些功能特性。</p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul>
<li>《数据库系统概论 第五版》，王珊、萨师煊著，2014</li>
<li><a href="https://ronsavage.github.io/SQL/" target="_blank" rel="noopener">BNF Grammars for SQL-92, SQL-99 and SQL-2003</a></li>
</ul>
]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title>南京大数据技术Meetup第四次活动</title>
    <url>/blog/224204e7.html</url>
    <content><![CDATA[<h4 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h4><p>很有幸今天去参加了“南京大数据技术Meetup第四次活动”，前三次由于不知道这样的活动，错过了非常可惜。以下就是我了解到的一些内容，在这里做一些记录。</p>
<h4 id="演讲嘉宾"><a href="#演讲嘉宾" class="headerlink" title="演讲嘉宾"></a>演讲嘉宾</h4><ul>
<li><p>汪军（伦敦大学学院，博士生导师，教授，AI、互联网变现和计算广告学）</p>
</li>
<li><p>王胤然（烽火通信大数据专家）</p>
</li>
<li><p>张毅（苏宁云商，软件工程师）</p>
</li>
<li><p>黄宜华（博士，教授，博导，NJU PASA实验室负责人）</p>
</li>
<li><p>顾荣（NJU PH.D，PASA成员，Tachyon和Spark Contributor）</p>
</li>
<li><p>罗胜美（中国电子学会云计算专委会委员、中兴通讯集团首席架构师）</p>
</li>
<li><p>樊建（华泰证券，南京载玄信息科技有限公司联合创始人，技术总监）</p>
</li>
</ul>
<h4 id="行业应用"><a href="#行业应用" class="headerlink" title="行业应用"></a>行业应用</h4><h5 id="苏宁云商"><a href="#苏宁云商" class="headerlink" title="苏宁云商"></a>苏宁云商</h5><ul>
<li><p>主要介绍了从2014年至今几大阶段的大数据技术演化；</p>
</li>
<li><p>SQL on Storm -&gt; Storm Monitor -&gt; Libra … -&gt; Spark Streaming（内测）</p>
</li>
<li><p>资产管理、资源隔离、Docker的应用研究</p>
</li>
</ul>
<h5 id="烽火安全云"><a href="#烽火安全云" class="headerlink" title="烽火安全云"></a>烽火安全云</h5><ul>
<li><p>主要介绍了从2010年至今大数据技术架构的情况；</p>
</li>
<li><p>互联网和信息安全这两个领域的大数据技术的主要区别；</p>
</li>
<li><p>HBase的二级索引解决方案；</p>
</li>
<li><p>针对业务需要、屏蔽底层实现方案，向上层两个主要服务，分别为：queryService（查询引擎）和taskService（任务调度引擎）</p>
</li>
</ul>
<h4 id="专家探讨"><a href="#专家探讨" class="headerlink" title="专家探讨"></a>专家探讨</h4><p>参与者： 樊建、汪军、黄宜华、罗胜美</p>
<p>话题一：大数据在行业落地过程中主要存在哪些问题？（如大数据不易获取、大数据技术使用困难）？应当如何解决？</p>
<blockquote>
<p>罗：数据获取有多种途径，需要企业或研究者们，关注相关行业的情况，比如政府可以免费提供一段时间内的交通数据，给大家进行挖掘和分析研究等。之后，就要看如何使用这些数据和基于这些数据分析方法。</p>
<p>樊：对企业内部上百个项目的业务系统和存储数据的总体把握，是目前比较困难的地方。大数据系统的架构设计、开发和部署的落地是其次。</p>
</blockquote>
<p>话题二： 预测大数据在未来两年可能会在哪几个行业进一步产生Killer App？</p>
<blockquote>
<p>樊：将传统的关系数据库的磁盘存储迁移到分布式文件系统上来，或者是基于内存级别的方式；还特别提到了后期对Docker的研究。</p>
</blockquote>
<p>话题三：大数据教育和人才培养方面有哪些建议和想法？（对学生和老师的建议）？</p>
<blockquote>
<p>黄：单位和高校的深度合作，定向培养；熟悉storm、spark等框架的部署、开发以及运维等系统性的知识。</p>
</blockquote>
<p>话题四：大数据技术和云计算技术的关系以及如何进一步理解？</p>
<blockquote>
<p> 罗：云计算技术侧重的是存储和计算，而大数据技术侧重基于上层服务或应用的需求而产生的各种大数据解决方案。但从某种程度上来说，这两者确实存在一定的交差关系，这一层可以抽象为一个服务层，主要是基于云向上层提供的以及基于大数据业务向下层索取的特定实现。</p>
</blockquote>
<h4 id="个人总结"><a href="#个人总结" class="headerlink" title="个人总结"></a>个人总结</h4><p>今天的meet up让我受益很多，了解到南京地区的企业在大数据技术方面的实践历程和演化之路，尤其表现出了对Docker和Spark的研究和应用的关注。其中，企业要不要使用大数据，如何落地大数据技术，最后的效果又是如何的？都是值得深入思考的。</p>
<p>同时也让我看到了南京大学PASA实验室的强悍之处，作为实验室的成员，meet up的主持人，顾荣博士更是Spark和Tachyon的Contributor，他和他的导师黄教授合作开发了一个v0.1的大章鱼项目。科研与学术的结合，算法模型的多角度验证、offline数据的测试到online数据的证实，是发表顶级paper的前提和过程。</p>
<p>由此可见，国内的大数据技术氛围也是越来越好，最后听取了四位专家的座谈，通过对topics的回答，由浅入深地从学术角度和企业应用方面提出了各自的见解，确实开阔了视野，激起了我学习大数据技术的热情！</p>
<h4 id="相关连接"><a href="#相关连接" class="headerlink" title="相关连接"></a>相关连接</h4><ul>
<li><a href="http://pasa-bigdata.nju.edu.cn/" target="_blank" rel="noopener">南京大学PASA大数据技术实验室</a></li>
</ul>
]]></content>
      <categories>
        <category>个人日志</category>
      </categories>
      <tags>
        <tag>大数据meetup</tag>
      </tags>
  </entry>
  <entry>
    <title>我的书单</title>
    <url>/blog/fa1b3b3e.html</url>
    <content><![CDATA[<h2 id="在读"><a href="#在读" class="headerlink" title="在读"></a>在读</h2><ul>
<li>数据密集性应用系统设计，电子稿</li>
<li>数据库系统内幕，纸质书</li>
</ul>
<h2 id="待读"><a href="#待读" class="headerlink" title="待读"></a>待读</h2><ul>
<li><a href="https://book.douban.com/subject/30296615/" target="_blank" rel="noopener">SparkSQL内核剖析</a></li>
<li><a href="http://e.dangdang.com/products/1900396912.html" target="_blank" rel="noopener">大规模分布式存储系统:原理解析与架构实战</a></li>
<li><a href="http://product.dangdang.com/23462041.html" target="_blank" rel="noopener">大型网站系统与Java中间件实践</a></li>
<li><a href="https://book.douban.com/subject/1467587/" target="_blank" rel="noopener">Unix编程艺术</a></li>
<li><a href="http://product.dangdang.com/25184920.html" target="_blank" rel="noopener">Kafka入门与实践</a></li>
<li>MySQL技术内幕 InnoDB存储引擎第2版</li>
<li><p>编程之法-面试和算法心得</p>
</li>
<li><p><a href="http://e.dangdang.com/products/1900679392.html" target="_blank" rel="noopener">企业IT架构转型之道：阿里巴巴中台战略思想与架构实战</a></p>
</li>
<li><a href="http://e.dangdang.com/products/1900752857.html" target="_blank" rel="noopener">淘宝技术这十年</a></li>
<li><a href="http://product.dangdang.com/23690515.html" target="_blank" rel="noopener">Netty权威指南(第2版)</a></li>
<li><a href="http://product.dangdang.com/25107115.html" target="_blank" rel="noopener">大数据之路：阿里巴巴大数据实践</a></li>
<li><a href="https://book.douban.com/subject/1477390/" target="_blank" rel="noopener">代码大全（第2版）</a></li>
<li>从Paxos到Zookeeper分布式一致性原理与实践</li>
</ul>
<h2 id="已读"><a href="#已读" class="headerlink" title="已读"></a>已读</h2><ul>
<li><a href="http://book.douban.com/subject/24868904/" target="_blank" rel="noopener">高效能程序员的修炼</a></li>
<li><a href="http://book.douban.com/subject/6522893/" target="_blank" rel="noopener">深入理解Java虚拟机：JVM高级特性与最佳实践</a></li>
<li><a href="http://product.dangdang.com/25346848.html" target="_blank" rel="noopener">码出高效：Java开发手册</a></li>
<li><a href="https://book.douban.com/subject/30335935/" target="_blank" rel="noopener">从零开始学架构</a></li>
</ul>
<h2 id="大牛推荐"><a href="#大牛推荐" class="headerlink" title="大牛推荐"></a>大牛推荐</h2><ul>
<li><a href="https://www.iteblog.com/recommend_book/" target="_blank" rel="noopener">过往记忆·程序员图书推荐</a></li>
<li><a href="https://cloud.tencent.com/developer/article/1401391" target="_blank" rel="noopener">Java后端工程师必备书单(含大后端方向相关书籍)</a></li>
<li><a href="https://mp.weixin.qq.com/s/Gdr66GYRgf8FuwIb2u8pNA" target="_blank" rel="noopener">那些影响了几代程序员的编程书籍</a>(程序员书库 CodingBook)</li>
</ul>
]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>技术管理</tag>
        <tag>书籍</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu-linux学习笔记</title>
    <url>/blog/d563d963.html</url>
    <content><![CDATA[<h1 id="一、ubuntu的安装"><a href="#一、ubuntu的安装" class="headerlink" title="一、ubuntu的安装"></a>一、ubuntu的安装</h1><ol>
<li><strong>所需资源</strong>：（1）Ubuntu的iso文件，可从<a href="http://www.ubuntu.com/" target="_blank" rel="noopener">ubuntu官网</a>下载；(2)u盘刻录软件<a href="http://dl.pconline.com.cn/download/58150.html" target="_blank" rel="noopener">unetbootin</a>。</li>
<li><p><strong>安装步骤</strong>：</p>
<ol>
<li>进入bois界面，设置usb优先启动，进入ubuntu的安装界面；</li>
<li><p>一些简单的前期设置这里直接忽略，主要说明一下磁盘分区。</p>
<blockquote>
<p>1）至少需要两个磁盘分区，分别用于创建”/“文件系统与交换分区。其中，linux系统使用交换分区提供虚拟内存，在一个32位的pc中，不超过2GB。2）若磁盘存储空间较大，可划分多个磁盘分区，在每一个磁盘分区中创建一个单独的文件系统，如/usr,/var,/home等文件系统，但不能把/bin,/dev/,/etc/,/lib,/root和/sbin目录作为单独的文件系统分区，这些目录应位于”/“文件系统分区中。3）若要创建多个磁盘分区，查阅分区要求与建议。</p>
</blockquote>
</li>
</ol>
</li>
</ol>
<a id="more"></a>
<h1 id="二、命令行基础知识"><a href="#二、命令行基础知识" class="headerlink" title="二、命令行基础知识"></a>二、命令行基础知识</h1><ol>
<li><span class="red-back">对于ubunut桌面版(若为服务器版，略过)，进入终端的方式：</span><ol>
<li><code>ctrl+alt+T</code>，默认屏幕大小打开终端；</li>
<li><code>alt+F2</code>，进入执行命令文本框，输入<code>gnome-terminal --full-screen</code>，即进入全屏显示，输入<code>exit</code>，即可退到1中打开的状态；</li>
<li>非Gnome图形界面，进入字符终端，输入<code>crtl+alt+f1-6</code>，打开<code>tty1-6</code>的终端界面。输入<code>ctrl+alt+f7</code>，返回GNOME界面；</li>
<li>也可以通过设置/etc/default下的grub文件，设置开启启动到图形界面还是字符界面。</li>
</ol>
</li>
<li>Linux系统默认的命令解释程序是bash，GNU Bourne-Again Shell, 是GNU组织开发和推广的一个项目。</li>
<li><p>一个命令由以下3部分内容组成，中间以空格或制表符等空白字符隔开，形如：<strong>&lt;命令名&gt; &lt;命令选项&gt; &lt;命令参数&gt;</strong></p>
<ol>
<li>其中的命令选项以”-“开头，以”–”为起始标志的命令选项，称为<strong>GUN</strong>选项。除个别命令选项外，大部分”–”开头的是”-“的同义词，可替换使用。</li>
</ol>
</li>
<li><span class="red-back">普通用户与超级用户的切换</span><ol>
<li><strong>超级用户</strong>的默认命令提示符为”#”,<strong>普通用户</strong>的默认命令提示符为”$”；</li>
<li>在终端，键入sudo su，输入密码，进入超级用户模式；键入exit，回到普通用户模式。</li>
</ol>
</li>
<li>前后台进程切换<ol>
<li>前台形式，在shell执行命令期间，用户只能等待，不能做其他操作；</li>
<li>后台形式，在命令执行同时，shell会立即输出命令提示符，等待用户输入新的命令。<strong>只要在命令的后面加上”&amp;”即可</strong>。</li>
</ol>
</li>
<li><p>输入输出</p>
<ol>
<li>从终端输入，这个数据输入源是标准输入<strong>stdin（0）</strong>；</li>
<li>运行结果返回到终端屏幕上，这个输入目的是标准输出<strong>stdout（1）</strong>；</li>
<li><p>运行期间的错误也显示在屏幕上<strong>stderr(2)</strong>。</p>
<blockquote>
<p><strong>注意</strong>：其中（n），n指文件描述符。Linux系统启动一个进程（该进程可能用于执行Shell命令）时，将自动为该进程打开三个文件：标准输入、标准输出和标准错误输出，分别由文件标识符0、1、2标识。</p>
</blockquote>
</li>
</ol>
</li>
<li><p><strong>输入输出重定向</strong></p>
<ol>
<li><p><code>&lt;fname</code>，使用指定的文件作为标准输入（其文件描述符为0），以便从指定的文件中接收输入数据；</p>
<blockquote>
<p><code>wc -l &lt; io.txt</code>,表示将io.txt中的记录行数显示到屏幕终端。</p>
</blockquote>
</li>
<li><p><code>&gt;fname</code>，使用指定的文件作为标准输出（其文件描述符为1），若文件不存在则新建，存在且noclobber标志已经设置，将产生错误，否则覆盖原文件中所有内容。若需要追加在原文件内容后面，需要用<code>&gt;&gt;fname</code>。</p>
<blockquote>
<p>例如：<code>ls -l &gt; io.txt</code>,表示将当前目录下的所有文件信息写入io.txt文件中，若io.txt没有，则自动创建，存在则直接覆盖其中的内容。</p>
</blockquote>
</li>
<li><p><code>&gt;|fname</code>,除了忽略noclobber标致之外，其功能与<code>&gt;fname</code>相同。</p>
<blockquote>
<p>注意：&gt;|符号是强制覆盖文件的符号，它与Shell的noclobber选项有关系，如果noclobber选项开启，表示不允许覆盖任何文件，而&gt;|符号则可以不管noclobber选项的作用，强制将文件覆盖。<code>set -C noclobber</code>开启noclobber，<code>set +C noclobber</code>关闭noclobber。</p>
</blockquote>
</li>
<li><code>&lt;&gt;fname</code>，以读写方式打开指定的文件，并使之作为标准输入。</li>
<li><code>&lt;&lt;[-]fstr</code>，<a href="http://zh.wikipedia.org/wiki/Here文档" target="_blank" rel="noopener">Here document</a>文档的使用技巧。</li>
<li><code>&lt;&amp;digti</code>，使用指定的文件描述符复制一个标准输入。</li>
<li><code>&gt;&amp;digti</code>，使用指定的文件描述符复制一个标准输出。</li>
<li><code>&lt;&amp;-</code>，关闭标准输入，而”n&lt;&amp;-“则表示关闭输入文件描述符n。</li>
<li><code>&gt;&amp;-</code>，关闭标准输出，而”n&gt;&amp;-“则表示关闭输出文件描述符n。</li>
<li><code>&lt;&amp;j</code>，把标准输入重定向到文件描述符j表示的输入文件中。</li>
<li><code>&gt;&amp;j</code>，把标准输出重定向到文件描述符j表示的输出文件中。</li>
<li><p><code>&amp;&gt;fname</code>，把标准输出和标准错误输出均重定向到指定的文件中。</p>
<blockquote>
<p>以下I/O重定向符号”&lt;”或”&gt;”前面有一个数字，则表示相应的文件描述符对应的文件。</p>
</blockquote>
</li>
<li><code>0&lt;fname</code>，把标准输入重定向到指定的文件中。</li>
<li><code>1&gt;fname</code>，把标准输出重定向到指定的文件中；<code>1&gt;&gt;fname</code>，把标准输出重定向并附加到指定的文件中。</li>
<li><code>2&gt;fname</code>，把标准错误输出重定向到指定的文件中；<code>2&gt;&gt;fname</code>，把标准错误输出重定向并附加到指定的文件中。例如:<blockquote>
<p><code>$ errfile=script.errors</code>(errfile就是文件描述符);<br><br><code>$ sss 2&gt;$errfile</code>，sss是错误指令，则输出错误信息，2被重定向到了errfile对应的文件中，故错误信息写在script.errors;<br><br><code>$ aaa 2&gt;&gt;$errfile</code>，继续追加错误信息；<br>cat script.errors，显示错误信息。</p>
</blockquote>
</li>
</ol>
</li>
<li><p><code>i&gt;&amp;j</code>，把文件描述符i表示的输出文件重定向到文件描述符j表示的文件中。例如：</p>
<pre><code>&gt; `command &gt;command.log 2&gt;&amp;1`，标准输出和标准错误输出都重定向到同一个文件中&lt;br&gt;
</code></pre><blockquote>
<p><code>$ echo &quot;hello&quot; &gt;&gt; command.log 2&gt;&amp;1</code><br><br><code>$ sss &quot;hello&quot; &gt;&gt; command.log 2&gt;&amp;1</code></p>
</blockquote>
<ol start="17">
<li><code>[j]&lt;&gt;fname</code>，以读写方式打开指定的文件，并把文件描述符j分配到指定的文件。如果文件不存在，则创建该文件。如果未指定文件描述符j，则表示默认的文件描述符0，即标准输入。</li>
</ol>
</li>
<li><p><strong>管道</strong></p>
<ol>
<li>基本概念：在linux系统中，<strong>管道</strong>是一种先进先出的单向数据通路。是一种特殊的管道重定向。</li>
<li><p>用途：</p>
<ol>
<li><p><span class="red-back">利用管道符号”|”,可以把一个命令的标准输出连接到另一个命令的标准输入。</span>例如：</p>
<blockquote>
<p><code>ls /usr | wc -w</code>，统计/usr目录下文件的数量（利用管道把ls和wc两个命令连接在一起）<br> 传统的方法，是利用一个中间临时文件，如<code>ls /usr &gt; file.tmp ; wc -w &lt; file.tmp</code>。</p>
</blockquote>
</li>
<li><span class="red-back">为滤通程序提供原始数据。由该程序读取来自标准输入的数据，按照指定的检索原则和模式，从输入数据中提取期望的，包含给定字符串的数据</span>，如<a href="http://zh.wikipedia.org/wiki/Grep" target="_blank" rel="noopener">grep</a>。</li>
<li>可以依次加工处理多个命令、脚本和程序的输出数据。<code>command1 | command2 | command3 &gt; output-file</code>。</li>
<li><code>tee</code>命令，一个相当于三通管的实用程序。主要功能是通过标准输入接收并显示数据，同时把数据存储到指定的文件中。</li>
</ol>
</li>
</ol>
</li>
<li><p>元字符与文件名生成</p>
<ol>
<li>linux中，很多命令采用文件名作为命令参数，例如：<blockquote>
<p><code>$ ls -l io.txt</code><br><br><code>-rw-rw-r-- 1 sucab sucab 12 5月 14 10：42 io.txt</code></p>
</blockquote>
</li>
<li><p>Shell支持的与文件名生成有关的元字符极其说明。注意，元字符可以组合使用。</p>
<ol>
<li><p><code>*</code>，可以匹配任何数量的字符或字符串，包括空字符串。例如：</p>
<blockquote>
<p><code>ls -l *.txt</code>，列出所有<code>.txt</code>为文件后缀名的txt文件。<code>su*</code>，表示任何一个以”su”为起始的字符串。</p>
</blockquote>
</li>
<li><code>?</code>，匹配单字符串。</li>
<li><code>[...]</code>，匹配给定范围的字符。例如[a-z]，[0-9]。<code>[!...]或[^...]</code>，表示不在该范围中。例如：<blockquote>
<p>列出当前目录下，以s或i开头的文件。<code>ls -l [io]*</code><br><br>列出不是a-z中字母开头的文件。ls -l [^a-z]*</p>
</blockquote>
</li>
<li><strong>注意</strong>：<code>set -f</code>，<span class="red-back">能够禁止文件名的生成。当shell无法解释元字符时，应注意检查是否设置了该标志。</span></li>
</ol>
</li>
<li>转义和引用<ol>
<li>本身具有特殊意义的元字符，在前面加上转义符号<code>\</code>，则失去其特殊意义。而对于一些普通字符，加上转义符号<code>\</code>，则具有特殊意义。例如：<blockquote>
<p><code>\a</code>,生成声音提示；<code>\b</code>退格符；<code>\e</code>Esc字符；<code>\f</code>换页符；<code>\n</code>换行符；<code>\r</code>回车符；<code>\t</code>制表符；<code>\v</code>竖向制表符；<code>\\反</code>斜线；\’单引号；<code>\nnn</code>采用1-3位8进制数值表示等价ASCII字符；<code>\xHH</code>采用1-2位16进制数值表示的等价ASCII字符；<code>\cX</code>Ctrl+X字符。<br><br>其中，<code>\n</code>换到下一行，\r`回到本行的开头。<span class="red-back">Unix系统里，每行结尾只有”\n”;Windows系统里面，每行结尾是”\n\r”；Mac系统里，每行结尾是”\r”。</span></p>
</blockquote>
</li>
</ol>
</li>
</ol>
</li>
<li>历史命令（很强大呀！）<ol>
<li><span class="red-back">fc命令的常见用法</span><ol>
<li><code>fc -l 10 20</code>，显示命令缓冲区或文件中序号为10-20的命令；</li>
<li><code>fc -l -10</code>，列出最近输入的10条命令；</li>
<li><code>fc -l cat</code>，列出最近一次输入的以<code>cat</code>命令为其实字符串的命令；</li>
<li><code>fc -e gedit/vim 10 20</code>，要利用vim或gedit编辑并执行序号为10-20的命令</li>
<li>先执行<code>ls -l /ect/profile</code>，要想再执行一遍该指令，可以输入<code>fc -s</code>；</li>
<li><code>fc -s 100</code>，执行先前的第100号命令。</li>
</ol>
</li>
<li>history命令的常见用法<ol>
<li>其实是fc的一个特列，是<code>fc -l</code>命令的别名。在bash中，列出所有命令；在korn shell中，仅列出16条命令。经实验，在ubuntu linux中执行<code>fc -l</code>，显示16条；执行<code>history</code>，显示所有；</li>
<li><code>history 10</code>，列出最近执行的10条命令；</li>
<li><code>history -c</code>，清除命令历史缓冲区中的命令。</li>
</ol>
</li>
<li>重复执行先前命令的方式<ol>
<li>在bash中，执行<code>$ !!</code>；</li>
<li>常用的部分”!”命令，自行查阅。</li>
</ol>
</li>
</ol>
</li>
<li>命令的别名（略）</li>
<li>作业控制<ol>
<li>在bash中，set命令的”-m”或”-o monitor”选项用于启用shell的作业控制功能。</li>
<li>除了进程ID之外，shell还会为每个作业分配一个数字较小的作业号。例如，利用”&amp;”符号启动后台作业，并使之异步运行，shell将会输出如下信息。<blockquote>
<p><code>$ find / -name &quot;*conf&quot; -print &gt; conf.log 2&gt;&amp;1 &amp;</code><br><br><code>[1] 2136</code>，<code>[1]</code>作业号，<code>2136</code>作业的进程ID。</p>
</blockquote>
</li>
<li><span class="red-back"> shell 采用作业控制表记录和跟踪当前的作业</span>。利用<code>jobs</code>内部命令，可以显示作业控制表中保存的当前作业。</li>
<li>linux中的作业有运行running，停止stopping，退出exited，完成finished等状态。<blockquote>
<p><code>$ bg %1(作业号)</code>，把停止运行的作业放到后台运行；<br><br><code>$ fg %2</code>，让一个后台作业回到前台继续运行； <br><br><code>kill %1</code>，停止后台作业 <code>wait %2</code>，等待当前正在运行的作业完成。</p>
</blockquote>
</li>
</ol>
</li>
<li>会话记录与命令确认<ol>
<li>保存会话记录：在linux中提供了<code>script</code>命令，可以记录用户从注册到推出系统的整个或部分会话过程，包括用户的输入和系统的响应信息。例如：<blockquote>
<p>$ script，在当前目录自动产生typescript文件<br><br>script started，file is typescript<br><br>$ ….，用户操作<br><br>$ exit<br><br>exit<br><br>script done, file is typescript<br><br>$ cat typescript，显示用户相关操作信息<br></p>
</blockquote>
</li>
<li><span class="red-back">确保使用命令的正确性</span><ol>
<li><code>which</code>：区分同名指令，如tar命令，如何知道是用的哪个tar命令，<code>$ which tar</code>，显示<code>/bin/tar</code>；</li>
<li><code>whereis</code>，显示所有与给定命令相关的文件，如$ <code>whereis tar</code>，显示<code>tar:/bin/tar /usr/lib/tar /usr/include/tar.h /usr/share/man/man1/tar.1.gz</code>。</li>
<li>通过<code>apropos</code>命令，模糊查找给定命令在文档中的说明；<code>whatis</code>命令，完全匹配给定的关键字命令。</li>
</ol>
</li>
</ol>
</li>
</ol>
<h1 id="三、文件系统"><a href="#三、文件系统" class="headerlink" title="三、文件系统"></a>三、文件系统</h1><ol>
<li>路径名称规则<ol>
<li>如果路径名以”/“开头，则说明该路径名是从根目录开始的<strong>绝对路径名</strong>，除此之外，其他所有的路径名都是相对于当前目录的<strong>相对路径名</strong>；</li>
<li>路径名要么是单个名字，或者是以”/“分隔的多个名字，最后一个是文件，可以是任何类型；</li>
<li>在任何目录位置，在路径名中使用”..”，可以往上攀升系统的目录层次。</li>
</ol>
</li>
<li><p>树形层次结构</p>
<ol>
<li><code>-/</code><br></li>
<li><code>--/bin</code>，<span class="red-back">其中包含系统、系统管理员和普通用户可以共享的各种通用程序，如cat、cp、mv、mkdir、rm、ls及ps等常用的基本命令，以及如bash等的各种shell。</span><br></li>
<li><code>--/boot</code>，其中包含系统引导程序、linux内核程序文件vmlinuz、磁盘内存印象文件initrd以及GRUB初始引导程序和配置文件等。<br></li>
<li><code>--/dev</code>，在linux系统中，任何设备均对应一个或多个特殊文件（或称为设备文件），这个目录包含了系统支持的所有设备文件。例如：console表示控制台，lp0表示打印机。<br><blockquote>
<p><code>----/mem</code>，表示系统的物理内存。<br><br><code>----/sda</code>，sda表示连接到主控制器的第一个磁盘，sda1或sda2等则分别表示其中的第一个和第二个磁盘分区等。<br><br><code>----/...</code>，其他相关，这里略。<br><br><code>----/tty</code>，表示系统的串口设备。<br></p>
</blockquote>
</li>
<li><code>--/etc</code>，<span class="red-back">该目录是整个linux系统的中心，其中包含所有系统管理和维护方面的配置文件，如host.conf，syslog.conf和vsftpd.conf等。</span><br><blockquote>
<p><code>----/apache2</code>，apache配置文件的根目录，其中包含apache服务器的各种配置文件，如apache2.conf等。<br><br><code>----/apt</code>，<span class="red-back">其中包含了软件管理工具使用的配置文件，如sources.list等。</span><br><br><code>----/cron.d</code>，用于存储cron进程调度运行后台进程所用的配置和控制文件。<br><br><code>----/init.d</code>，用于存储系统启动过程中需要有init调度执行的脚本文件。<br><br><code>----/mysql</code>，<span class="red-back">其中包含MySQL数据库的配置文件，如my.cnf等。</span><br><br><code>----/network</code>，其中包含网络接口的配置文件interfaces，以及相关的配置工具。<br><br><code>----/...</code><br><br><code>----/ssh</code>，<span class="red-back">OpenSHH网络服务所在配置和控制文件的根目录，其中含有sshd_config等重要配置文件。</span><br></p>
</blockquote>
</li>
<li><code>--/home</code>，<span class="green-back">用户主目录的根目录。新增一个用户，系统将会在/home目录中创建一个形如/home/$USER的子目录，作为用户的目录，其中的$USER为用户名。也可以作为<em>单独的文件系统</em>。</span><br></li>
<li><code>--/lib</code>，该目录含有系统引导过程，以及运行系统命令所需要的内核模块和各种动态链接共享库文件（扩展名为.so，相当于windows系统中的.dll文件）。其中，内核模块（驱动程序）位于/lib/modules/kernel-version子目录中。<br></li>
<li><code>--/lost+found</code>，每个文件系统分区都存在一个lost+found目录，用于存储fsck命令在检测与修复文件系统时删除的文件或目录。<br></li>
<li><code>--/media</code>，移动存储介质的安装点，当利用GNOME界面安装移动存储介质时，系统将会自动地把移动介质安装到此目录下的某个子目录中。<br></li>
<li><code>--/mnt</code>，文件系统的临时安装点。<br></li>
<li><code>--/opt</code>，应用程序等附加软件的安装目录。<br></li>
<li><p><code>--/proc</code>，进程文件系统proc的根目录，其中的部分文件分别对应当前正在运行的进程，可用于访问当前进程的地址空间。<br></p>
<blockquote>
<p><code>----/net</code>，其中的文件分别表示各种网络协议（如tcp，udp及arp等）的状态与统计信息。<br></p>
</blockquote>
</li>
<li><code>--/root</code>，<span class="red-back">超级用户root的主目录（在linux系统中，”/“是整个系统的根目录，而非超级用户的主目录）</span><br></li>
<li><code>--/sbin</code>，该目录中的命令主要供超级用户使用，普通用户通常无法使用。其中包含与系统引导、管理维护，以及与硬件配置等方面有关的命令和脚本文件，符fdisk、init和ifconfig等。<br></li>
<li><code>--/tmp</code>，临时文件目录，用于存储系统运行过程中生成的临时文件，也可以用户存储自己的临时文件。一般不要自己删除这个目录中的文件。<br></li>
<li><code>--/usr</code>，<span class="green-back">既可以作为一个<em>单独的文件系统</em>，也可以作为根目录下的一个子目录，其中存有系统提供的各种共享数据（如用户命令、库函数、头文件和文档等）</span><br><blockquote>
<p><code>----/include</code>，用于存储各种c语言文件。是c开发人员需要经常引用的文件。<br><code>----/bin</code>，其中包含用户经常使用的各种命令(find,make,…,who)<br><br><code>----/lib</code>，包含各种共享的库函数，可供程序员以静态或动态链接自己开发的应用程序。<br><br><code>----/sbin</code><br><br><code>----/share</code><br><br><code>----/src</code>，用于存放linux系统内核的源代码和文档等。<br></p>
</blockquote>
</li>
<li><code>--/var</code>，<span class="green-back">既可作为一个 <em>单独的文件系统</em>，也可作为根目录下的一个子目录，用于存储各种可变长的数据文件（如日志文件）、暂存文件或待处理的临时文件等。</span><br><blockquote>
<p><code>----/lib</code>，用于存储软件包特定的动态链接共享库、配置文件和状态信息等。<br><code>----/lock</code><br><code>----/log</code>，用于守护进程日志文件的存储目录<br><br><code>----/mail</code><br><br><code>----/run</code><br><br><code>----/spool</code>，用于缓存各种待处理的文件。<br><br><code>----/tmp</code>，用于存储各种临时文件。<br><br><code>----/www</code>，<span class="red-back">apache服务器的用户文档根目录，用于存储和发布各种HTML文档。</span></p>
</blockquote>
</li>
</ol>
</li>
<li>文件的类型<ol>
<li>普通文件</li>
<li>目录文件<ol>
<li><code>$ pwd</code>，查看当前所处的目录；</li>
<li><code>$ mkdir dirname</code>，创建一个目录；</li>
<li><code>$ rmdir dirname</code>，删除一个目录。</li>
</ol>
</li>
<li>特殊文件：也称设备文件，linux系统利用特殊文件作为用户与I/O设备之间的接口，使用户能够像读写普通文件一样访问外部设备。</li>
<li>链接文件</li>
<li>符号链接文件</li>
<li>管道文件</li>
</ol>
</li>
<li><p><span class="red-back">文件保护机制</span></p>
<ol>
<li>linux系统把用户分为三类：<ol>
<li>文件属主</li>
<li>同组用户</li>
<li>其他用户</li>
</ol>
</li>
<li>文件的三种基本访问权限<ol>
<li>r（读）：如果文件具有读许可，则相应的用户额可以读文件，如显示文件内容、复制文件等，但不能修改文件。如果允许用户进入某个目录，列举目录下的文件，则至少应赋予用户“读”目录的访问权限；</li>
<li>w（写）:如果文件具有写许可，则相应的用户可以读、写文件，包括显示文件内容以及复制、修改、移动和删除文件等。对于目录而言，如果允许用户创建新文件和删除文件，则必须赋予用户“写”目录的访问权限；</li>
<li>x（执行）：如果具有执行许可，则相应的用户可以运行文件（如程序文件）。对于目录而言，如果允许用户访问其中的任何子目录，则必须赋予用户“执行”目录的访问权限。</li>
</ol>
</li>
<li>举例<blockquote>
<p><code>$ ls -l /bin</code>，查看对bin目录的访问权限<br><br><code>drwxr-xr-x 2 root root 4096 5月 9 05：47 /bin</code><br><br>d表示文件类型为目录，rwx r-x r-x表示文件属主的访问权限为rwx、同组用户的访问权限为r-x、其他用户的访问权限为r-x。</p>
</blockquote>
</li>
<li><p><span class="red-back">修改文件的访问权限</span></p>
<ol>
<li>前提条件<ol>
<li>用户必须是文件或目录的属主</li>
<li>或超级用户</li>
</ol>
</li>
<li>chmod用法示例<ol>
<li>相对权限设置法：<blockquote>
<p>格式<code>$ chmod permissions dir-or-file</code> <br><br><span class="red-back">用字符表示用户类型：u（文件属主），g（同组用户），o（其他用户），a(所有用户)</span>；<br><br>“+”或”-“表示增加或撤销相应的权限；<br><br><code>$ chmod o+w script</code>，表示对其他用户添加写权限<br><br><code>$ chmod o-rw script</code>，表示对其他用户撤销读写权限。</p>
</blockquote>
</li>
<li>绝对权限设置法<blockquote>
<p>格式<code>$ chmod numcode dir-or-file</code>，numcode是一个数字代码，用于表示文件的访问权限，由3位数字组成，分别对应于文件属主、同组用户和其他用户。<br><br><code>$ chmod 755 file</code>，表示对属主用户设定权限码为7，同组用户权限码为5，其他用户权限码为5。<br><br><span class="red-back">“r”对应的二进制码为100（4）,”w”对应的二进制码为010（2），”x”对应的二进制为001。其中，rwx构成的某一用户的权限码是8进制的。</span>。<br><br>解读755，7=4+2+1（rwx），5=4+0+1（r-x），5=4+0+1（r-x）。</p>
</blockquote>
</li>
</ol>
</li>
<li><p>其他访问权限设置</p>
<ol>
<li><p>默认权限：无论何时创建一个文件，linux系统通常会为用户设置一个默认的访问权限。用<span class="red-back">umask</span>命令。</p>
<blockquote>
<p>格式，<code>umask [-S] [nnn]</code>，这个[nnn]与chmod命令中的numcode相反，在对应位上设置1，即为撤销该权限。如000 000 000，设置为000 010 010，即为022，表示u有rwx权限，g有r-x权限，o有r-x权限。</p>
</blockquote>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h1 id="四、文件和目录操作"><a href="#四、文件和目录操作" class="headerlink" title="四、文件和目录操作"></a>四、文件和目录操作</h1><ol>
<li><p><strong>创建文件</strong></p>
<p> <code>$ touch emptyfile</code>，当前目录下，要是没有该文件，就自动创建一个，有则不产生作用；<br><br> <code>$ &gt; emptyfile</code>，利用重定向，产生的文件将覆盖已有的同名文件；<br><br> <code>$ echo &quot;hello word!&quot; &gt; newfile</code>，创建一个含有内容为”hello world”的新文件；<br><br> <code>$ cat &gt; myfile</code>，通过终端输入内容，创建并写入myfile文件；<br><br> 还可以通过vim编辑器，在terminal里面进行操作。</p>
</li>
</ol>
<ol start="2">
<li><p><strong>显示文件列表</strong></p>
<ol>
<li>使用<code>ls</code>命令：ls [options] [dir or file]</li>
<li><p>几个常用的参数：<code>ls -a</code>，列出当前目录下的所有文件（包括.*隐藏文件）;<code>ls -d</code>如果指定的参数是目录，只显示目录的名字，而不是列出目录下的文件；<code>ls -l</code>，这个经常使用，以每行一个文件的长格式列出文件的类型、访问权限、链接数、用户属主、用户组、文件大小、最后修改时间和文件名等；<code>ls -r</code>，以文件名反向排序显示文件列表（–reverse）；<code>ls -R</code>，注意区分与<code>r</code>的区别(–recursive)，递归显示目录及子目录下的所有文件；<code>ls -s</code>，显示分配给文件的数据块，因此，当文件大小相近时，它们的数据块可能是一样的。</p>
<blockquote>
<p><strong> 示例：</strong><br></p>
<ol>
<li>列出当前目录下的文件，<code>ls</code><br></li>
<li>列出指定目录下的文件，<code>ls /</code><br></li>
<li>利用通配符显示文件，如<code>ls -l *.c</code>，<code>*</code>匹配多个字符，列出当前目录下以c结尾的所有文件，为了避免列出子目录中的文件，可以使用<code>ls -ld *</code><br></li>
</ol>
</blockquote>
</li>
</ol>
</li>
<li><p><strong>显示文件内容</strong></p>
<ol>
<li><code>cat [options] [file]</code>，将文件中的内容全部显示到终端，若内容太多，只能看到最后部分；</li>
<li><code>more [options] [file]</code>，将从头到尾一页一页地仔细阅读文件。如果文件很长，在左下角会出现<code>--More--(n%)</code>，<code>（n%）</code>表示已显示的数据内容占整个文件的百分比。<code>less</code>命令的功能与<code>more</code>类似且比它强大；</li>
<li><code>head [-number | -n number] [file]</code>，number表示需要输出的行数。默认，输出前10行（包括空行）；</li>
<li><code>tail [+/-number [-lbcf]] [file]</code>，<code>+</code>表示从文件的其实位置开始计算，<code>-</code>表示从文件的结束位置开始计算。number表示要输出的行数。<strong>备注</strong>：<code>tail</code>，只能访问静态文件的最后几行，对于内容动态增长的文件如日志文件，需要这样访问，<code>tail -f somelogfile</code>。也就是说，somelogfile文件内容加一行，终端就显示一行，实时的。</li>
</ol>
</li>
<li><p><strong>复制文件</strong></p>
</li>
<li><p><code>cp [-ir] source_file target_file</code>，其中，<code>-i</code>表示交互复制方式。例如，要将src文件拷贝到target文件时，如果target文件存在，则给出提示信息，是否要覆盖已有的文件。<code>-r</code>表示递归操作，就是如果src是目录文件，则将目录中的子目录递归复制，拷贝到目标文件中。</p>
</li>
<li><p><strong>移动文件</strong></p>
</li>
<li><p><code>mv [-fi] source_file target_file</code>，把文件从一个目录移动到另外一个目录中，或者重新命名一个文件。<code>-f</code>表示强制移动或改名（force），<code>-i</code>用于交互，给出目标文件存在时的提示信息。</p>
</li>
<li><p><strong>删除文件</strong></p>
</li>
<li><p><code>rm [-rfi] [file]</code>，<code>-r</code>表示递归地删除目录中的文件及目录本身；<code>-i</code>表示交互操作，询问是否真的要删除；<code>-f</code>表示强制删除，即使不存在该文件，也不会输出任何信息。</p>
</li>
<li><p><strong>改换目录</strong></p>
<ol>
<li>在linux系统，每个用户都有一个属于自己的主目录。在注册之后，系统将会自动地把用户引导至自己的主目录；</li>
<li>在bash、korn shell等shell中，<code>~</code>表示用户主目录缩写，在bourne shell中，停替代方法是引用<code>$HOME</code>。</li>
</ol>
</li>
<li><p><strong>创建、移动和复制目录</strong></p>
<ol>
<li><code>mkdir src</code></li>
<li><code>mv src</code></li>
<li><code>cp -r dir1 dir2</code></li>
</ol>
</li>
<li><p><strong>删除目录</strong></p>
<ol>
<li>指令模版，<code>rmdir [-p] directory</code>；</li>
<li>删除一个空的目录，其指令为<code>rmdir dir1</code>；</li>
<li>删除一个非空目录，需要添加<code>-r</code>参数选项,意思是recursive递归，即<code>rm -r dir1</code>，可以把指定目录及其任何子目录中的所有文件全部删除。</li>
</ol>
</li>
<li><p><strong>比较文件之间的差别</strong></p>
<ol>
<li>当面对两个类似的文件，想找出其中的细微差别时，可以使用diff命令比较，<code>diff file1 file2</code></li>
<li>比较三个文件的不同，可以使用<code>diff3 file1 file2 file3</code>。</li>
</ol>
</li>
<li><p><strong>从系统中检索文件</strong></p>
<ol>
<li><code>find directory [options]</code><ol>
<li>find命令将按用户指定的检索条件，从指定的目录开始，找出满足匹配准则的所有文件。指定的检索条件可以是文件（包括通配符）、文件大小及文件修改日期等。</li>
<li>其中，directory是检索的起始目录，options是一种表达式选项，用于指定各种匹配准则或检索条件。</li>
<li>find命令的部分常用选项，自查。</li>
</ol>
</li>
<li><hr>
</li>
<li><hr>
</li>
</ol>
</li>
<li><p><strong>检索文件内容</strong></p>
<ol>
<li><p>利用grep检索文件内容</p>
<ol>
<li>检索文件中的特定字符串，<code>grep [-inv] string file</code>，其中，<code>-i</code>表示忽略字母大小写，<code>-n</code>表示在输出结果之前给出文本行所在文件中的行号，<code>-v</code>表示检索不包含个i定字符串或模式的所有文本行。<code>string</code>是一个检索模式。检索模式可以是一个准备检索的字符串、一个单词或词语。</li>
</ol>
</li>
<li>在grep中使用正则表达式<ol>
<li>在grep中可使用简单的正则表示，复杂的模式匹配需要使用egrep；</li>
</ol>
</li>
<li><hr>
</li>
</ol>
</li>
<li><p>排序</p>
<ol>
<li><code>sort [-bdfimnru] -k key -t sepchar -o output [file]</code>，可对输入的数据或文件内容进行排序。其中，<code>-n</code>按照字符串数值进行排序，<code>-r</code>表示按照从大到小或反向顺序排序，<code>-k</code>表示关键字的字段位置或关键字字段起止字符位置或范围，<code>-b</code>表示忽略前置的空白字符，<code>-d</code>表示仅考虑字母数字和空格字符，按照字典顺序排序；</li>
<li>使用示例：<blockquote>
<p><code>ls -l /var/log/syslog*</code>，该指令是显示所有系统日志的，显示的部分结果如下：<br><br><code>-rw-r----- 1 syslog adm 552   6月 8 15：17 syslog</code> <br><br><code>-rw-r----- 1 syslog adm 103381 6月 8 13：30 syslog.1</code> <br><br><code>...</code><br><br>现在需要根据文件的大小降序排列，具体指令如下：<br><br><code>ls -l /var/log/syslog* | sort -rn -k5</code><br><br><code>-rw-r----- 1 syslog adm 103381 6月 8 13：30 syslog.1</code> <br><br><code>-rw-r----- 1 syslog adm 552   6月 8 15：17 syslog</code> <br><br><code>...</code><br><br>其中，-r表示降序排序，-n表示按照数值大小，-k按照第5个关键字字段位置（也就是文件的大小），实现最终的排序。</p>
</blockquote>
</li>
</ol>
</li>
</ol>
<h1 id="五、编辑文件（vim）"><a href="#五、编辑文件（vim）" class="headerlink" title="五、编辑文件（vim）"></a>五、编辑文件（<a href="http://www.vim.org" target="_blank" rel="noopener">vim</a>）</h1><ol>
<li><p><strong>vim来源</strong>：vim是对unix系统上vi编辑器的扩充与增强，提供许多附加的功能特性，与vi几乎完全兼容。可运行在windows、Macintosh、Unix和Linux系统上。</p>
</li>
<li><p><strong>启动vim</strong></p>
<ol>
<li><code>$ vim myfile</code>，若myfile存在，则会打开指定的文件，并显示文件第一页的内容。若文件不存在，vim将会打开一个新文件；</li>
<li>进入vim编辑器界面之后，屏幕左边的波浪符”~”表示空行；</li>
<li>vim可以同时编辑多个文件，也可以不指定文件名，等到完成文件再写入新文件，然后退出；</li>
<li>直接输入<code>vim</code>命令而未指定文件的名字，会显示vim的介绍信息。此时，可以直接进入插入模式（输入i），编写文件，然后保存（:w filename），退出（:q）。如果想放弃保存当前的操作，则可以强制退出（：qa!）。具体可同:help查看帮助；</li>
<li>状态行：编辑窗口的最后一行是vim的状态行，用于显示编辑器的状态、编辑过程中出现的错误信息、光标所在的行列位置、删除或复制的行数等。初始启动时，状态行会显示文件的名字、行数和字节数。</li>
</ol>
</li>
<li><p><strong>vim编辑器的工作模式</strong></p>
<ol>
<li>命令模式：按下<code>esc</code>键，总是进入命令模式；</li>
<li>输入模式：在要输入文本之前，输入i或a进入输入模式，然后esc，返回命令模式，执行保存退出；</li>
<li>输入”:”开始执行命令的时候</li>
</ol>
</li>
<li><p><strong>保存编辑文件并退出vim</strong></p>
<ol>
<li>注意随时保存数据，特别是当编辑重要的文件时，vim编辑器提供了许多命令，用于把内存缓冲区中的数据内容保存到磁盘文件中，然后退出vim。如”保存并退出”、”强制退出并不保存”等；</li>
<li><strong>相关命令</strong><ol>
<li><code>:w</code>，保存编辑后的文件内容，但不退出vim编辑器。把内存缓冲区中的数据写到启动vim时指定的文件中</li>
<li><code>:w!</code>，强制写文件，即强制覆盖原有的文件。如果原文件是只读文件，并且当前用户是该文件的属主，则可以强制写入；</li>
<li><code>:wq</code>，保存文件内容后退出vim编辑器。写入并退出。另一个替代指令为<code>ZZ</code>；</li>
<li><code>：wq!</code>，强制保存文件内容后退出；</li>
<li><code>ZZ</code>，如果文件已经做过编辑处理，则把内存缓冲区中的数据强制写到启动vim时指定的文件中，然后退出。否则只是退出而已；</li>
<li><code>:q</code>，在未做任何编辑处理而准备退出vim时，使用；</li>
<li><code>:q!</code>，强制退出vim编辑器，放弃编辑处理的结果。如果确实不需要保存修改后的文件内容，可输入该命令，强制退出；</li>
<li><code>:w filename</code>，把编辑处理后的结果写到指定的文件中保存；</li>
<li><code>:w! filename</code>，把编辑处理后的结果强制保存在指定的文件中，如果文件已经存在，则覆盖现有的文件；</li>
<li><code>:wq! filename</code>，把编辑处理后的结果强制保存在指定的文件中，如果文件已经存在，则覆盖现有的文件，最后退出；</li>
</ol>
</li>
</ol>
</li>
<li><p><strong>vim编辑器基本命令</strong></p>
<ol>
<li>移动光标位置：</li>
<li>输入文本：<ol>
<li><code>a</code>，可在当前光标所在字符位置之后输入数据；</li>
<li><code>A</code>，可在光标当前所在行的行尾输入数据；</li>
<li><code>i</code>，可在光标当前所在字符位置之前输入数据；</li>
<li><code>I</code>，可在当前光标所在行的行首输入数据；</li>
<li><code>o</code>，可在光标所在行的下一行新起，输入数据；</li>
<li><code>O</code>，可在光标所在行的上一行新起，输入数据。</li>
</ol>
</li>
<li>修改和替换文本<ol>
<li><code>C</code>，替换从光标位置<strong>开始直至行尾</strong>的所有数据内容，然后输入数据；</li>
<li><code>cw</code>，替换单个字。如”hello world”，光标在hello的h上，输入cw，会取代hello(以空格分隔，一个单词为一个字)，输入新的字替换，而不会去掉world；</li>
<li><code>[n]cc</code>，cc命令，指将光标所在行，全部去掉，输入新数据；加上n后，表示要替换的行数，从光标所在位置，往下替换掉n行；</li>
<li><code>[n]s</code>，表示替换字符，s表示替换光标所在位置的字符，n表示从光标开始往后的n个字符要被替换；</li>
<li><code>S</code>，替换当前光标所在的行；类似cc；</li>
<li><code>r</code>，替换单个字符，类似与1s。<strong>唯一不同的是，r执行完后，自动返回命令模式</strong>;</li>
<li><code>R</code>，替换多个字符，可以从光标位置开始，替换多个字符，数量不限，直至按下esc；</li>
<li><code>[n]~</code>，转换光标当前所在位置字母的大小写，一直按<code>~</code>或者指定需要替换字母大小写的个数，可实现多个字母大小写转换。</li>
</ol>
</li>
<li>撤销先前的修改<ol>
<li><code>u</code>，用于撤销先前执行的编辑命令；</li>
<li><code>U</code>，用于撤销或回复最精一次的操作。</li>
</ol>
</li>
<li>删除文件（一直处于命令模式，而非输入模式（插入，取代））<ol>
<li><code>[n]x</code>，删除字符。n为要删除字符的个数，从光标所在位置的字符往后开始（相当于往后删除），<strong>包括当前光标所在位置</strong>；</li>
<li><code>[n]X</code>，删除字符。n为要删除字符的个数，从光标所在位置的字符往前开始（相当远往前删除），<strong>不包括当前光标所在位置</strong>；</li>
<li><code>dw</code>，删除单个字或部分字。删除整个字的话，包括其占用的空间位置，如字与字之间的分隔符；</li>
<li><code>[n]dd</code>，删除文本行。dd为删除当前光标所在行，n为删除多行；</li>
<li><code>D</code>，表示删除本行的行尾部分。</li>
</ol>
</li>
<li>复制、删除和粘贴文本<ol>
<li>复制-粘贴：先yy复制文本行，再用p（或P）实际复制，</li>
<li>剪切-粘贴：先dd删除文本行，再用p（或P）实现文本行的移动。</li>
<li>具体命令：<ol>
<li><code>[n]yy</code>，记住，<strong>该命令是复制文本行的，不是选中的部分</strong>，过程：<ol>
<li>把光标移至准备复制的文本行的任何位置；</li>
<li>输入<code>yy</code>命令；</li>
<li>再把光标移至目标行的任何位置；</li>
<li><code>p</code>，复制到所在行的<strong>下面</strong>，<code>P</code>，复制所在行的<strong>上面</strong>；</li>
<li>如果在yy前面输入n，可以复制多个文本行。</li>
</ol>
</li>
<li><code>[n]Y</code>，同yy；</li>
<li><code>[n]dd</code>，删除文本行。然后操作过程同yy类似；</li>
<li><strong>上面的指令必须结合p，P使用</strong>。</li>
</ol>
</li>
</ol>
</li>
<li>按指定的数量重复执行指令<ol>
<li>许多vim命令前面都可以加一个计数值，表明相应的命令重复执行的次数；</li>
<li>使用<code>.</code>，可以重复执行先前的文本编辑命令；</li>
</ol>
</li>
</ol>
</li>
<li><p><strong>使用ex命令</strong></p>
<ol>
<li>显示行号，<code>:set nu</code>，挺实用的！！！</li>
<li>取消行号，<code>:set nonu</code></li>
<li>多行复制，<code>:line#1，line#2 co line#3</code>，其中，<code>co</code>相当于copy<ol>
<li>例如，<code>:1,5 co 12</code>把第1行到第5行复制到第12行；</li>
<li>其中，<code>.</code>表示当前光标所在的行，<code>$</code>表示最后一行；</li>
</ol>
</li>
<li>移动文本，<code>:line#1, line#2 m line#3</code>，其中，<code>m</code>相当于move</li>
<li>删除文本，<code>:line#1, line#2 d</code></li>
</ol>
</li>
<li><p><strong>检索与替换</strong></p>
<ol>
<li>概述：使用户能够以检索指定字符串的方式，直接跳转至期望的文件位置。还提供全局检索和替换功能；</li>
<li>检索命令：<ol>
<li><code>:/str</code>，检索给定的字符；</li>
<li><code>:?str</code>，从当前位置开始，反向检索给定的字符串；</li>
<li><code>n</code>，从当前位置开始，继续检索下一个匹配的字符串；</li>
<li><code>N</code>，从当前位置开始，反向检索；</li>
<li><code>:/str/+n</code>，将光标移至匹配的字符串str所在行之后的第n行；</li>
<li><code>:?str?-n</code>，将光标移至匹配的字符串str所在行之前的第n行；</li>
</ol>
</li>
<li>模式检索<ol>
<li>仅检索出现在行首位置的字符串，<code>:/^search</code>；</li>
<li>仅检索出现在行尾位置，<code>:/search$</code>；</li>
<li>仅检索出现在字首位置的字符串，</li>
<li><hr>
</li>
</ol>
</li>
<li>替换字符串</li>
</ol>
</li>
<li><p><strong>编辑多个文件</strong></p>
<ol>
<li>编辑多个文件<ol>
<li><code>vim file1 file2</code>，先进入file1文件，编辑好后，<code>:w</code>保存；输入<code>:n</code>或<code>:n file2</code>，进入file2文件，编辑好后，输入<code>:w</code>保存。</li>
<li>可以使用<code>:e filename，:n filename</code>直接转到指定的文件，也可以使用<code>:n</code>命令转到下一个文件。<code>:n#</code>交替编辑最近处理过的两个文件。</li>
<li><code>:e! filename</code>，强行转到指定的文件。</li>
</ol>
</li>
<li>合并文件与合并文本行<ol>
<li><code>line# r filename</code>，将filename中的内容读到指定行line中；</li>
<li>如果未指定line，则默认是当前光标所在的行</li>
</ol>
</li>
</ol>
</li>
<li><p><strong>定制vim编辑器的运行环境</strong></p>
<ol>
<li>vim编辑器采用一系列默认的选项定义作为自己的运行环境。为了提高编辑效率，需要改变部分选项的默认值。</li>
<li><code>:set all</code>，可以查看到所有的可设置选项，<code>:set option</code>，设置该选项，<code>:set nooption</code>取消该选项设置。</li>
<li>具体options如下：<blockquote>
<p>all，在编辑器窗口中列出编辑器支持的所有选项；<br><br>magic，设置magic，可启动字符的特殊意义，如”.”表示匹配任意一个字符；”[…]”表示匹配指定字符集和或字符范围中的任何一个字符。设置nomagic就会使特殊字符意义失效。<br><br>autoindent，这个选项与shiftwidth选项一起使用，使新输入的文本行与上一行起始位置自动对齐。<br><br>autowrite，自动保存，当打开多个文件的时候，当前编辑完成后，要切换到另一个文件，就会将当前文件自动保存；<br><br>ignorecase，字符串匹配时，可设置忽略大小写；<br><br>laststatus，是否在编辑窗口中显示状态行；<br><br>number，显示文本行的行号；<br><br>readonly，对正在编辑的文件进行写保护；<br><br>report，默认值为2，表示复制或删除了多少行，在状态栏中报告；<br><br>scroll，设置前滚（ctrl+U）多少行，后滚(ctrl+D)多少行，如:set scroll=10;<br><br>shell，确定vim调用哪一个shell，如:set shell=path，path为shell的绝对路径；<br><br>shiftwidth，这个选项用于设定制表符的跳转位置，按下<strong>ctrl+t或ctrl+d</strong>，自动跳转到下一个或上一个制表符位置，默认为8；<br><br>showmatch，输入右圆括号、花括号或方括号时，提示与左边的括号相匹配；<br><br>tabstop，设置制表键tab的右移距离。默认为8；<br><br>wrap，控制vim显示较长的文本行。把较长的文本行延续到下一行，可利用该选项实现自动折行；<br><br>wrapmargin，指定编辑器窗口的右边距；</p>
</blockquote>
</li>
<li>永久性定制vim运行环境<ol>
<li>上面的方法都只能临时地设置vim编辑器的当前运行环境，一旦退出，这种临时设置也随之作废；</li>
<li>永久性设置<ol>
<li>需要设置<strong>VIMINTI变量</strong>。在.bash_profile中添加export VIMINTI = ‘set para1 para2 …’；</li>
<li>也可以把常用的vim选项以及定义加到系统范围的初始化文件<code>/etc/vim/vimrc</code>文件，或用户主目录下的.vimrc(或.exrc) 文件中，可以自己在用户主目录下建立.vimrc文件，在里面写入<code>ab abc 中国农业银行</code>，等</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li><p><strong>其他特殊说明</strong></p>
<ol>
<li>删除或替换特殊字符</li>
<li>在编辑期间运行linux命令<ol>
<li><code>:sh</code>，在vim编辑器中，要转到执行shell的命令话，可使用该命令。此时，使用ctrl+d或exit命令，可退回到vim编辑，继续刚才文件的编辑；</li>
<li><code>:!command</code>，在编辑期间，如果想临时地运行某一个shell命令，可使用该命令，执行完毕后，按enter，即可回复原来的编辑处理状态；</li>
<li><code>!!command</code>，在编辑期间，如果想把某个shell命令的运行结果直接加到当前编辑的文件中</li>
</ol>
</li>
</ol>
</li>
</ol>
<h1 id="六、Shell"><a href="#六、Shell" class="headerlink" title="六、Shell"></a>六、Shell</h1><ol>
<li>基础知识</li>
<li>高级编程</li>
</ol>
<h1 id="七、软件管理"><a href="#七、软件管理" class="headerlink" title="七、软件管理"></a>七、软件管理</h1><ol>
<li><p><strong>软件维护工具</strong></p>
<ol>
<li><strong>命令行</strong>的软件维护工具，包括<code>apt-get</code>、<code>aptitude</code>以及<code>dpkg</code>等；</li>
<li><strong>图形界面</strong>的软件维护工具，包括<code>gnome-app-install</code>与<code>snaptic</code>。</li>
</ol>
</li>
<li><p><strong>软件管理</strong></p>
<ol>
<li>软件包：在ubuntu Linux系统中，所有的软件和文档都是以软件包档案文件的形式提供的。软件包可分为<strong>二进制软件包</strong>（用于封装可执行程序、相关文档以及配置文件等）和源代码软件包（包含源代码以及生成二进制软件包的制作方法）；</li>
<li>常见的软件包的格式：<ol>
<li><strong>Debian</strong>格式软件包（.deb）:<strong>ubuntu软件仓储中提供的软件包均采用这种封装格式</strong>，<strong>apt-get、aptitude和synaptic都支持此类软件包</strong>；</li>
<li><strong>Red Hat</strong>格式软件包（.rpm）:RPM（Red hat Package Manager）是另外一种流行的Linux系统软件包，是Red hat以及派生（如fedora）支持的；</li>
<li><strong>Tarball</strong>:一种由大量文件（包括目录结构）组装成单个档案文件的大型文件集合。其中<code>tar</code>命令用于组合多个文件，生成一个文档，以便于发行；<code>gzip</code>用于压缩文件的容量，以便节省文件的存储空间。Tarball非常类似于windows的”.zip”文件。Tarball文件具有<code>.tar.gz</code>，<code>.tar.bz2</code>或<code>TGZ</code>形式的文件扩展名。<strong>在命令行终端窗口，可以使用</strong><code>tar -xzf filename</code><strong>来解压相应的文件，然后在执行其中包含的软件安装命令</strong>。</li>
</ol>
</li>
<li>软件仓库:指的是一个网站或存储目录，其中提供按一定组织形式存储的软件包与索引文件。</li>
</ol>
</li>
<li><p><strong>利用apt-get管理软件包</strong></p>
<ol>
<li>APT（advanced package tool）:<strong>一个通用的综合软件管理与维护工具，apt-get、aptitude和synaptic等软件工具包都是基于APT及其配置文件发展而来，是APT的前端软件管理工具</strong>。</li>
<li>早期，APT配置命令都存储在单独的配置文件中<strong>/etc/apt/apt.conf</strong>中。在ubuntu linux中，把这个文件分解成多个小型文件，存储在<strong>/etc/apt/apt.conf.d</strong>目录中。<strong>/var/lib/apt/lists</strong>目录存有APT本地软件包索引文件。<strong>/var/cache/apt/archives</strong>目录是APT的本地缓冲目录，其中缓存了最近下载的deb软件包文件。</li>
<li>APT将采用<strong>/etc/apt/sources.list</strong>和<strong>/etc/apt/apt.conf.d目录中所有文件</strong>作为配置文件。</li>
<li>apt-get是一个命令行软件管理工具，能够利用软件仓库安装选定的软件包，或者删除、更新系统中已经安装的软件包，升级linux系统。具体的使用命令为：<code>apt-get [-hvs] [-o=config string] [-c file] {[update] | [upgrade] | [dselect-upgrade] | [install pkgs] | [remove pkgs] | [purge pkgs] | [check] | [clean] | [autoclean] | [autoremove]}</code>，其中，apt-get命令支持的部分功能选项，自查。</li>
<li><strong>常见使用</strong><ol>
<li><code>sudo apt-get install packagename</code>，安装指定的软件包；</li>
<li><code>sudo apt-get update</code>，用于同步软件源的软件包索引，获取最新的可用软件包版本信息；<code>sudo apt-get upgrade</code>，用于升级整个Ubuntu Linux系统（升级的过程，就是软件包的删除和重装过程）；</li>
<li><code>sudo apt-get remove/purge packagename</code>，用于删除软件包，其中remove属于部分删除，保留软件包中配置文件，而purge属于彻底删除；</li>
</ol>
</li>
<li><strong>sources.list配置文件</strong><ol>
<li>apt-get、aptitude以及synaptic是基于APT的，而ATP是使用<strong>/etc/apt/sources.list</strong>配置文件来定义软件的发行源的；</li>
<li>sources.list是主配置文件，可以在/etc/apt/sources.list.d目录中定义其他辅助配置文件，作为sources.list的补充；</li>
</ol>
</li>
</ol>
</li>
<li><p><strong>利用aptitude管理软件包</strong></p>
<ol>
<li>aptitude是一个可以替代apt-get的软件管理工具，语法格式：<blockquote>
<p><code>aptitude [options] {updtae | autoclean | clean | safe-upgrade}</code> <br><br><code>aptitude [options] {install | reinstall | full-upgrade | download | purge | remove | show} pkgs</code> <br><br><code>aptitude [options] search patterns</code> <br><br><code>aptitude help</code></p>
</blockquote>
</li>
<li>常见使用<ol>
<li><code>sudo aptitude install package</code>，安装软件包；</li>
<li><code>sudo aptitude safe-update</code>同步软件源的软件包索引文件，<code>sudo aptitude safe-upgrade</code>升级整个系统；</li>
<li><code>aptitude show vsftpd</code>用于查询各种软件包信息；</li>
<li><code>aptitude search pkg-pattern</code>，用于检索软件包，可以检索系统中已经安装的软件包；</li>
<li><code>aptitude search ~T</code>，查询所有的软件包；</li>
<li><code>aptitude search ~U</code>，列出软件仓库中可供更新的软件包；</li>
<li><code>aptitude search ~i</code>，列出系统中已经安装的软件包，如aptitude search ‘-i apache’表示要检索系统中已经安装的apache服务器软件包；</li>
</ol>
</li>
</ol>
</li>
<li><p><strong>synaptic图形界面软件管理工具</strong></p>
<ol>
<li>synaptic软件包管理器是一种基于APT开发的高级图形界面软件管理工具，其中实现了apt-get命令行工具的所有功能;</li>
<li>在命令行输入synaptic命令，启动；</li>
<li><hr>
</li>
</ol>
</li>
<li><p><strong>GNOME软件增删工具</strong></p>
<ol>
<li>是一种GNOME桌面菜单的软件管理工具，可以根据软件的功能或菜单分类，补充安装或删除选定的软件包，也可用于浏览、检索及查询软件包的说明信息；</li>
<li>在命令行，输入gnome-app-install启动；</li>
<li><hr>
</li>
</ol>
</li>
</ol>
<h1 id="八、用户管理"><a href="#八、用户管理" class="headerlink" title="八、用户管理"></a>八、用户管理</h1><ol>
<li><p><strong>相关概念</strong></p>
<ol>
<li>linux系统中的用户可以分为3类：<strong>超级用户</strong>(root)、<strong>管理用户</strong>和<strong>普通用户</strong>。但也可以把<strong>超级用户</strong>和<strong>管理用户</strong>通称为<strong>系统用户</strong>；<ol>
<li><strong>超级用户</strong>是一个特殊的用户，<strong>用户标识号为0</strong>，可以访问任何程序和文件，任何系统都会自动提供一个超级用户帐号；</li>
<li><strong>管理用户</strong>用于运行一定的系统服务程序，支持和维护相应的系统功能，<strong>用户标识号在1-999范围之内</strong>；</li>
<li>除了超级用户和管理用户，其他均为<strong>普通用户</strong>。访问linux系统的每个用户，都需要有一个用户帐号。只有利用用户名和密码注册到系统之后，才能够访问系统提供的资源和服务；</li>
</ol>
</li>
<li>Ubuntu linux系统强烈建议，应尽量避免使用超级用户注册到系统中，如果确实需要执行系统管理与维护任务，可以在具体的命令前冠以sudo命令；</li>
</ol>
</li>
<li><p><strong>/etc/passwd文件</strong></p>
<ol>
<li>安装linux系统之后，系统已经事先创建了若干系统用户帐号，其中包括超级用户root和管理用户daemon、bin和sys等，用于执行不同类型的系统管理和日常维护任务；</li>
<li>用户的帐号信息是有/etc/passwd和/etc/shadow文件共同维护的；</li>
<li>passwd文件中包含了linux系统中每个用户除密码之外的重要信息，每个用户信息占用一行，每一行由7个字段组成，中间以冒号分开：<code>username:password:uid:gid:comment:home_dir:login_shell</code><strong>(用户名:密码:用户id:用户组id:主目录:命令解释程序)</strong>，在passwd中部分内容如下：<blockquote>
<p><code>root:x:0:0:root:/root:/bin/bash</code> <br><br><code>daemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin</code> <br><br>…. <br><br><code>sucab:x:1000:1000:sucab,,,:/home/sucab:/bin/bash</code> <br><br>其中，每一个字段的具体说明如下：username:注册用户名，用户名在linux系统中必须是唯一的，且至少第一个字符选用字母；password为用户密码，但实际的密码已移至/etc/shadow文件中，如果用户设有密码，会显示为x，若显示*，则用户无法正常地注册到系统；uid用户id，是系统识别用户的主要手段，由系统或系统管理员分配，id为32为无符号整数，即为0-65536，其中0为超级用户，1-999为管理用户，1000-65536为普通的自定义用户；gid用户组id，系统中的每个用户均属于某个用户组，每个用户组除有组名之外，也有一个相应的用户组id，在0-999保留作系统用户组使用；comment为注释信息，包含用户全名，电话号码和电子邮件等用户信息；home_dir指定用户的主目录，环境变量${HOME}，形如/home/username；login_shell指定用户注册后调用的shell，即命令解释程序，如果该字段为空，则默认的命令解释程序为/bin/bash，用户可以根据自己的爱好，选用其他命令解释程序，如korn shell，zsh，tcsh等。</p>
</blockquote>
</li>
</ol>
</li>
<li><p><strong>/etc/shadow文件</strong></p>
<ol>
<li>是一个限制访问的系统文件，其中存有加密形式的密码和其他相关信息，格式如下：<code>username:password:lastchanged:mindays:maxdays:warn:inactive:expire:reserve</code>，在shadow中的部分内容如下：<blockquote>
<p><code>root:!:16198:0:99999:7:::</code><br><br><code>daemon:*:16177:0:99999:7:::</code><br><br>…. <br><br><code>sucab:$6$En29/eVv$n1IaVG0PJ0x3QQtEoTDQU...FVYD.1:16198:0:99999:7:::</code><br><br>其中，password是加密形式的密码，通常用crypt(3)函数生成；lastchanged是从1970年1月1日开始算起，直至最后一次修改密码之日的天数；mindays保持密码稳定不变的最小天数，必须大于等于0，仅当超过此限才能修改密码；maxdays保持密码有效的最大天数，超过此限，系统将会强制提示用户更换新密码；warn指定在密码有效期到期之前需提前多少天向用户发出警告信息；</p>
</blockquote>
</li>
</ol>
</li>
<li><p><strong>增加、修改和删除用户</strong></p>
<ol>
<li>添加用户<ol>
<li><code>useradd [-u uid] [-g group] [-d home_dir] [-s shell] [-c comment] [-m [-k skel_dir]] [-N] [-f inactive] [-e expire] login</code></li>
<li>其中，login表示新用户的注册用户名；</li>
<li><code>-u uid</code>，（–uid uid），用于指定新增用户的用户id，是当前已经分配的最大id号加1；</li>
<li><code>-g group</code>，（–gid group），用于指定一个现有用户组的id或用户组名；</li>
<li><code>-N</code>，（–no-user-group），？</li>
<li><code>-d home_dir</code>，（–home home_dir），用于指定新增用户的主目录；</li>
<li><code>-s shell</code>，（–shell shell），用与指定命令解释程序shell的完整路径名。默认为/bin/bash</li>
<li><code>-c comment</code>，（–comment comment），用于指定用户全名、电话号码以及电子邮件地址等注释信息；</li>
<li><code>-m</code>，（–create-home），在增加新用户时，如果用户的主目录不存在，则创建用户主目录。同时，把/etc/skel目录或<code>-k</code>选项指定目录中的初始化文件复制到用户主目录中；</li>
<li><code>-k skel_dir</code>，（–skel skel_dir），用于指定存储用户初始化文件（.profile）的目录，以便useradd命令能够把其中的文件复制到用户主目录；</li>
<li><code>-f inactive</code>，（–inactive inactive），用于指定相应用户一直未访问系统，但仍保证其注册帐号信息有效的最多天数，超过此限将锁住用户帐号；</li>
<li><code>-e expire</code>，（–expiredate expire），指定注册用户的有效期，即截止日期；<blockquote>
<p><code>sudo useradd -u 1001 -d /home/sucab -m -s /bin/bash sucab</code> <br><br>系统将会在/etc/passwd、/etc/shadow、/etc/group文件中各添加一行与用户sucab相关的信息。一旦创建了用户帐号，就可以使用passwd命令设置密码，使用usermod等命令修改passwd和shadow文件，更改用户的其他相关属性</p>
</blockquote>
</li>
</ol>
</li>
<li>修改用户<ol>
<li>除非用户名或用户id与其他用户冲突，一般情况下不要轻易修改用户帐号中的用户名和用户id，因为这将涉及到用户已经创建的所有文件和目录。但commnet，shell，password，home_dir等可以修改；</li>
<li>修改用户信息时，可以利用编辑器，直接修改passwd和shadow，也可以使用usermode命令，自查。</li>
</ol>
</li>
<li>删除用户<ol>
<li>使用userdel命令，只需要一个命令即可删除passwd，shadow和group文件中的相应用户和用户组信息，同时还会把用户主目录中的所有文件和目录一同删除；</li>
<li><code>userdel [-r] login</code>，-r表示删除用户主目录，包括其中的文件和子目录。</li>
</ol>
</li>
<li>封锁用户帐号<ol>
<li>在/etc/shadow文件的密码字段增加一个感叹号”!”前缀；</li>
<li>在/etc/passwd文件的密码字段增加一个星号”*”；</li>
<li>修改shadow的expire字段为一个过时的日期。</li>
</ol>
</li>
<li>定期更改密码<ol>
<li>普通用户可以使用不带任何选项和参数的passwd命令修改自己的密码，使用实例如下：<blockquote>
<p><code>$ passwd</code><br><br> 更改 username 的密码。<br><br> （当前）unix密码：输入原密码 <br><br> 输入新的unix密码：输入新密码 <br><br> 重新输入新的unix密码:再输一次<br><br> 修改成功</p>
</blockquote>
</li>
<li><code>passwd [-adehlSu] [-i inactive] [-m min] [-m warn] [-x max] [login]</code>。超级用户则可以利用此命令的大量选项及参数维护系统中的用户帐号信息，passwd命令的部分选项自查，可以通过<code>$ passwd --help</code>，查看参数选项的作用，部分如下:<blockquote>
<p><code>-a</code>，只能与<code>-S</code>选项一起使用，以查询所有用户帐号的状态信息，必须具有超级用户权限，即<code>sudo passwd -aS</code>；<br><br><code>-d</code>，删除指定账户的密码；<br><br><code>-e</code>，强制指定账户密码过期；<br><br><code>-h</code>，显示此帮助信息并退出；<br><br><code>-l</code>，锁住指定的账户；<br><br><code>-S</code>，（–status），报告指定账户密码的状态；<br><br><code>-u</code>，(–unlock)，解锁被指定账户。<br><br>使用示例：<br><br><code>$ sudo passwd -S sucab</code><br><br><code>sucab P 05/08/2014 0 99999 7 -1</code>，其中，P表示用户sucab已经设置了密码；若为NP表示未设置密码，L表示账户已经锁住。</p>
</blockquote>
</li>
</ol>
</li>
<li>检验用户的有效用户ID<ol>
<li>要检查用户的有效用户ID，可以使用id命令。例如，当sucab注册到系统之后，id命令的输出结果如下：<blockquote>
<p><code>$ id</code> <br><br><code>uid=1000(sucab) gid=1000(sucab) 组=1000(sucab),4(adm),24(cdrom),27(sudo),30(dip),46(plugdev),108(lpadmin),124(sambashare)</code></p>
</blockquote>
</li>
<li>su命令<ol>
<li>su命令能够改变用户的有效用户ID，而不必另行注册；</li>
<li>使用示例(假设当前注册用户是sucab，另一个账户是zyt)：<blockquote>
<p><code>sucab@X-PC:~$ su zyt</code> （切换到zyt身份）<br><br>密码：输入zyt的密码 <br><br><code>zyt@X-PC:/home/sucab$</code>，说明，即使改变用户身份，仍会保持之前的工作目录不变；<br><br>如果，在su命令之后加上”-“、”-l”或”–login”，则可以直接进入其他用户的主目录，相当于zyt的直接登录。命令如下：<br><br><code>sucab@X-PC:~$ su --login zyt</code> <br><br>密码：输入zyt的密码<br><br><code>zyt@X-PC:~$</code><br><strong>5. 定制用户的工作环境</strong></p>
</blockquote>
</li>
</ol>
</li>
</ol>
</li>
<li>相关概念：当用户注册到系统之后，用户的工作环境是由选定的命令解释程序和相应的用户初始化文件确定的。因此，<strong>用户管理的另一个任务是选择作为用户界面的命令解释程序和用户初始化文件。</strong></li>
<li><p>选择命令解释程序</p>
<ol>
<li>linux系统配备的标准命令解释程序是bash，但同时也支持korn shell(ksh)，基于C shell的TC shell（tcsh）以及Z shell（zsh）等，可以自由选用。</li>
<li><p>/etc/shells中列出了ubuntu linux系统支持的所有命令解释程序，下面介绍4个常见的：</p>
<ol>
<li><p><strong>Bourne Again Shell</strong></p>
<blockquote>
<p>bash是基于POSIX 1003.2标准开发的一个免费版的shell，与Bourne shell和korn shell一脉相承，在功能上有较大的提高。bash，支持emacs与vi两种命令行编辑功能，支持历史命令、命令别名和作业控制等机制。因此，Bash是linux系统首选的shell。</p>
</blockquote>
</li>
<li><p><strong>Korn shell</strong></p>
<blockquote>
<p>(/bin/ksh)是Unix系统中继Bourne shell与C shell之后推出的第3个著名的shell。Korn shell以Borne shell为基础，同时充分吸纳了C shell的优点，极大增强和丰富了Bourne Shell的基础功能。</p>
</blockquote>
</li>
<li><p><strong>TC shell</strong></p>
<blockquote>
<p>C shell原为BSD版unix系统的命令解释程序，由加州大学伯克利分校计算机系的Bill Joy开发。TC shell（/bin/tcsh）是在C shell的基础上开发的，继承并发展了C shell的全部功能特性。但与bash 和korn shell等Bourne系列的shell不兼容。</p>
</blockquote>
</li>
<li><p><strong>Z shell</strong></p>
<blockquote>
<p>同上述的shell一样，zsh既是一个交互式的命令解释程序，也是一个强有力的编程语言。zsh吸收并集成了bash、ksh以及tcsh等shell的许多功能特性，能够提高用户与linux系统交互的效率，非常适合用作交互式shell。zsh是上述shell的超集。</p>
</blockquote>
</li>
<li>任何时候，每个用户只能使用一个命令解释程序，但可以随时从一个shell切换到另一个shell环境。可以使用<code>ps -f</code>指令，找出ps的父进程，即可确定当前使用的哪一个shell。</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<style type="text/css">
    .red-back {
        background:#D9534F;
        color:#FFF;
    }
    .green-back {
        background:#228B22;
        color:#FFF;
    }
</style>

]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop Ecosystem介绍</title>
    <url>/blog/d0befed5.html</url>
    <content><![CDATA[<h3 id="什么是Hadoop"><a href="#什么是Hadoop" class="headerlink" title="什么是Hadoop"></a>什么是Hadoop</h3><ol>
<li><p>Hadoop是Apache基金会下的一个开源分布式计算平台，以Hadoop分布式文件系统（Hadoop distributed file system, HDFS）和MapReduce分布式计算框架为核心，为用户提供了底层细节透明的分布式基础设施。</p>
</li>
<li><p>经过几年的快速发展，Hadoop现在已经发展成为包含多个相关项目的软件生态系统。<strong>侠义的Hadoop核心</strong>：</p>
<ul>
<li><p><strong>Hadoop Common</strong>：0.20+版本中，从core更名为common，提供一些常用的工具，包括系统配置工具Configuration、远程过程调用RPC、序列化机制和Hadoop抽象文件系统FileSystem等。</p>
</li>
<li><p><strong>Hadoop HDFS</strong>：是一个高度容错的系统、能检测和应对硬件故障，用于在低成本的通用硬件上运行。HDFS简化了文件的一致性模型，通过流式数据访问，提供高吞吐量应用程序数据访问功能，适合带有大型数据集的应用程序。</p>
</li>
<li><p><strong>Hadoop MapReduce</strong>：是一种编程模型，用以进行大数据量的计算。将应用划分为Map和Reduce两个步骤，其中Map对数据集上的独立元素进行指定的操作，生成键-值对形式中间结果，Reduce对中间结果中相同的“键”的所有“值”进行规约，得到最终结果。</p>
</li>
</ul>
</li>
</ol>
<a id="more"></a>
<hr>
<h3 id="Hadoop生态系统"><a href="#Hadoop生态系统" class="headerlink" title="Hadoop生态系统"></a>Hadoop生态系统</h3><ol>
<li><p><strong>Sqoop</strong></p>
<ul>
<li><p>是SQL-to-Hadoop的缩写，是Hadoop的周边工具，主要作用是在结构化数据存储与Hadoop之间进行数据交换。</p>
</li>
<li><p>Sqoop可以将一个关系型数据库（Mysql、Oracle等）中的数据导入Hadoop的HDFS、Hive中，也可逆向导入。</p>
</li>
</ul>
</li>
<li><p><strong>Mahout</strong></p>
<ul>
<li><p>主要目标是创建一些可扩展的机器学习领域经典算法的实现，旨在帮助开发人员更加方便快捷地创建智能应用程序。</p>
</li>
<li><p>Mahout现在已经包含了聚类、分类、推荐引擎（协同过滤）和频繁集挖掘等使用的数据挖掘方法。</p>
</li>
<li><p>除了算法，还包含数据的输入和输出工具、与其他存储系统集成（数据库、MongoDB）</p>
</li>
</ul>
</li>
<li><p><strong>ZooKeeper</strong></p>
<ul>
<li><p>作为一个分布式的服务框架，解决分布式计算中的一致性问题。</p>
</li>
<li><p>如统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等。常作为组件使用。</p>
</li>
<li><p>主要用于协调各个组件之间的一致性。</p>
</li>
</ul>
</li>
<li><p><strong>Hive</strong></p>
<ul>
<li><p>最早由FaceBook设计，是建立在Hadoop基础上的<strong>数据仓库架构</strong>。</p>
</li>
<li><p>数据仓库的管理，包括了<strong>数据ETL（抽取、转换和加载）工具</strong>、数据存储管理和大型数据集的查询和分析能力。</p>
</li>
<li><p>Hive提供的是一种结构化数据机制，<strong>Hive QL</strong>，通过该查询语言，数据分析人员可以很方便地运行数据分析业务。</p>
</li>
</ul>
</li>
<li><p><strong>Pig</strong></p>
<ul>
<li><p>运行在Hadoop之上，是对<strong>大型数据集进行分析和评估</strong>的平台。</p>
</li>
<li><p>简化使用Hadoop进行数据分析的要求，<strong>提供一个高层次的、面向领域的抽象语言Pig Latin</strong>。</p>
</li>
<li><p>通过Pig Latin，可以将复杂且相互关联的数据分析任务编码为Pig操作上的数据流脚本，通过将脚本转换为MapReduce任务链。和Hive一样，Pig降低了对大型数据集进行分析和评估的门槛。</p>
</li>
</ul>
</li>
</ol>
<ol start="6">
<li><p><strong>HBase</strong></p>
<ul>
<li><p>继Google的BigTable系统论文，开源社区在HDFS上构建HBase。</p>
</li>
<li><p>是一个针对结构化数据的可伸缩、高可靠、高性能、分布式和面向列的动态模式数据库。</p>
</li>
<li><p>和传统的关系数据库不同，HBase采用了BigTable的数据模型：增强的稀疏排序映射表（Key/Value)，其中，<strong>键由行关键字、列关键字和时间戳构成</strong>。提供对大规模数据的随机、实时读写访问。</p>
</li>
<li><p>HBase中保存的数据可以使用MapReduce来处理，将数据存储和并行计算完美结合。</p>
</li>
</ul>
</li>
</ol>
<hr>
<h3 id="Hadoop的Master和Slave"><a href="#Hadoop的Master和Slave" class="headerlink" title="Hadoop的Master和Slave"></a>Hadoop的Master和Slave</h3><h4 id="Namenode-和-Datanode"><a href="#Namenode-和-Datanode" class="headerlink" title="Namenode 和 Datanode"></a>Namenode 和 Datanode</h4><ol>
<li><p>HDFS采用master/slave架构。一个HDFS集群是由一个Namenode和一定数目的Datanodes组成。</p>
</li>
<li><p>Namenode是一个中心服务器，负责管理文件系统的名字空间(namespace)以及客户端对文件的访问。Namenode执行文件系统的名字空间操作，比如打开、关闭、重命名文件或目录。它也负责确定数据块到具体Datanode节点的映射。</p>
</li>
<li><p>集群中的Datanode一般是一个节点一个，负责管理它所在节点上的存储。HDFS暴露了文件系统的名字空间，用户能够以文件的形式在上面存储数据。从内部看，一个文件其实被分成一个或多个数据块，这些块存储在一组Datanode上。</p>
</li>
<li><p>Datanode负责处理文件系统客户端的读写请求。在Namenode的统一调度下进行数据块的创建、删除和复制。</p>
</li>
</ol>
<h4 id="JobTracker和TaskTracker"><a href="#JobTracker和TaskTracker" class="headerlink" title="JobTracker和TaskTracker"></a>JobTracker和TaskTracker</h4><ol>
<li><p>Hadoop MapReduce采用Master/Slave结构</p>
<ul>
<li><p>Master：是整个集群的唯一的全局管理者，功能包括：作业管理、状态监控和任务调度等，即MapReduce中的JobTracker</p>
</li>
<li><p>Slave：负责任务的执行和任务状态的汇报，即MapReduce中的TaskTracker。</p>
</li>
</ul>
</li>
<li><p>JobTracker</p>
<ul>
<li><p>JobTracker是一个后台服务进程，启动之后，<strong>会一直监听并接收来自各个TaskTracker发送的心跳信息</strong>，包括资源使用情况和任务运行情况等信息。</p>
</li>
<li><p>作业控制：在hadoop中每个应用程序被表示成一个作业，每个作业又被分成多个任务，JobTracker的作业控制模块则负责作业的分解和状态监控。</p>
</li>
<li><p>资源管理</p>
</li>
</ul>
</li>
<li><p>TaskTracker</p>
<ul>
<li><p>TaskTracker是JobTracker和Task之间的桥梁：一方面，从JobTracker接收并执行各种命令：运行任务、提交任务、杀死任务等；另一方面，<strong>将本地节点上各个任务的状态通过心跳周期性汇报给JobTracker</strong>。TaskTracker与JobTracker和Task之间采用了RPC协议进行通信</p>
</li>
<li><p>汇报心跳：Tracker周期性将所有节点上各种信息通过心跳机制汇报给JobTracker。这些信息包括两部分：</p>
<ul>
<li><p>机器级别信息：节点健康情况、资源使用情况等</p>
</li>
<li><p>任务级别信息：任务执行进度、任务运行状态等</p>
</li>
</ul>
</li>
<li><p>执行命令：JobTracker会给TaskTracker下达各种命令.</p>
</li>
</ul>
</li>
</ol>
<hr>
<h3 id="Hadoop集群配置"><a href="#Hadoop集群配置" class="headerlink" title="Hadoop集群配置"></a>Hadoop集群配置</h3><ol>
<li><p>Hadoop现在的最新版本2.5.1，主要包括三个核心组件：</p>
<ul>
<li><p>Common</p>
</li>
<li><p>HDFS</p>
</li>
<li><p>YARN（0.23.0 +，新Hadoop MapReduce框架MapReduceV2 或者叫 Yarn）</p>
<ul>
<li>参考<a href="http://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-yarn/" target="_blank" rel="noopener">http://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-yarn/</a></li>
</ul>
</li>
</ul>
</li>
<li><p>配置前提</p>
<ul>
<li><p>平台：Unbuntu Linux</p>
</li>
<li><p>软件要求：JDK（与hadoop版本对应）、SSH   </p>
</li>
<li><p>下载Hadoop最新稳定版hadoop-2.5.1.tar.gz</p>
</li>
<li><p>解压hadoop-2.5.1.tar.gz，编辑<code>etc/hadoop/hadoop-env.sh</code>文件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># set to the root of your Java installation</span></span><br><span class="line"> <span class="built_in">export</span> JAVA_HOME=/usr/java/latest</span><br><span class="line"></span><br><span class="line"> <span class="comment"># Assuming your installation directory is /usr/local/hadoop</span></span><br><span class="line"> <span class="built_in">export</span> HADOOP_PREFIX=/usr/<span class="built_in">local</span>/hadoop</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行<code>$ bin/hadoop</code>指令</p>
</li>
</ul>
</li>
<li><p>开启hadoop集群有三种模式</p>
<ul>
<li><p>本地（单例）模式</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ mkdir input</span><br><span class="line">$ cp etc/hadoop/*.xml input</span><br><span class="line">   $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.1.jar grep input output <span class="string">'dfs[a-z.]+'</span></span><br><span class="line">   $ cat output/*</span><br></pre></td></tr></table></figure>
</li>
<li><p>伪分布式模式</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"> <span class="comment">&lt;!--编辑 etc/hadoop/core-site.xml--&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> <span class="comment">&lt;!--编辑 etc/hadoop/hdfs-site.xml:--&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 开启ssh</span></span><br><span class="line">$ ssh localhost</span><br><span class="line"><span class="comment"># 如果启动不了，执行以下操作</span></span><br><span class="line">$ ssh-keygen -t dsa -P <span class="string">''</span> -f ~/.ssh/id_dsa</span><br><span class="line">   $ cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 下面就可以在本地运行map reduce程序了</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 首先格式化文件系统</span></span><br><span class="line">$ bin/hdfs namenode -format</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二步，开启namenode和datanode后台守护进程。同时产生的日志将会写到$HADOOP_LOG_DIR目录中，默认为 $HADOOP_HOME/logs</span></span><br><span class="line">$ sbin/start-dfs.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三步，浏览namenode的web界面，默认的地址是NameNode - http://localhost:50070/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第四步，设定mapreduce 程序运行要求的工作目录</span></span><br><span class="line">$ bin/hdfs dfs -mkdir /user</span><br><span class="line">   $ bin/hdfs dfs -mkdir /user/&lt;username&gt;</span><br><span class="line"></span><br><span class="line">   <span class="comment"># 第五步，将输入文件拷贝到hadoop的分布式文件系统中</span></span><br><span class="line">   $ bin/hdfs dfs -put etc/hadoop input</span><br><span class="line"></span><br><span class="line">   <span class="comment"># 第六步，运行hadoop中自带的一些例子程序</span></span><br><span class="line">   $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.1.jar grep input output <span class="string">'dfs[a-z.]+'</span></span><br><span class="line"></span><br><span class="line">   <span class="comment"># 第七步，查看输出的结果</span></span><br><span class="line">   <span class="comment"># 可以将结果文件从分布式文件系统中拷贝到本地文件系统中查看</span></span><br><span class="line">    $ bin/hdfs dfs -get output output</span><br><span class="line">    $ cat output/*</span><br><span class="line">    <span class="comment"># 也可以直接在分布式文件系统中查看</span></span><br><span class="line">     $ bin/hdfs dfs -cat output/*</span><br><span class="line"></span><br><span class="line">     <span class="comment"># 第八步，完成任务后，关闭后台守护进程</span></span><br><span class="line">     $ sbin/stop-dfs.sh</span><br></pre></td></tr></table></figure>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">  <span class="comment">&lt;!--如果想要在YARN上来运行map reduce程序，需要配置一些参数--&gt;</span></span><br><span class="line">     <span class="comment">&lt;!--从第五-八步有所区别--&gt;</span></span><br><span class="line">     <span class="comment">&lt;!--编辑etc/hadoop/mapred-site.xml--&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--编辑etc/hadoop/yarn-site.xml:--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 开启ResourceManager daemon和NodeManager daemon</span></span><br><span class="line">$ sbin/start-yarn.sh</span><br><span class="line"><span class="comment"># 浏览ResourceManage的web界面，默认地址为ResourceManager - http://localhost:8088/</span></span><br><span class="line"><span class="comment"># 运行MapReduce job</span></span><br><span class="line"><span class="comment"># 执行完成，关闭daemon</span></span><br><span class="line"> $ sbin/stop-yarn.sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>完全分布式模式  </p>
<ul>
<li><p>涉及到的主要配置文件</p>
<ul>
<li><p>只读默认配置：core-default.xml, hdfs-default.xml, yarn-default.xml and mapred-default.xml.</p>
</li>
<li><p>特定配置：conf/core-site.xml, conf/hdfs-site.xml, conf/yarn-site.xml and conf/mapred-site.xml.</p>
</li>
<li><p>Hadoop Daemon运行环境配置</p>
<ul>
<li>conf/hadoop-env.sh和conf/yarn-env.sh</li>
</ul>
</li>
</ul>
</li>
<li><p>参考文档：<a href="http://hadoop.apache.org/docs/r2.5.1/hadoop-project-dist/hadoop-common/ClusterSetup.html" target="_blank" rel="noopener">http://hadoop.apache.org/docs/r2.5.1/hadoop-project-dist/hadoop-common/ClusterSetup.html</a></p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>数据库与大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>正则表达式</title>
    <url>/blog/2f57a694.html</url>
    <content><![CDATA[<h3 id="Java中的正则表达式工具包"><a href="#Java中的正则表达式工具包" class="headerlink" title="Java中的正则表达式工具包"></a>Java中的正则表达式工具包</h3><p>包名：java.util.regex.用于匹配字符序列与正则表达式指定模式的类<br>接口：MatchResult:匹配操作的结果<br>类：</p>
<ul>
<li>Matcher:通过解释 Pattern 对 character sequence 执行匹配操作的引擎</li>
<li>Pattern:正则表达式的编译表示形式。</li>
<li>示例：<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Pattern p = Pattern.compile(<span class="string">"a*b"</span>);</span><br><span class="line">Matcher m = p.matcher(<span class="string">"aaaaab"</span>);</span><br><span class="line"><span class="keyword">boolean</span> b = m.matches();</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="正则表达式的基本语法-（部分参考）"><a href="#正则表达式的基本语法-（部分参考）" class="headerlink" title="正则表达式的基本语法 （部分参考）"></a>正则表达式的基本语法 （部分<a href="http://doslin.com/learn-regular-expressions-in-about-55-minutes/" target="_blank" rel="noopener">参考</a>）</h3><a id="more"></a>
<p>由只代表自身的 <strong>字面值</strong> 和代表特定含义的 <strong>元字符</strong> 组成。  </p>
<p>字面值：大部分字符，包括 <strong>字母数字字符</strong>，会以字面值的形式出现。这意味着它们 <strong>查找的是自身</strong>。比如，正则表达式 <code>bobo</code>，就是先找到b，接着o，然后是b，最后找到o。</p>
<h4 id="元字符"><a href="#元字符" class="headerlink" title="元字符"></a>元字符</h4><ul>
<li><code>.</code> 表示匹配任何单字符（与 <strong>行结束符</strong> 可能匹配也可能不匹配）</li>
<li>任何 <strong>元字符</strong> 如果用一个 <strong>反斜杆</strong> <code>\</code> 进行转义就会变成字面值</li>
<li><p>如果未指定 DOTALL 标志，则正则表达式 <code>.</code> 可以与任何字符（<strong>行结束符除外</strong>）匹配</p>
</li>
<li><p><strong>行结束符</strong></p>
<ul>
<li>一个或两个字符的序列，标记输入字符序列的行结尾。</li>
<li>新行（换行）符 (‘\n’)</li>
<li>后面紧跟新行符的回车符 (“\r\n”)</li>
<li>单独的回车符 (‘\r’)</li>
<li>下一行字符 (‘\u0085’)</li>
<li>行分隔符 (‘\u2028’)</li>
<li>段落分隔符 (‘\u2029)</li>
</ul>
</li>
<li><p><code>\</code> <strong>反斜杠</strong> 是一个元字符，这意味着它也可以使用反斜杠转义。示例：</p>
<ul>
<li><code>b.b.</code> 能够匹配bobo,  b b</li>
<li><code>b\.b\.</code> 只能匹配b.b. 这里的.是字面值，不是元字符；</li>
<li><code>b\\</code> 只能匹配<code>b\</code> 这里 <code>\</code> 也是字面值。上述. <code>\</code>  都已经通过 <code>\</code> 进行转义,把元字符变为字面值</li>
<li><code>\t</code> 制表符 (‘\u0009’)</li>
<li><code>\n</code> 新行（换行）符 (‘\u000A’)</li>
<li><code>\r</code> 回车符 (‘\u000D’)</li>
<li><code>\f</code> 换页符 (‘\u000C’)</li>
<li><code>\a</code> 报警 (bell) 符 (‘\u0007’)</li>
<li><code>\e</code> 转义符 (‘\u001B’)</li>
<li><code>\\</code> 反斜线字符</li>
<li><code>x</code> 字符 x</li>
</ul>
</li>
</ul>
<h4 id="字符类"><a href="#字符类" class="headerlink" title="字符类"></a>字符类</h4><p><strong>字符类</strong> 是字符在方括号<code>[ ]</code>中的集合，表示“找到集合里任意一个字符”。<br>存在一些字符，在[ ]中和外面，有的含义是不一样的，有的含义一致。</p>
<ul>
<li>比如，<code>.</code>表示匹配任意字符，<code>[.]</code>表示匹配字面值<code>.</code></li>
<li><code>[a-z]</code>表示匹配<code>a-z</code>范围中任意字符，<code>a</code>表示匹配<code>a</code></li>
<li><code>a|b</code>表示匹配<code>a</code>或者<code>b</code>，<code>[a|b]</code>表示匹配字符<code>a</code>，字符<code>|</code>，字符<code>b</code></li>
</ul>
<p>常用字符类表</p>
<table>
<thead>
<tr>
<th>字符类</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td> <code>[abc]</code></td>
<td>a、b 或 c（简单类）</td>
</tr>
<tr>
<td> <code>[^abc]</code></td>
<td>任何字符，除了 a、b 或 c（否定），其中<code>^</code>表示否定，通过<code>\^</code>可转义成字面值，如<code>[^\^]</code>表示找到除了插入符外的任意字符。</td>
</tr>
<tr>
<td> <code>[a-zA-Z]</code></td>
<td>a 到 z 或 A 到 Z，两头的字母包括在内（范围）</td>
</tr>
<tr>
<td> <code>[a-d[m-p]]</code></td>
<td>a 到 d <strong>或</strong> m 到 p：[a-dm-p]（并集）</td>
</tr>
<tr>
<td> <code>[a-z&amp;&amp;[^bc]]</code></td>
<td>a 到 z，<strong>除了 b 和 c</strong>：[ad-z]（减去）</td>
</tr>
<tr>
<td> <code>[a-z&amp;&amp;[^m-p]]</code></td>
<td>a 到 z，而非 m 到 p：[a-lq-z]（减去）</td>
</tr>
<tr>
<td> <code>[0-9.,]</code></td>
<td>匹配一个数字或者一个句点或者一个逗号</td>
</tr>
<tr>
<td> <code>[0-9a-fA-F]</code></td>
<td>匹配一位十六进制数</td>
</tr>
<tr>
<td> <code>[a-zA-Z0-9\-]</code></td>
<td>匹配一个字母数字字符或连字符</td>
</tr>
<tr>
<td> <code>[0-9]</code></td>
<td>表示匹配0-9中的一个数字，但是<code>[1-31]</code>并不是匹配1-31之间的数字，而是1或2或3，当成了<code>[1-3][1]</code></td>
</tr>
</tbody>
</table>
<h4 id="预定义字符类"><a href="#预定义字符类" class="headerlink" title="预定义字符类"></a>预定义字符类</h4><p> 用一些元字符来预先定义字符的范围。常见的对应关系：</p>
<table>
<thead>
<tr>
<th>预定义字符</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>  <code>\d</code></td>
<td>数字：<code>[0-9]</code></td>
</tr>
<tr>
<td>  <code>\D</code></td>
<td>非数字： <code>[^0-9]</code></td>
</tr>
<tr>
<td>  <code>\w</code></td>
<td>单词字符：<code>[a-zA-Z_0-9]</code>，<strong>字母</strong> 或 <strong>数字</strong> 或 <strong>下划线</strong></td>
</tr>
<tr>
<td>  <code>\W</code></td>
<td>非单词字符：<code>[^\w]</code></td>
</tr>
<tr>
<td>  <code>\s</code></td>
<td>空白字符：<code>[ \t\n\x0B\f\r]</code> 空格，tab，回车或者换行等</td>
</tr>
<tr>
<td>  <code>\S</code></td>
<td>非空白字符：<code>[^\s]</code></td>
</tr>
</tbody>
</table>
<h4 id="乘法器"><a href="#乘法器" class="headerlink" title="乘法器"></a>乘法器</h4><p> 可以在一个<strong>字面值</strong>或者<strong>字符类</strong>后跟着一个<strong>大括号</strong>来使用乘法器。常见用法:</p>
<table>
<thead>
<tr>
<th>表达式</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>X{n}</code></td>
<td>X，恰好 n 次</td>
</tr>
<tr>
<td><code>X{n,}</code></td>
<td>X，至少 n 次</td>
</tr>
<tr>
<td><code>X{n,m}</code></td>
<td>X，至少 n 次，但是不超过 m 次  </td>
</tr>
<tr>
<td><code>X?</code></td>
<td>X，一次或一次也没有，相当于<code>X{0,1}</code></td>
</tr>
<tr>
<td><code>X*</code></td>
<td>X，零次或多次，相当于<code>X{0,}</code></td>
</tr>
<tr>
<td><code>X+</code></td>
<td>X，一次或多次 ，相当于<code>X{1,}</code></td>
</tr>
</tbody>
</table>
<blockquote>
<p> <strong>注意</strong>：表格中出现的{，}，?，+，<em>等符号，放在[ ]中，就变为了字面值[{}+?]，{}+\?也都变为字面值了</em>。</p>
</blockquote>
<h4 id="惰性Non-greedy，勉强Reluctant"><a href="#惰性Non-greedy，勉强Reluctant" class="headerlink" title="惰性Non-greedy，勉强Reluctant"></a>惰性Non-greedy，勉强Reluctant</h4><p> 乘法器可通过<strong>追加问号?</strong>来实现惰性。这里对优先顺序进行了反转，优先匹配字符少的。示例：</p>
<ul>
<li><code>\d{4,5}?</code>表示“匹配<code>\d\d\d\d</code>或<code>\d\d\d\d\d</code>”。其实跟<code>\d{4}</code>行为一致。</li>
<li><code>&quot;.*?&quot;</code>表示“匹配一个双引号，跟着一个<strong>尽可能少的字符</strong>，再跟着一个双引号”。</li>
</ul>
<p>常见用法</p>
<table>
<thead>
<tr>
<th>表达式</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td> <code>X{n}?</code></td>
<td>X，恰好 n 次</td>
</tr>
<tr>
<td> <code>X{n,}?</code></td>
<td>X，至少 n 次</td>
</tr>
<tr>
<td> <code>X{n,m}?</code></td>
<td>X，至少 n 次，但是不超过 m 次  </td>
</tr>
<tr>
<td> <code>X??</code></td>
<td>X，一次或一次也没有，相当于<code>X{0,1}</code></td>
</tr>
<tr>
<td> <code>X*?</code></td>
<td>X，零次或多次，相当于<code>X{0,}</code></td>
</tr>
<tr>
<td> <code>X+?</code></td>
<td>X，一次或多次 ，相当于<code>X{1,}</code></td>
</tr>
</tbody>
</table>
<h4 id="逻辑运算"><a href="#逻辑运算" class="headerlink" title="逻辑运算"></a>逻辑运算</h4><p>主要分类</p>
<ul>
<li><code>XY</code> 表示X 后跟 Y</li>
<li><code>X|Y</code>表示 X 或 Y ，分支，你可以使用<strong>管道符号</strong>来实现匹配多种选择</li>
<li><code>(X)</code> 表示X，作为捕获组。</li>
</ul>
<p>X|Y</p>
<ul>
<li><code>cat|dog</code>表示“匹配cat或dog”。</li>
<li><code>red|blue|</code>和<code>red||blue</code>以及<code>|red|blue</code>都是同样的意思，“匹配<strong>red</strong>或<strong>blue</strong>或<strong>空字符串</strong>”</li>
<li><code>a|b|c</code>跟<code>[abc]</code>一样</li>
<li>cat|dog|\ |表示“匹配<code>cat</code>或<code>dog</code>或<code>|</code>符号”。其中 \ | 被转义了，<code>|</code>属于字面字符。</li>
<li><code>[cat|dog]</code>表示“找到a或c或d或d或g或o或t或一个管道符号|”.</li>
</ul>
<p>(X)</p>
<ul>
<li>在一周中找到一天，使用<code>(Mon|Tues|Wednes|Thurs|Fri|Satur|Sun)day</code></li>
<li><code>\(\)</code>表示“匹配一个左圆括号后，再匹配一个右圆括号”</li>
<li><code>[()]</code>表示“匹配一个左圆括号或一个右圆括号”</li>
<li><code>(red|blue)?</code>等同于<code>(red|blue|)</code></li>
<li><code>\w+(\s+\w+)*</code>代表“找到一个或多个单词，它们以空格隔开”</li>
<li>主要用途：括号是用来表示<strong>组</strong>的，也可以用来<strong>捕获子串</strong>。</li>
<li><p>捕获组：可以拥有<strong>多个捕获组</strong>，它们甚至可以<strong>嵌套使用</strong>。捕获组从左到右进行编号。只要计算左圆括号。    </p>
<p>示例1：<code>(\w+) had a ((\w+) \w+)</code>，从左往右的话，共3个组，分别为<code>(\w+)</code>、<code>((\w+) \w+)</code>和<code>(\w+)</code>。 代码：</p>
  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> regex=<span class="string">"(\\w+) had a ((\\w+) \\w+)"</span>;</span><br><span class="line">str=<span class="string">"I had a nice day"</span>;</span><br><span class="line">Pattern pattern = Pattern.compile(regex);</span><br><span class="line">Matcher matcher = pattern.matcher(str);</span><br><span class="line"></span><br><span class="line">System.out.println(matcher.matches());</span><br><span class="line">System.out.println(matcher.groupCount());<span class="comment">//返回组的总数</span></span><br><span class="line">System.out.println(matcher.group(<span class="number">0</span>));<span class="comment">//表示完整匹配</span></span><br><span class="line">System.out.println(matcher.group(<span class="number">1</span>));<span class="comment">//匹配(\\w+)</span></span><br><span class="line">System.out.println(matcher.group(<span class="number">2</span>));<span class="comment">//匹配((\\w+) \\w+)</span></span><br><span class="line">System.out.println(matcher.group(<span class="number">3</span>));<span class="comment">//匹配(\\w+)</span></span><br><span class="line"><span class="comment">//System.out.println(matcher.group(4));</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//结果</span></span><br><span class="line"><span class="keyword">true</span></span><br><span class="line"><span class="comment">//从一个成功返回的匹配中捕获组数量总是等于原来正则表达式中捕获组的数量</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line">I had a nice day</span><br><span class="line">I</span><br><span class="line">nice day</span><br><span class="line">nice</span><br></pre></td></tr></table></figure>
<p>  示例2：正则表达式((cat)|dog)表示“匹配cat或dog”。这里总是存在两组捕获组。如果我们的输入文本是dog，那么捕获组1是dog，捕获组2是空字符串，因为另一个选择未被使用。代码：</p>
 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">regex=<span class="string">"((cat)|dog)"</span>;</span><br><span class="line">str=<span class="string">"dog"</span>;</span><br><span class="line">Pattern pattern = Pattern.compile(regex);</span><br><span class="line">Matcher matcher = pattern.matcher(str);</span><br><span class="line"></span><br><span class="line">System.out.println(matcher.matches());</span><br><span class="line">System.out.println(matcher.groupCount());<span class="comment">//返回组的总数</span></span><br><span class="line"><span class="comment">//System.out.println(matcher.group(0));//表示完整匹配</span></span><br><span class="line">System.out.println(matcher.group(<span class="number">1</span>));<span class="comment">//匹配(\\w+)</span></span><br><span class="line">System.out.println(matcher.group(<span class="number">2</span>));<span class="comment">//匹配((\\w+) \\w+)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//结果</span></span><br><span class="line"><span class="keyword">true</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line">dog</span><br><span class="line"><span class="keyword">null</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>替换<br> 一旦你用了正则表达式来查找字符串，你可以指定另一个字符串来替换它。第二个字符串时替换表达式。你尝试去用ISO 8691格式的日期（YYYY-MM-DD）去替换美式日期（MM/DD/YY）</p>
<ul>
<li>通过正则表达式<code>(\d\d)/(\d\d)/(\d\d)</code>开始。注意这里有三个捕获组：月，日和两个数字表示的年。通过使用一<strong>一个反斜</strong>和一个<strong>捕获组号</strong>来引用一个捕获组。所以，你的替换表达式为<code>20\3-\1-\2</code>。<code>\3</code>表示引用<code>YY</code>,<code>\1</code>表示引用<code>MM</code>,<code>\2</code>表示引用<code>DD</code>。</li>
<li>代码（调试中?）</li>
</ul>
</li>
<li><p>向后引用<br> <code>([abc])\1</code>表示“匹配aa或bb或cc”。其中<code>\1</code>表示引用第一个捕获组号，如果捕获组中匹配的是a，那么<code>\1</code>就表示a。代码示例：</p>
 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">regex=<span class="string">"([abc])([cd])\\1\\2"</span>;</span><br><span class="line">	str=<span class="string">"cdcd"</span>;</span><br><span class="line">	Pattern pattern = Pattern.compile(regex);</span><br><span class="line">	Matcher matcher = pattern.matcher(str);</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span>(matcher.matches()) &#123;</span><br><span class="line">  <span class="comment">//一定要先执行匹配成功了，才能统计下面的组</span></span><br><span class="line">		System.out.println(matcher.matches());</span><br><span class="line">		System.out.println(matcher.groupCount());</span><br><span class="line">		System.out.println(matcher.group(<span class="number">1</span>));</span><br><span class="line">		System.out.println(matcher.group(<span class="number">2</span>));</span><br><span class="line">		<span class="comment">//System.out.println(matcher.group(3));</span></span><br><span class="line"></span><br><span class="line">		<span class="comment">//结果</span></span><br><span class="line">		<span class="keyword">true</span></span><br><span class="line">		<span class="number">2</span></span><br><span class="line">		c</span><br><span class="line">		d</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="边界匹配器"><a href="#边界匹配器" class="headerlink" title="边界匹配器"></a>边界匹配器</h4><p>分类  </p>
<table>
<thead>
<tr>
<th>符号</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>     <code>^</code></td>
<td>行的开头，<strong>行头</strong></td>
</tr>
<tr>
<td>     <code>$</code></td>
<td>行的结尾，<strong>行尾</strong></td>
</tr>
<tr>
<td>     <code>\b</code></td>
<td>单词边界</td>
</tr>
<tr>
<td>     <code>\B</code></td>
<td>非单词边界</td>
</tr>
<tr>
<td>     <code>\A</code></td>
<td>输入的开头</td>
</tr>
<tr>
<td>     <code>\G</code></td>
<td>上一个匹配的结尾</td>
</tr>
<tr>
<td>     <code>\Z</code></td>
<td>输入的结尾，仅用于最后的结束符（如果有的话）</td>
</tr>
<tr>
<td>     <code>\z</code></td>
<td>输入的结尾</td>
</tr>
</tbody>
</table>
<p><strong>单词</strong> 边界</p>
<ul>
<li>单词边界是 <strong>一个单词字符</strong>和<strong>非单词字符</strong> 之间的位置。</li>
<li>单词边界不是字符。它们宽度为零。</li>
<li>输入的文本it’s a cat有八个单词边界。如果我们在cat后追加一个空格，这里就会有九个单词边界。</li>
<li><code>\b\w\w\w\b</code>表示“匹配一个三个字母的单词”。</li>
</ul>
<p><strong>行</strong> 边界</p>
<ul>
<li>每一块文本会分解成一个或多个行，用换行符分隔。</li>
<li>像单词边界一样，行边界也不是字符。它们宽度为零。</li>
<li>文本不是以换行符结束，而是<strong>以行结束</strong>。然而，任何行，包括最后一行，可以包含零个字符。</li>
<li><strong>行头位置</strong> 是在一个换行符和下一行的第一个字符之间。与单词边界一样，在文本的开头也算作一个起始的行。</li>
<li><strong>行尾位置</strong> 是在行的最后一个字符和换行符之间。与单词边界一样，文本结束也算作行结束。</li>
<li>用法：<ul>
<li><code>^$</code>表示“匹配空行”。</li>
<li><code>^.*$</code>将会匹配整个文本，因为换行符是一个字符，所以<code>.</code>会匹配它。为了匹配单行，要使用惰性乘法器，<code>^.*?$</code>。</li>
<li><code>\^\$</code>表示“匹配尖符号后跟着一个美元符号”</li>
<li><code>[$]</code>表示“匹配一个美元符”。然而，<code>[^]</code>是非法单正则表达式。要记住的是尖符号在方括号中时有不同的特殊含义。把尖符号放在字符类中，这么用<code>[\^]</code>。</li>
</ul>
</li>
</ul>
<p><strong>文本</strong> 边界</p>
<ul>
<li>从“行开始”和“行结束”变成“文本开始”和“文本结束”。</li>
<li>元字符<code>\A</code>和<code>\z</code></li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title>5种开源协议的用途和比较</title>
    <url>/blog/cf64afc8.html</url>
    <content><![CDATA[<p>本文，我们来看5种最常用的开源协议(BSD、Apache、GPL、LGPL、MIT)及它们的适用范围，供那些准备开源或者使用开源产品的开发人员/厂家参考。转自<a href="http://www.copu.org.cn/node/25" target="_blank" rel="noopener">中国开源软件推进联盟OPU</a>。</p>
<a id="more"></a>
<p>一、<a href="http://baike.baidu.com/view/209692.htm" target="_blank" rel="noopener">BSD (Berkeley Software Distribution，伯克利软件套件)</a></p>
<p>BSD开源协议是一个给予使用者很大自由的协议。基本上使用者可以”为所欲为”，可以自由的使用，修改源代码，也可以将修改后的代码作为开源或者专有软件再发布。但”为所欲为”的前提当你发布使用了BSD协议的代码，或则以BSD协议代码为基础做二次开发自己的产品时，需要满足三个条件：</p>
<blockquote>
<ol>
<li>如果再发布的产品中包含源代码，则在源代码中必须带有原来代码中的BSD协议；</li>
<li>如果再发布的只是二进制类库/软件，则需要在类库/软件的文档和版权声明中包含原来代码中的BSD协议；</li>
<li>不可以用开源代码的作者/机构名字和原来产品的名字做市场推广。</li>
</ol>
</blockquote>
<p><span class="green-back">BSD 代码鼓励代码共享，但需要尊重代码作者的著作权。BSD由于允许使用者修改和重新发布代码，也允许使用或在BSD代码上开发商业软件发布和销售，因此是对 商业集成很友好的协议。而很多的公司企业在选用开源产品的时候都首选BSD协议，因为可以完全控制这些第三方的代码，在必要的时候可以修改或者二次开发。</span></p>
<p>二、<a href="http://www.apache.org/licenses/LICENSE-2.0.html" target="_blank" rel="noopener">Apache Licence 2.0</a></p>
<p>Apache Licence是著名的非盈利开源组织Apache采用的协议。该协议和BSD类似，同样鼓励代码共享和尊重原作者的著作权，同样允许代码修改，再发布（作为开源或商业软件）。需要满足的条件也和BSD类似：</p>
<blockquote>
<ol>
<li>需要给代码的用户一份Apache Licence；</li>
<li>如果你修改了代码，需要再被修改的文件中说明；</li>
<li>在延伸的代码中（修改和有源代码衍生的代码中）需要带有原来代码中的协议，商标，专利声明和其他原来作者规定需要包含的说明；</li>
<li>如果再发布的产品中包含一个Notice文件，则在Notice文件中需要带有Apache Licence。你可以在Notice中增加自己的许可，但不可以表现为对Apache Licence构成更改。</li>
</ol>
</blockquote>
<p><span class="green-back">Apache Licence也是对商业应用友好的许可。使用者也可以在需要的时候修改代码来满足需要并作为开源或商业产品发布/销售。</span></p>
<p>三、<a href="http://baike.baidu.com/view/130692.htm" target="_blank" rel="noopener">GPL(General Public License)</a></p>
<ol>
<li><p>我们很熟悉的Linux就是采用了GPL。<span class="red-back">GPL协议和BSD, Apache Licence等鼓励代码重用的许可很不一样。</span>GPL的出发点是代码的开源/免费使用和引用/修改/衍生代码的开源/免费使用，但不允许修改后和衍生的代 码做为闭源的商业软件发布和销售。这也就是为什么我们能用免费的各种linux，包括商业公司的linux和linux上各种各样的由个人，组织，以及商 业软件公司开发的免费软件了。</p>
</li>
<li><p><span class="green-back">GPL协议的主要内容是只要在一个软件中使用(”使用”指类库引用，修改后的代码或者衍生代码)GPL 协议的产品，则该软件产品必须也采用GPL协议，既必须也是开源和免费。这就是所谓的”传染性”。</span>GPL协议的产品作为一个单独的产品使用没有任何问题， 还可以享受免费的优势。</p>
</li>
<li><p>由于GPL严格要求使用了GPL类库的软件产品必须使用GPL协议，对于使用GPL协议的开源代码，商业软件或者对代码有保密要求的部门就不适合集成/采用作为类库和二次开发的基础。</p>
</li>
<li><p>其它细节如再发布的时候需要伴随GPL协议等和BSD/Apache等类似。</p>
</li>
</ol>
<p>四、<a href="http://baike.baidu.com/view/606545.htm" target="_blank" rel="noopener">LGPL(Lesser General Public License，宽松通用公共许可证)</a></p>
<ol>
<li><p>LGPL是GPL的一个为主要为类库使用设计的开源协议。<span class="red-back">和GPL要求任何使用/修改/衍生之GPL类库的的软件必须采用GPL协议不同。</span>LGPL 允许商业软件通过类库引用(link)方式使用LGPL类库而不需要开源商业软件的代码。这使得采用LGPL协议的开源代码可以被商业软件作为类库引用并发布和销售。</p>
</li>
<li><p><span class="green-back">但是如果修改LGPL协议的代码或者衍生，则所有修改的代码，涉及修改部分的额外代码和衍生的代码都必须采用LGPL协议。</span>因此LGPL协议的开源 代码很适合作为第三方类库被商业软件引用，但不适合希望以LGPL协议代码为基础，通过修改和衍生的方式做二次开发的商业软件采用。</p>
</li>
<li><p>GPL/LGPL都保障原作者的知识产权，避免有人利用开源代码复制并开发类似的产品。</p>
</li>
</ol>
<p>五、<a href="http://baike.baidu.com/link?url=zutM1OedqgcqvqxI75UAVcbdcH_E5p3HQAdcGfnTlDkb1l8I7Mr70MQBDsusxBfNS_8xLwCCForigD1gkQHr0K" target="_blank" rel="noopener">MIT</a><br><span class="green-back">MIT是和BSD一样宽范的许可协议，作者只想保留版权，而无任何其他了限制。</span>也就是说，你必须在你的发行版里包含原许可协议的声明，无论你是以二进制发布的还是以源代码发布的。</p>
<style type="text/css">
    .red-back {
        background:#D9534F;
        color:#FFF;
    }
    .green-back {
        background:#228B22;
        color:#FFF;
    }
</style>
]]></content>
      <categories>
        <category>个人日志</category>
      </categories>
      <tags>
        <tag>开源协议</tag>
      </tags>
  </entry>
</search>
